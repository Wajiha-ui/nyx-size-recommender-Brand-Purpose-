{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Wajiha-ui/nyx-size-recommender-Brand-Purpose-/blob/main/AI_Outfit_Recommendation_Model_beta.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OtNdIuuGnfJN",
        "outputId": "0e48c1de-b4f3-4a59-b680-05dfa86b49c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: tensorflow 2.18.0\n",
            "Uninstalling tensorflow-2.18.0:\n",
            "  Successfully uninstalled tensorflow-2.18.0\n",
            "Found existing installation: numpy 1.26.4\n",
            "Uninstalling numpy-1.26.4:\n",
            "  Successfully uninstalled numpy-1.26.4\n",
            "Found existing installation: pandas 2.2.2\n",
            "Uninstalling pandas-2.2.2:\n",
            "  Successfully uninstalled pandas-2.2.2\n",
            "Found existing installation: numba 0.60.0\n",
            "Uninstalling numba-0.60.0:\n",
            "  Successfully uninstalled numba-0.60.0\n",
            "Found existing installation: catboost 1.2.7\n",
            "Uninstalling catboost-1.2.7:\n",
            "  Successfully uninstalled catboost-1.2.7\n",
            "Found existing installation: scikit-learn 1.6.1\n",
            "Uninstalling scikit-learn-1.6.1:\n",
            "  Successfully uninstalled scikit-learn-1.6.1\n",
            "Found existing installation: xgboost 3.0.0\n",
            "Uninstalling xgboost-3.0.0:\n",
            "  Successfully uninstalled xgboost-3.0.0\n",
            "Found existing installation: lightgbm 4.6.0\n",
            "Uninstalling lightgbm-4.6.0:\n",
            "  Successfully uninstalled lightgbm-4.6.0\n",
            "Found existing installation: imbalanced-learn 0.13.0\n",
            "Uninstalling imbalanced-learn-0.13.0:\n",
            "  Successfully uninstalled imbalanced-learn-0.13.0\n",
            "Files removed: 68\n"
          ]
        }
      ],
      "source": [
        "!pip uninstall -y tensorflow numpy pandas numba catboost scikit-learn xgboost lightgbm imbalanced-learn\n",
        "!pip cache purge\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --no-cache-dir numpy==1.26.4 pandas==2.2.2 numba==0.60.0 catboost==1.2.7 tensorflow==2.18.0\n",
        "!pip install --no-cache-dir scikit-learn xgboost lightgbm imbalanced-learn\n",
        "!pip install --no-cache-dir scikit-learn\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OnydqJx3vf3P",
        "outputId": "921d2f23-01a1-4c50-9b2a-d8f148fb8b5d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy==1.26.4 in /usr/local/lib/python3.11/dist-packages (1.26.4)\n",
            "Requirement already satisfied: pandas==2.2.2 in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numba==0.60.0 in /usr/local/lib/python3.11/dist-packages (0.60.0)\n",
            "Requirement already satisfied: catboost==1.2.7 in /usr/local/lib/python3.11/dist-packages (1.2.7)\n",
            "Requirement already satisfied: tensorflow==2.18.0 in /usr/local/lib/python3.11/dist-packages (2.18.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas==2.2.2) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas==2.2.2) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas==2.2.2) (2025.1)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba==0.60.0) (0.43.0)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.11/dist-packages (from catboost==1.2.7) (0.20.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from catboost==1.2.7) (3.10.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from catboost==1.2.7) (1.15.2)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.11/dist-packages (from catboost==1.2.7) (5.24.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from catboost==1.2.7) (1.17.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.18.0) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.18.0) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.18.0) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.18.0) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.18.0) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.18.0) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.18.0) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.18.0) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.18.0) (5.29.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.18.0) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.18.0) (75.1.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.18.0) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.18.0) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.18.0) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.18.0) (1.71.0)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.18.0) (2.18.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.18.0) (3.8.0)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.18.0) (3.13.0)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.18.0) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.18.0) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow==2.18.0) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow==2.18.0) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow==2.18.0) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow==2.18.0) (0.14.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow==2.18.0) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow==2.18.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow==2.18.0) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow==2.18.0) (2025.1.31)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow==2.18.0) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow==2.18.0) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow==2.18.0) (3.1.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost==1.2.7) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost==1.2.7) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost==1.2.7) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost==1.2.7) (1.4.8)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost==1.2.7) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost==1.2.7) (3.2.1)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from plotly->catboost==1.2.7) (9.0.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow==2.18.0) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow==2.18.0) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow==2.18.0) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow==2.18.0) (0.1.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.11/dist-packages (3.0.0)\n",
            "Requirement already satisfied: lightgbm in /usr/local/lib/python3.11/dist-packages (4.6.0)\n",
            "Requirement already satisfied: imbalanced-learn in /usr/local/lib/python3.11/dist-packages (0.13.0)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: nvidia-nccl-cu12 in /usr/local/lib/python3.11/dist-packages (from xgboost) (2.21.5)\n",
            "Requirement already satisfied: sklearn-compat<1,>=0.1 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn) (0.1.3)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import catboost\n",
        "import sklearn\n",
        "\n",
        "print(f\"TensorFlow Version: {tf.__version__}\")\n",
        "print(f\"NumPy Version: {np.__version__}\")\n",
        "print(f\"Pandas Version: {pd.__version__}\")\n",
        "print(f\"CatBoost Version: {catboost.__version__}\")\n",
        "print(f\"Scikit-Learn Version: {sklearn.__version__}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rzFmtDPuwIcp",
        "outputId": "6c5eca4c-5888-40f4-8bcb-2f54fbc6fc81"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow Version: 2.18.0\n",
            "NumPy Version: 1.26.4\n",
            "Pandas Version: 2.2.2\n",
            "CatBoost Version: 1.2.7\n",
            "Scikit-Learn Version: 1.6.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4aOrobzPnLBD",
        "outputId": "1283383c-f25d-4b20-bd98-46bbe48cf68d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Exception ignored on calling ctypes callback function: <function ThreadpoolController._find_libraries_with_dl_iterate_phdr.<locals>.match_library_callback at 0x7a3399df3920>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/threadpoolctl.py\", line 1005, in match_library_callback\n",
            "    self._make_controller_from_path(filepath)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/threadpoolctl.py\", line 1187, in _make_controller_from_path\n",
            "    lib_controller = controller_class(\n",
            "                     ^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/threadpoolctl.py\", line 114, in __init__\n",
            "    self.dynlib = ctypes.CDLL(filepath, mode=_RTLD_NOLOAD)\n",
            "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/ctypes/__init__.py\", line 376, in __init__\n",
            "    self._handle = _dlopen(self._name, mode)\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "OSError: dlopen() error\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ“Š RandomForest Accuracy: 63.42%\n",
            "ðŸ“Š XGBoost Accuracy: 58.64%\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002095 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 1115\n",
            "[LightGBM] [Info] Number of data points in the train set: 6364, number of used features: 28\n",
            "[LightGBM] [Info] Start training from score -1.604579\n",
            "[LightGBM] [Info] Start training from score -1.614795\n",
            "[LightGBM] [Info] Start training from score -1.608496\n",
            "[LightGBM] [Info] Start training from score -1.610853\n",
            "[LightGBM] [Info] Start training from score -1.608496\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ“Š LightGBM Accuracy: 54.68%\n",
            "ðŸ“Š CatBoost Accuracy: 58.96%\n",
            "ðŸ“Š NeuralNetwork Accuracy: 38.09%\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002023 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 1115\n",
            "[LightGBM] [Info] Number of data points in the train set: 6364, number of used features: 28\n",
            "[LightGBM] [Info] Start training from score -1.604579\n",
            "[LightGBM] [Info] Start training from score -1.614795\n",
            "[LightGBM] [Info] Start training from score -1.608496\n",
            "[LightGBM] [Info] Start training from score -1.610853\n",
            "[LightGBM] [Info] Start training from score -1.608496\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001632 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 1115\n",
            "[LightGBM] [Info] Number of data points in the train set: 5091, number of used features: 28\n",
            "[LightGBM] [Info] Start training from score -1.604735\n",
            "[LightGBM] [Info] Start training from score -1.614558\n",
            "[LightGBM] [Info] Start training from score -1.608653\n",
            "[LightGBM] [Info] Start training from score -1.610617\n",
            "[LightGBM] [Info] Start training from score -1.608653\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001723 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 1115\n",
            "[LightGBM] [Info] Number of data points in the train set: 5091, number of used features: 28\n",
            "[LightGBM] [Info] Start training from score -1.604735\n",
            "[LightGBM] [Info] Start training from score -1.614558\n",
            "[LightGBM] [Info] Start training from score -1.608653\n",
            "[LightGBM] [Info] Start training from score -1.610617\n",
            "[LightGBM] [Info] Start training from score -1.608653\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001601 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 1115\n",
            "[LightGBM] [Info] Number of data points in the train set: 5091, number of used features: 28\n",
            "[LightGBM] [Info] Start training from score -1.604735\n",
            "[LightGBM] [Info] Start training from score -1.614558\n",
            "[LightGBM] [Info] Start training from score -1.608653\n",
            "[LightGBM] [Info] Start training from score -1.610617\n",
            "[LightGBM] [Info] Start training from score -1.608653\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001646 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 1115\n",
            "[LightGBM] [Info] Number of data points in the train set: 5091, number of used features: 28\n",
            "[LightGBM] [Info] Start training from score -1.604735\n",
            "[LightGBM] [Info] Start training from score -1.614558\n",
            "[LightGBM] [Info] Start training from score -1.608653\n",
            "[LightGBM] [Info] Start training from score -1.610617\n",
            "[LightGBM] [Info] Start training from score -1.608653\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002039 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 1115\n",
            "[LightGBM] [Info] Number of data points in the train set: 5092, number of used features: 28\n",
            "[LightGBM] [Info] Start training from score -1.603954\n",
            "[LightGBM] [Info] Start training from score -1.615742\n",
            "[LightGBM] [Info] Start training from score -1.607868\n",
            "[LightGBM] [Info] Start training from score -1.611797\n",
            "[LightGBM] [Info] Start training from score -1.607868\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ”— Stacking Ensemble Accuracy: 63.54%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Best model saved successfully!\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "import joblib\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, StackingClassifier\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import xgboost as xgb\n",
        "import lightgbm as lgb\n",
        "import catboost as cb\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "# Generate synthetic dataset\n",
        "def generate_synthetic_data(num_entries=5000):\n",
        "    data = []\n",
        "    genders = [\"Male\", \"Female\"]\n",
        "    age_groups = [\"18-24\", \"25-34\", \"35-44\", \"45-54\", \"55+\"]\n",
        "    body_shapes = [\"Slim\", \"Athletic\", \"Regular\", \"Plus-size\"]\n",
        "    fit_preferences = [\"Slim\", \"Regular\", \"Loose\"]\n",
        "    clothing_types = [\"T-Shirt\", \"Hoodie\", \"Dress\", \"Jacket\", \"Pants\"]\n",
        "    materials = [\"Cotton\", \"Wool\", \"Silk\", \"Polyester\", \"Linen\"]\n",
        "    sizes = [\"XS\", \"S\", \"M\", \"L\", \"XL\", \"XXL\"]\n",
        "    regions = [\"Europe\", \"USA\", \"Asia\"]\n",
        "\n",
        "    for i in range(num_entries):\n",
        "        gender = random.choice(genders)\n",
        "        age_group = random.choice(age_groups)\n",
        "        height = random.randint(150, 200)\n",
        "        weight = random.randint(45, 120)\n",
        "        chest = random.randint(75, 130)\n",
        "        waist = random.randint(60, 110)\n",
        "        hips = random.randint(80, 130)\n",
        "        body_shape = random.choice(body_shapes)\n",
        "        fit = random.choice(fit_preferences)\n",
        "        clothing = random.choice(clothing_types)\n",
        "        size = random.choice(sizes)\n",
        "        material = random.choice(materials)\n",
        "        region = random.choice(regions)\n",
        "\n",
        "        data.append({\n",
        "            \"Gender\": gender,\n",
        "            \"Age Group\": age_group,\n",
        "            \"Height (cm)\": height,\n",
        "            \"Weight (kg)\": weight,\n",
        "            \"Chest (cm)\": chest,\n",
        "            \"Waist (cm)\": waist,\n",
        "            \"Hips (cm)\": hips,\n",
        "            \"Body Shape\": body_shape,\n",
        "            \"Preferred Fit\": fit,\n",
        "            \"Clothing Type\": clothing,\n",
        "            \"Brand Size\": size,\n",
        "            \"Material Preference\": material,\n",
        "            \"Country/Region\": region\n",
        "        })\n",
        "\n",
        "    return pd.DataFrame(data)\n",
        "\n",
        "# Generate and preprocess dataset\n",
        "df = generate_synthetic_data(5000)\n",
        "df[\"BMI\"] = df[\"Weight (kg)\"] / ((df[\"Height (cm)\"] / 100) ** 2)\n",
        "df[\"Chest_Waist_Ratio\"] = df[\"Chest (cm)\"] / df[\"Waist (cm)\"]\n",
        "df[\"Waist_Hips_Ratio\"] = df[\"Waist (cm)\"] / df[\"Hips (cm)\"]\n",
        "\n",
        "# Encode categorical variables\n",
        "df = pd.get_dummies(df, drop_first=True)\n",
        "\n",
        "# Identify the one-hot encoded 'Brand Size' columns\n",
        "brand_size_cols = [col for col in df.columns if col.startswith(\"Brand Size_\")]\n",
        "X = df.drop(columns=brand_size_cols)\n",
        "y = df[brand_size_cols].idxmax(axis=1).str.replace(\"Brand Size_\", \"\")\n",
        "\n",
        "# Encode target labels numerically\n",
        "label_encoder = LabelEncoder()\n",
        "y = label_encoder.fit_transform(y)\n",
        "\n",
        "# Balance the dataset with SMOTE\n",
        "smote = SMOTE(random_state=42)\n",
        "X, y = smote.fit_resample(X, y)\n",
        "\n",
        "# Standardize numerical features\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define models with improved hyperparameters\n",
        "rf_model = RandomForestClassifier(n_estimators=500, max_depth=30, random_state=42)\n",
        "xgb_model = xgb.XGBClassifier(objective=\"multi:softmax\", num_class=len(np.unique(y)), learning_rate=0.05, max_depth=10, n_estimators=300, random_state=42)\n",
        "lgb_model = lgb.LGBMClassifier(num_leaves=31, learning_rate=0.05, n_estimators=300, random_state=42)\n",
        "cat_model = cb.CatBoostClassifier(iterations=500, depth=10, learning_rate=0.05, verbose=0, random_state=42)\n",
        "nn_model = MLPClassifier(hidden_layer_sizes=(128, 64), activation='relu', solver='adam', max_iter=500, random_state=42)\n",
        "\n",
        "# Train models\n",
        "models = {\"RandomForest\": rf_model, \"XGBoost\": xgb_model, \"LightGBM\": lgb_model, \"CatBoost\": cat_model, \"NeuralNetwork\": nn_model}\n",
        "for name, model in models.items():\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    acc = accuracy_score(y_test, y_pred)\n",
        "    print(f\"ðŸ“Š {name} Accuracy: {acc * 100:.2f}%\")\n",
        "\n",
        "# Stacking Model\n",
        "stacking_clf = StackingClassifier(\n",
        "    estimators=[(\"RandomForest\", rf_model), (\"XGBoost\", xgb_model), (\"LightGBM\", lgb_model), (\"CatBoost\", cat_model)],\n",
        "    final_estimator=nn_model\n",
        ")\n",
        "\n",
        "# Train and evaluate stacking model\n",
        "stacking_clf.fit(X_train, y_train)\n",
        "y_pred_stack = stacking_clf.predict(X_test)\n",
        "stacking_accuracy = accuracy_score(y_test, y_pred_stack)\n",
        "print(f\"ðŸ”— Stacking Ensemble Accuracy: {stacking_accuracy * 100:.2f}%\")\n",
        "\n",
        "# Save best model\n",
        "best_model = max(models.items(), key=lambda x: accuracy_score(y_test, x[1].predict(X_test)))[1]\n",
        "joblib.dump(best_model, \"best_size_model.pkl\")\n",
        "print(\"âœ… Best model saved successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "3Ul8FrGQ1E-J",
        "outputId": "13ce62c2-4d77-424b-875a-bb236933626c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸš€ Training RandomForest...\n",
            "ðŸš€ Training XGBoost...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ” Top Features from RandomForest:\n",
            "        Feature  Importance\n",
            "5    Feature_5    0.092551\n",
            "6    Feature_6    0.089814\n",
            "7    Feature_7    0.089514\n",
            "1    Feature_1    0.085923\n",
            "0    Feature_0    0.083891\n",
            "2    Feature_2    0.083373\n",
            "4    Feature_4    0.081762\n",
            "3    Feature_3    0.078440\n",
            "17  Feature_17    0.018875\n",
            "8    Feature_8    0.018087\n",
            "13  Feature_13    0.017366\n",
            "27  Feature_27    0.017149\n",
            "26  Feature_26    0.017146\n",
            "22  Feature_22    0.016738\n",
            "15  Feature_15    0.016313\n",
            "ðŸ” Top Features from XGBoost:\n",
            "        Feature  Importance\n",
            "21  Feature_21    0.042588\n",
            "20  Feature_20    0.042539\n",
            "17  Feature_17    0.041728\n",
            "18  Feature_18    0.041603\n",
            "15  Feature_15    0.041280\n",
            "11  Feature_11    0.038252\n",
            "25  Feature_25    0.037404\n",
            "26  Feature_26    0.037224\n",
            "19  Feature_19    0.037092\n",
            "22  Feature_22    0.036955\n",
            "12  Feature_12    0.036934\n",
            "27  Feature_27    0.036386\n",
            "5    Feature_5    0.035092\n",
            "10  Feature_10    0.034932\n",
            "13  Feature_13    0.034914\n"
          ]
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Ensure models are trained before calling feature importance\n",
        "models = {\n",
        "    \"RandomForest\": RandomForestClassifier(n_estimators=300, random_state=42),\n",
        "    \"XGBoost\": xgb.XGBClassifier(objective=\"multi:softmax\", num_class=len(np.unique(y_train)), random_state=42)\n",
        "}\n",
        "\n",
        "# Train models on the full dataset if not already trained\n",
        "for name, model in models.items():\n",
        "    print(f\"ðŸš€ Training {name}...\")\n",
        "    model.fit(X_train, y_train)\n",
        "    models[name] = model  # Save trained model\n",
        "\n",
        "# Assign trained models\n",
        "best_rf = models[\"RandomForest\"]\n",
        "best_xgb = models[\"XGBoost\"]\n",
        "\n",
        "# Function to get feature importance from different models\n",
        "def plot_feature_importance(model, X, model_name):\n",
        "    importance = model.feature_importances_\n",
        "    feature_importance_df = pd.DataFrame({\n",
        "        'Feature': X.columns,\n",
        "        'Importance': importance\n",
        "    }).sort_values(by=\"Importance\", ascending=False)\n",
        "\n",
        "    # Plot\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    sns.barplot(x=\"Importance\", y=\"Feature\", data=feature_importance_df[:15])\n",
        "    plt.title(f\"Top 15 Feature Importance - {model_name}\")\n",
        "    plt.show()\n",
        "\n",
        "    return feature_importance_df\n",
        "\n",
        "# Convert X_train to DataFrame (Ensure feature names are correctly assigned)\n",
        "feature_names = [f\"Feature_{i}\" for i in range(X_train.shape[1])]  # If feature names are missing\n",
        "X_train_df = pd.DataFrame(X_train, columns=feature_names)\n",
        "\n",
        "# Train models & plot feature importance\n",
        "rf_feature_importance = plot_feature_importance(best_rf, X_train_df, \"RandomForest\")\n",
        "xgb_feature_importance = plot_feature_importance(best_xgb, X_train_df, \"XGBoost\")\n",
        "\n",
        "# Display top features\n",
        "print(\"ðŸ” Top Features from RandomForest:\\n\", rf_feature_importance.head(15))\n",
        "print(\"ðŸ” Top Features from XGBoost:\\n\", xgb_feature_importance.head(15))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2O4FSMcMlvVO",
        "outputId": "78e37610-2a15-4617-abfc-4be76d259b01"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸš€ Training RandomForest with selected features...\n",
            "ðŸ“Š RandomForest Accuracy (after feature selection): 59.84%\n",
            "ðŸš€ Training XGBoost with selected features...\n",
            "ðŸ“Š XGBoost Accuracy (after feature selection): 52.29%\n",
            "ðŸš€ Training LightGBM with selected features...\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001899 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 1100\n",
            "[LightGBM] [Info] Number of data points in the train set: 6364, number of used features: 23\n",
            "[LightGBM] [Info] Start training from score -1.604579\n",
            "[LightGBM] [Info] Start training from score -1.614795\n",
            "[LightGBM] [Info] Start training from score -1.608496\n",
            "[LightGBM] [Info] Start training from score -1.610853\n",
            "[LightGBM] [Info] Start training from score -1.608496\n",
            "ðŸ“Š LightGBM Accuracy (after feature selection): 49.53%\n",
            "ðŸš€ Training CatBoost with selected features...\n",
            "ðŸ“Š CatBoost Accuracy (after feature selection): 54.81%\n"
          ]
        }
      ],
      "source": [
        "# Select important features from both RandomForest & XGBoost\n",
        "selected_features = [\n",
        "    \"Feature_0\", \"Feature_1\", \"Feature_2\", \"Feature_3\", \"Feature_4\", \"Feature_5\", \"Feature_6\", \"Feature_7\",\n",
        "    \"Feature_22\", \"Feature_24\", \"Feature_23\", \"Feature_15\", \"Feature_19\", \"Feature_18\", \"Feature_21\", \"Feature_11\",\n",
        "    \"Feature_20\", \"Feature_25\", \"Feature_9\", \"Feature_27\", \"Feature_14\", \"Feature_12\", \"Feature_10\"\n",
        "]\n",
        "\n",
        "# Filter dataset to keep only selected features\n",
        "X_train_selected = pd.DataFrame(X_train, columns=feature_names)[selected_features]\n",
        "X_test_selected = pd.DataFrame(X_test, columns=feature_names)[selected_features]\n",
        "\n",
        "# Retrain models on reduced feature set\n",
        "models = {\n",
        "    \"RandomForest\": RandomForestClassifier(n_estimators=300, random_state=42),\n",
        "    \"XGBoost\": xgb.XGBClassifier(objective=\"multi:softmax\", num_class=len(np.unique(y_train)), random_state=42),\n",
        "    \"LightGBM\": lgb.LGBMClassifier(random_state=42),\n",
        "    \"CatBoost\": cb.CatBoostClassifier(verbose=0, random_state=42)\n",
        "}\n",
        "\n",
        "# Train & evaluate models\n",
        "for name, model in models.items():\n",
        "    print(f\"ðŸš€ Training {name} with selected features...\")\n",
        "    model.fit(X_train_selected, y_train)\n",
        "    y_pred = model.predict(X_test_selected)\n",
        "    acc = accuracy_score(y_test, y_pred)\n",
        "    print(f\"ðŸ“Š {name} Accuracy (after feature selection): {acc * 100:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mq2FotXot0xQ",
        "outputId": "62c3ecdb-d024-44e2-bc5a-1c9fe4f3681d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002368 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 1115\n",
            "[LightGBM] [Info] Number of data points in the train set: 6364, number of used features: 28\n",
            "[LightGBM] [Info] Start training from score -1.604579\n",
            "[LightGBM] [Info] Start training from score -1.614795\n",
            "[LightGBM] [Info] Start training from score -1.608496\n",
            "[LightGBM] [Info] Start training from score -1.610853\n",
            "[LightGBM] [Info] Start training from score -1.608496\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002205 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 1115\n",
            "[LightGBM] [Info] Number of data points in the train set: 6364, number of used features: 28\n",
            "[LightGBM] [Info] Start training from score -1.604579\n",
            "[LightGBM] [Info] Start training from score -1.614795\n",
            "[LightGBM] [Info] Start training from score -1.608496\n",
            "[LightGBM] [Info] Start training from score -1.610853\n",
            "[LightGBM] [Info] Start training from score -1.608496\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001609 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 1115\n",
            "[LightGBM] [Info] Number of data points in the train set: 5091, number of used features: 28\n",
            "[LightGBM] [Info] Start training from score -1.604735\n",
            "[LightGBM] [Info] Start training from score -1.614558\n",
            "[LightGBM] [Info] Start training from score -1.608653\n",
            "[LightGBM] [Info] Start training from score -1.610617\n",
            "[LightGBM] [Info] Start training from score -1.608653\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001677 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 1115\n",
            "[LightGBM] [Info] Number of data points in the train set: 5091, number of used features: 28\n",
            "[LightGBM] [Info] Start training from score -1.604735\n",
            "[LightGBM] [Info] Start training from score -1.614558\n",
            "[LightGBM] [Info] Start training from score -1.608653\n",
            "[LightGBM] [Info] Start training from score -1.610617\n",
            "[LightGBM] [Info] Start training from score -1.608653\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001714 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 1115\n",
            "[LightGBM] [Info] Number of data points in the train set: 5091, number of used features: 28\n",
            "[LightGBM] [Info] Start training from score -1.604735\n",
            "[LightGBM] [Info] Start training from score -1.614558\n",
            "[LightGBM] [Info] Start training from score -1.608653\n",
            "[LightGBM] [Info] Start training from score -1.610617\n",
            "[LightGBM] [Info] Start training from score -1.608653\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001624 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 1115\n",
            "[LightGBM] [Info] Number of data points in the train set: 5091, number of used features: 28\n",
            "[LightGBM] [Info] Start training from score -1.604735\n",
            "[LightGBM] [Info] Start training from score -1.614558\n",
            "[LightGBM] [Info] Start training from score -1.608653\n",
            "[LightGBM] [Info] Start training from score -1.610617\n",
            "[LightGBM] [Info] Start training from score -1.608653\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002146 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 1115\n",
            "[LightGBM] [Info] Number of data points in the train set: 5092, number of used features: 28\n",
            "[LightGBM] [Info] Start training from score -1.603954\n",
            "[LightGBM] [Info] Start training from score -1.615742\n",
            "[LightGBM] [Info] Start training from score -1.607868\n",
            "[LightGBM] [Info] Start training from score -1.611797\n",
            "[LightGBM] [Info] Start training from score -1.607868\n",
            "ðŸ”— Stacking Ensemble Accuracy: 60.65%\n",
            "âœ… Best model saved successfully!\n"
          ]
        }
      ],
      "source": [
        "from sklearn.ensemble import StackingClassifier\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "# Reintroduce mid-importance features\n",
        "selected_features_extended = [\n",
        "    \"Feature_0\", \"Feature_1\", \"Feature_2\", \"Feature_3\", \"Feature_4\", \"Feature_5\", \"Feature_6\", \"Feature_7\",\n",
        "    \"Feature_22\", \"Feature_24\", \"Feature_23\", \"Feature_15\", \"Feature_19\", \"Feature_18\", \"Feature_21\", \"Feature_11\",\n",
        "    \"Feature_20\", \"Feature_25\", \"Feature_9\", \"Feature_27\", \"Feature_14\", \"Feature_12\", \"Feature_10\",\n",
        "    \"Feature_8\", \"Feature_13\", \"Feature_16\", \"Feature_17\", \"Feature_26\"  # Bringing back some mid-range features\n",
        "]\n",
        "\n",
        "# Filter dataset to keep only selected features\n",
        "X_train_selected = pd.DataFrame(X_train, columns=feature_names)[selected_features_extended]\n",
        "X_test_selected = pd.DataFrame(X_test, columns=feature_names)[selected_features_extended]\n",
        "\n",
        "# Hyperparameter tuning for RandomForest\n",
        "rf_params = {\n",
        "    \"n_estimators\": [300, 500, 700],\n",
        "    \"max_depth\": [20, 30, None],\n",
        "    \"min_samples_split\": [2, 5, 10],\n",
        "    \"min_samples_leaf\": [1, 2, 4]\n",
        "}\n",
        "best_rf = RandomizedSearchCV(RandomForestClassifier(random_state=42), rf_params, n_iter=5, cv=3, n_jobs=-1)\n",
        "best_rf.fit(X_train_selected, y_train)\n",
        "\n",
        "# Hyperparameter tuning for XGBoost\n",
        "best_xgb = xgb.XGBClassifier(objective=\"multi:softmax\", num_class=len(np.unique(y_train)), random_state=42)\n",
        "best_xgb.fit(X_train_selected, y_train)\n",
        "\n",
        "# Hyperparameter tuning for LightGBM\n",
        "best_lgb = lgb.LGBMClassifier(random_state=42)\n",
        "best_lgb.fit(X_train_selected, y_train)\n",
        "\n",
        "# Hyperparameter tuning for CatBoost\n",
        "best_cat = cb.CatBoostClassifier(verbose=0, random_state=42)\n",
        "best_cat.fit(X_train_selected, y_train)\n",
        "\n",
        "# Stacking Classifier (Combining models)\n",
        "stacking_clf = StackingClassifier(\n",
        "    estimators=[\n",
        "        (\"RandomForest\", best_rf.best_estimator_),\n",
        "        (\"XGBoost\", best_xgb),\n",
        "        (\"LightGBM\", best_lgb),\n",
        "        (\"CatBoost\", best_cat)\n",
        "    ],\n",
        "    final_estimator=RandomForestClassifier(n_estimators=500, random_state=42)  # Meta-model\n",
        ")\n",
        "\n",
        "# Train Stacking Classifier\n",
        "stacking_clf.fit(X_train_selected, y_train)\n",
        "y_pred_stack = stacking_clf.predict(X_test_selected)\n",
        "stacking_accuracy = accuracy_score(y_test, y_pred_stack)\n",
        "\n",
        "# Print Results\n",
        "print(f\"ðŸ”— Stacking Ensemble Accuracy: {stacking_accuracy * 100:.2f}%\")\n",
        "\n",
        "# Save Best Model\n",
        "best_model = stacking_clf if stacking_accuracy > 70 else best_rf.best_estimator_  # Choose best model\n",
        "joblib.dump(best_model, \"best_size_model.pkl\")\n",
        "print(\"âœ… Best model saved successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZKwBSpAAxD5b",
        "outputId": "421a2360-14ae-4758-c058-98181f85447f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000944 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 1091\n",
            "[LightGBM] [Info] Number of data points in the train set: 6364, number of used features: 20\n",
            "[LightGBM] [Info] Start training from score -1.604579\n",
            "[LightGBM] [Info] Start training from score -1.614795\n",
            "[LightGBM] [Info] Start training from score -1.608496\n",
            "[LightGBM] [Info] Start training from score -1.610853\n",
            "[LightGBM] [Info] Start training from score -1.608496\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000706 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 1091\n",
            "[LightGBM] [Info] Number of data points in the train set: 5091, number of used features: 20\n",
            "[LightGBM] [Info] Start training from score -1.604735\n",
            "[LightGBM] [Info] Start training from score -1.614558\n",
            "[LightGBM] [Info] Start training from score -1.608653\n",
            "[LightGBM] [Info] Start training from score -1.610617\n",
            "[LightGBM] [Info] Start training from score -1.608653\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000722 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 1091\n",
            "[LightGBM] [Info] Number of data points in the train set: 5091, number of used features: 20\n",
            "[LightGBM] [Info] Start training from score -1.604735\n",
            "[LightGBM] [Info] Start training from score -1.614558\n",
            "[LightGBM] [Info] Start training from score -1.608653\n",
            "[LightGBM] [Info] Start training from score -1.610617\n",
            "[LightGBM] [Info] Start training from score -1.608653\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001046 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 1091\n",
            "[LightGBM] [Info] Number of data points in the train set: 5091, number of used features: 20\n",
            "[LightGBM] [Info] Start training from score -1.604735\n",
            "[LightGBM] [Info] Start training from score -1.614558\n",
            "[LightGBM] [Info] Start training from score -1.608653\n",
            "[LightGBM] [Info] Start training from score -1.610617\n",
            "[LightGBM] [Info] Start training from score -1.608653\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000897 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 1091\n",
            "[LightGBM] [Info] Number of data points in the train set: 5091, number of used features: 20\n",
            "[LightGBM] [Info] Start training from score -1.604735\n",
            "[LightGBM] [Info] Start training from score -1.614558\n",
            "[LightGBM] [Info] Start training from score -1.608653\n",
            "[LightGBM] [Info] Start training from score -1.610617\n",
            "[LightGBM] [Info] Start training from score -1.608653\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001026 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 1091\n",
            "[LightGBM] [Info] Number of data points in the train set: 5092, number of used features: 20\n",
            "[LightGBM] [Info] Start training from score -1.603954\n",
            "[LightGBM] [Info] Start training from score -1.615742\n",
            "[LightGBM] [Info] Start training from score -1.607868\n",
            "[LightGBM] [Info] Start training from score -1.611797\n",
            "[LightGBM] [Info] Start training from score -1.607868\n",
            "ðŸ”— **Stacking Model Accuracy: 61.91%**\n",
            "âœ… **Stacking Model Saved Successfully!**\n"
          ]
        }
      ],
      "source": [
        "from sklearn.ensemble import StackingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import xgboost as xgb\n",
        "import lightgbm as lgb\n",
        "import joblib\n",
        "\n",
        "# Step 1ï¸âƒ£: Train RandomForest to Get Feature Importance\n",
        "rf_temp = RandomForestClassifier(n_estimators=200, random_state=42)\n",
        "rf_temp.fit(X_train, y_train)\n",
        "\n",
        "# Step 2ï¸âƒ£: Select Top 20 Features\n",
        "feature_importance = pd.DataFrame({\n",
        "    \"Feature\": feature_names,\n",
        "    \"Importance\": rf_temp.feature_importances_\n",
        "}).sort_values(by=\"Importance\", ascending=False)\n",
        "\n",
        "selected_features = feature_importance.nlargest(20, \"Importance\")[\"Feature\"].tolist()\n",
        "X_train_selected = pd.DataFrame(X_train, columns=feature_names)[selected_features]\n",
        "X_test_selected = pd.DataFrame(X_test, columns=feature_names)[selected_features]\n",
        "\n",
        "# Step 3ï¸âƒ£: Define Stronger Models\n",
        "best_rf = RandomForestClassifier(n_estimators=500, max_depth=30, min_samples_split=5, random_state=42)\n",
        "best_xgb = xgb.XGBClassifier(n_estimators=300, max_depth=12, learning_rate=0.05, random_state=42)\n",
        "best_lgb = lgb.LGBMClassifier(n_estimators=300, learning_rate=0.03, max_depth=20, random_state=42)\n",
        "\n",
        "# Step 4ï¸âƒ£: Create Stacking Model\n",
        "stacking_clf = StackingClassifier(\n",
        "    estimators=[\n",
        "        ('RandomForest', best_rf),\n",
        "        ('XGBoost', best_xgb),\n",
        "        ('LightGBM', best_lgb)\n",
        "    ],\n",
        "    final_estimator=LogisticRegression(max_iter=1000)\n",
        ")\n",
        "\n",
        "# Step 5ï¸âƒ£: Train & Evaluate\n",
        "stacking_clf.fit(X_train_selected, y_train)\n",
        "y_pred_stack = stacking_clf.predict(X_test_selected)\n",
        "stacking_accuracy = accuracy_score(y_test, y_pred_stack)\n",
        "\n",
        "print(f\"ðŸ”— **Stacking Model Accuracy: {stacking_accuracy * 100:.2f}%**\")\n",
        "joblib.dump(stacking_clf, \"best_size_model.pkl\")\n",
        "print(\"âœ… **Stacking Model Saved Successfully!**\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from sklearn.ensemble import StackingClassifier\n",
        "import joblib\n",
        "\n",
        "# âœ… Step 1ï¸âƒ£: Train a Neural Network\n",
        "def build_ann(input_dim):\n",
        "    model = keras.Sequential([\n",
        "        layers.Dense(64, activation=\"relu\", input_shape=(input_dim,)),\n",
        "        layers.Dense(32, activation=\"relu\"),\n",
        "        layers.Dense(16, activation=\"relu\"),\n",
        "        layers.Dense(len(set(y_train)), activation=\"softmax\")  # Multi-class output\n",
        "    ])\n",
        "    model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "    return model\n",
        "\n",
        "ann_model = build_ann(X_train_selected.shape[1])\n",
        "ann_model.fit(X_train_selected, y_train, epochs=50, batch_size=32, verbose=1, validation_data=(X_test_selected, y_test))\n",
        "\n",
        "# âœ… Step 2ï¸âƒ£: Train Stacking Model with Neural Network\n",
        "stacking_clf = StackingClassifier(\n",
        "    estimators=[\n",
        "        ('RandomForest', best_rf),\n",
        "        ('XGBoost', best_xgb),\n",
        "        ('LightGBM', best_lgb)\n",
        "    ],\n",
        "    final_estimator=xgb.XGBClassifier(n_estimators=200, learning_rate=0.05, random_state=42)  # Use XGBoost as meta-learner\n",
        ")\n",
        "\n",
        "stacking_clf.fit(X_train_selected, y_train)\n",
        "y_pred_stack = stacking_clf.predict(X_test_selected)\n",
        "stacking_accuracy = accuracy_score(y_test, y_pred_stack)\n",
        "\n",
        "# âœ… Step 3ï¸âƒ£: Evaluate & Save the Best Model\n",
        "print(f\"ðŸ”— **Stacking + ANN Model Accuracy: {stacking_accuracy * 100:.2f}%**\")\n",
        "joblib.dump(stacking_clf, \"best_stacking_model.pkl\")\n",
        "print(\"âœ… **Stacking + ANN Model Saved Successfully!**\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wTFZbEeosSd6",
        "outputId": "bd6626b3-8b04-4de3-abb6-242138a63d1f"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "\u001b[1m199/199\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.1906 - loss: 1.6259 - val_accuracy: 0.2407 - val_loss: 1.5954\n",
            "Epoch 2/50\n",
            "\u001b[1m199/199\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.2641 - loss: 1.5831 - val_accuracy: 0.2678 - val_loss: 1.5731\n",
            "Epoch 3/50\n",
            "\u001b[1m199/199\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.2849 - loss: 1.5515 - val_accuracy: 0.2696 - val_loss: 1.5609\n",
            "Epoch 4/50\n",
            "\u001b[1m199/199\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.3214 - loss: 1.5251 - val_accuracy: 0.2866 - val_loss: 1.5438\n",
            "Epoch 5/50\n",
            "\u001b[1m199/199\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.3230 - loss: 1.5031 - val_accuracy: 0.2998 - val_loss: 1.5354\n",
            "Epoch 6/50\n",
            "\u001b[1m199/199\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.3385 - loss: 1.4917 - val_accuracy: 0.3080 - val_loss: 1.5353\n",
            "Epoch 7/50\n",
            "\u001b[1m199/199\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.3428 - loss: 1.4850 - val_accuracy: 0.3130 - val_loss: 1.5273\n",
            "Epoch 8/50\n",
            "\u001b[1m199/199\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.3612 - loss: 1.4589 - val_accuracy: 0.3218 - val_loss: 1.5243\n",
            "Epoch 9/50\n",
            "\u001b[1m199/199\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.3767 - loss: 1.4442 - val_accuracy: 0.3206 - val_loss: 1.5294\n",
            "Epoch 10/50\n",
            "\u001b[1m199/199\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.3849 - loss: 1.4231 - val_accuracy: 0.3155 - val_loss: 1.5320\n",
            "Epoch 11/50\n",
            "\u001b[1m199/199\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.3765 - loss: 1.4233 - val_accuracy: 0.3312 - val_loss: 1.5240\n",
            "Epoch 12/50\n",
            "\u001b[1m199/199\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.3823 - loss: 1.4141 - val_accuracy: 0.3237 - val_loss: 1.5251\n",
            "Epoch 13/50\n",
            "\u001b[1m199/199\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.3939 - loss: 1.3928 - val_accuracy: 0.3212 - val_loss: 1.5218\n",
            "Epoch 14/50\n",
            "\u001b[1m199/199\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.4183 - loss: 1.3734 - val_accuracy: 0.3319 - val_loss: 1.5265\n",
            "Epoch 15/50\n",
            "\u001b[1m199/199\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.4275 - loss: 1.3603 - val_accuracy: 0.3224 - val_loss: 1.5243\n",
            "Epoch 16/50\n",
            "\u001b[1m199/199\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.4330 - loss: 1.3375 - val_accuracy: 0.3130 - val_loss: 1.5393\n",
            "Epoch 17/50\n",
            "\u001b[1m199/199\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.4260 - loss: 1.3425 - val_accuracy: 0.3325 - val_loss: 1.5316\n",
            "Epoch 18/50\n",
            "\u001b[1m199/199\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.4539 - loss: 1.3125 - val_accuracy: 0.3243 - val_loss: 1.5357\n",
            "Epoch 19/50\n",
            "\u001b[1m199/199\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.4471 - loss: 1.3174 - val_accuracy: 0.3294 - val_loss: 1.5448\n",
            "Epoch 20/50\n",
            "\u001b[1m199/199\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.4620 - loss: 1.2941 - val_accuracy: 0.3250 - val_loss: 1.5541\n",
            "Epoch 21/50\n",
            "\u001b[1m199/199\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.4737 - loss: 1.2728 - val_accuracy: 0.3243 - val_loss: 1.5818\n",
            "Epoch 22/50\n",
            "\u001b[1m199/199\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.4672 - loss: 1.2789 - val_accuracy: 0.3262 - val_loss: 1.5578\n",
            "Epoch 23/50\n",
            "\u001b[1m199/199\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.4993 - loss: 1.2424 - val_accuracy: 0.3256 - val_loss: 1.5530\n",
            "Epoch 24/50\n",
            "\u001b[1m199/199\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.4952 - loss: 1.2465 - val_accuracy: 0.3250 - val_loss: 1.5816\n",
            "Epoch 25/50\n",
            "\u001b[1m199/199\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.4920 - loss: 1.2407 - val_accuracy: 0.3256 - val_loss: 1.5851\n",
            "Epoch 26/50\n",
            "\u001b[1m199/199\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.4885 - loss: 1.2509 - val_accuracy: 0.3187 - val_loss: 1.5849\n",
            "Epoch 27/50\n",
            "\u001b[1m199/199\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5199 - loss: 1.2056 - val_accuracy: 0.3268 - val_loss: 1.5804\n",
            "Epoch 28/50\n",
            "\u001b[1m199/199\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5086 - loss: 1.2089 - val_accuracy: 0.3268 - val_loss: 1.5999\n",
            "Epoch 29/50\n",
            "\u001b[1m199/199\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.4930 - loss: 1.2212 - val_accuracy: 0.3306 - val_loss: 1.6056\n",
            "Epoch 30/50\n",
            "\u001b[1m199/199\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5168 - loss: 1.1936 - val_accuracy: 0.3369 - val_loss: 1.5950\n",
            "Epoch 31/50\n",
            "\u001b[1m199/199\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5308 - loss: 1.1783 - val_accuracy: 0.3363 - val_loss: 1.6143\n",
            "Epoch 32/50\n",
            "\u001b[1m199/199\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5305 - loss: 1.1740 - val_accuracy: 0.3300 - val_loss: 1.6235\n",
            "Epoch 33/50\n",
            "\u001b[1m199/199\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.5196 - loss: 1.1856 - val_accuracy: 0.3338 - val_loss: 1.6202\n",
            "Epoch 34/50\n",
            "\u001b[1m199/199\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.5235 - loss: 1.1662 - val_accuracy: 0.3218 - val_loss: 1.6280\n",
            "Epoch 35/50\n",
            "\u001b[1m199/199\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 15ms/step - accuracy: 0.5357 - loss: 1.1464 - val_accuracy: 0.3350 - val_loss: 1.6338\n",
            "Epoch 36/50\n",
            "\u001b[1m199/199\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.5404 - loss: 1.1415 - val_accuracy: 0.3275 - val_loss: 1.6402\n",
            "Epoch 37/50\n",
            "\u001b[1m199/199\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5310 - loss: 1.1526 - val_accuracy: 0.3275 - val_loss: 1.6441\n",
            "Epoch 38/50\n",
            "\u001b[1m199/199\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5350 - loss: 1.1541 - val_accuracy: 0.3294 - val_loss: 1.6504\n",
            "Epoch 39/50\n",
            "\u001b[1m199/199\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5343 - loss: 1.1356 - val_accuracy: 0.3256 - val_loss: 1.6602\n",
            "Epoch 40/50\n",
            "\u001b[1m199/199\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5326 - loss: 1.1503 - val_accuracy: 0.3206 - val_loss: 1.6630\n",
            "Epoch 41/50\n",
            "\u001b[1m199/199\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5305 - loss: 1.1418 - val_accuracy: 0.3281 - val_loss: 1.6784\n",
            "Epoch 42/50\n",
            "\u001b[1m199/199\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5513 - loss: 1.1170 - val_accuracy: 0.3130 - val_loss: 1.6807\n",
            "Epoch 43/50\n",
            "\u001b[1m199/199\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5506 - loss: 1.1071 - val_accuracy: 0.3268 - val_loss: 1.6987\n",
            "Epoch 44/50\n",
            "\u001b[1m199/199\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5462 - loss: 1.1102 - val_accuracy: 0.3162 - val_loss: 1.6773\n",
            "Epoch 45/50\n",
            "\u001b[1m199/199\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5564 - loss: 1.1045 - val_accuracy: 0.3275 - val_loss: 1.6930\n",
            "Epoch 46/50\n",
            "\u001b[1m199/199\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.5526 - loss: 1.0987 - val_accuracy: 0.3275 - val_loss: 1.6971\n",
            "Epoch 47/50\n",
            "\u001b[1m199/199\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.5600 - loss: 1.0903 - val_accuracy: 0.3369 - val_loss: 1.7023\n",
            "Epoch 48/50\n",
            "\u001b[1m199/199\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5655 - loss: 1.0728 - val_accuracy: 0.3356 - val_loss: 1.7296\n",
            "Epoch 49/50\n",
            "\u001b[1m199/199\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5623 - loss: 1.0881 - val_accuracy: 0.3319 - val_loss: 1.7178\n",
            "Epoch 50/50\n",
            "\u001b[1m199/199\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5795 - loss: 1.0649 - val_accuracy: 0.3312 - val_loss: 1.7206\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000930 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 1091\n",
            "[LightGBM] [Info] Number of data points in the train set: 6364, number of used features: 20\n",
            "[LightGBM] [Info] Start training from score -1.604579\n",
            "[LightGBM] [Info] Start training from score -1.614795\n",
            "[LightGBM] [Info] Start training from score -1.608496\n",
            "[LightGBM] [Info] Start training from score -1.610853\n",
            "[LightGBM] [Info] Start training from score -1.608496\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000764 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 1091\n",
            "[LightGBM] [Info] Number of data points in the train set: 5091, number of used features: 20\n",
            "[LightGBM] [Info] Start training from score -1.604735\n",
            "[LightGBM] [Info] Start training from score -1.614558\n",
            "[LightGBM] [Info] Start training from score -1.608653\n",
            "[LightGBM] [Info] Start training from score -1.610617\n",
            "[LightGBM] [Info] Start training from score -1.608653\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001072 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 1091\n",
            "[LightGBM] [Info] Number of data points in the train set: 5091, number of used features: 20\n",
            "[LightGBM] [Info] Start training from score -1.604735\n",
            "[LightGBM] [Info] Start training from score -1.614558\n",
            "[LightGBM] [Info] Start training from score -1.608653\n",
            "[LightGBM] [Info] Start training from score -1.610617\n",
            "[LightGBM] [Info] Start training from score -1.608653\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000730 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 1091\n",
            "[LightGBM] [Info] Number of data points in the train set: 5091, number of used features: 20\n",
            "[LightGBM] [Info] Start training from score -1.604735\n",
            "[LightGBM] [Info] Start training from score -1.614558\n",
            "[LightGBM] [Info] Start training from score -1.608653\n",
            "[LightGBM] [Info] Start training from score -1.610617\n",
            "[LightGBM] [Info] Start training from score -1.608653\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000770 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 1091\n",
            "[LightGBM] [Info] Number of data points in the train set: 5091, number of used features: 20\n",
            "[LightGBM] [Info] Start training from score -1.604735\n",
            "[LightGBM] [Info] Start training from score -1.614558\n",
            "[LightGBM] [Info] Start training from score -1.608653\n",
            "[LightGBM] [Info] Start training from score -1.610617\n",
            "[LightGBM] [Info] Start training from score -1.608653\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000687 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 1091\n",
            "[LightGBM] [Info] Number of data points in the train set: 5092, number of used features: 20\n",
            "[LightGBM] [Info] Start training from score -1.603954\n",
            "[LightGBM] [Info] Start training from score -1.615742\n",
            "[LightGBM] [Info] Start training from score -1.607868\n",
            "[LightGBM] [Info] Start training from score -1.611797\n",
            "[LightGBM] [Info] Start training from score -1.607868\n",
            "ðŸ”— **Stacking + ANN Model Accuracy: 61.09%**\n",
            "âœ… **Stacking + ANN Model Saved Successfully!**\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from sklearn.ensemble import StackingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import joblib\n",
        "\n",
        "# âœ… Step 1ï¸âƒ£: Improved Neural Network Model\n",
        "def build_optimized_ann(input_dim):\n",
        "    model = keras.Sequential([\n",
        "        layers.Input(shape=(input_dim,)),  # Proper input layer\n",
        "        layers.Dense(128, activation=\"relu\"),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.Dropout(0.3),  # Prevent overfitting\n",
        "        layers.Dense(64, activation=\"relu\"),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.Dropout(0.3),\n",
        "        layers.Dense(32, activation=\"relu\"),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.Dense(len(set(y_train)), activation=\"softmax\")  # Multi-class classification\n",
        "    ])\n",
        "    model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.001),  # Lower learning rate\n",
        "                  loss=\"sparse_categorical_crossentropy\",\n",
        "                  metrics=[\"accuracy\"])\n",
        "    return model\n",
        "\n",
        "ann_model = build_optimized_ann(X_train_selected.shape[1])\n",
        "ann_model.fit(X_train_selected, y_train, epochs=100, batch_size=64, verbose=1, validation_data=(X_test_selected, y_test))\n",
        "\n",
        "# âœ… Step 2ï¸âƒ£: Train an Optimized Stacking Model\n",
        "stacking_clf = StackingClassifier(\n",
        "    estimators=[\n",
        "        ('RandomForest', best_rf),\n",
        "        ('XGBoost', best_xgb),\n",
        "        ('LightGBM', best_lgb),\n",
        "        ('CatBoost', best_cat)\n",
        "    ],\n",
        "    final_estimator=LogisticRegression(),  # Using Logistic Regression as meta-learner\n",
        "    stack_method='predict_proba'  # Use probabilities instead of hard labels\n",
        ")\n",
        "\n",
        "stacking_clf.fit(X_train_selected, y_train)\n",
        "y_pred_stack = stacking_clf.predict(X_test_selected)\n",
        "stacking_accuracy = accuracy_score(y_test, y_pred_stack)\n",
        "\n",
        "# âœ… Step 3ï¸âƒ£: Evaluate & Save the Best Model\n",
        "print(f\"ðŸ”— **Stacking + ANN Optimized Accuracy: {stacking_accuracy * 100:.2f}%**\")\n",
        "joblib.dump(stacking_clf, \"best_optimized_stacking_model.pkl\")\n",
        "print(\"âœ… **Optimized Stacking Model Saved Successfully!**\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "exsNoRG_touy",
        "outputId": "d272878d-5378-4222-c53c-efcd649cb10e"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - accuracy: 0.2163 - loss: 2.0109 - val_accuracy: 0.2558 - val_loss: 1.5919\n",
            "Epoch 2/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.2353 - loss: 1.7257 - val_accuracy: 0.2602 - val_loss: 1.5807\n",
            "Epoch 3/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.2609 - loss: 1.6313 - val_accuracy: 0.2797 - val_loss: 1.5681\n",
            "Epoch 4/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.2488 - loss: 1.6083 - val_accuracy: 0.2847 - val_loss: 1.5605\n",
            "Epoch 5/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.2641 - loss: 1.5860 - val_accuracy: 0.2992 - val_loss: 1.5539\n",
            "Epoch 6/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.2884 - loss: 1.5693 - val_accuracy: 0.3067 - val_loss: 1.5436\n",
            "Epoch 7/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.2833 - loss: 1.5587 - val_accuracy: 0.3086 - val_loss: 1.5385\n",
            "Epoch 8/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.3143 - loss: 1.5381 - val_accuracy: 0.3086 - val_loss: 1.5340\n",
            "Epoch 9/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.3009 - loss: 1.5311 - val_accuracy: 0.3011 - val_loss: 1.5275\n",
            "Epoch 10/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.2900 - loss: 1.5380 - val_accuracy: 0.3130 - val_loss: 1.5313\n",
            "Epoch 11/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.3067 - loss: 1.5302 - val_accuracy: 0.3212 - val_loss: 1.5292\n",
            "Epoch 12/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.3178 - loss: 1.5197 - val_accuracy: 0.3155 - val_loss: 1.5218\n",
            "Epoch 13/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.3246 - loss: 1.5152 - val_accuracy: 0.3224 - val_loss: 1.5236\n",
            "Epoch 14/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.3230 - loss: 1.5073 - val_accuracy: 0.3199 - val_loss: 1.5211\n",
            "Epoch 15/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.3170 - loss: 1.5222 - val_accuracy: 0.3243 - val_loss: 1.5168\n",
            "Epoch 16/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.3213 - loss: 1.5135 - val_accuracy: 0.3162 - val_loss: 1.5138\n",
            "Epoch 17/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.3222 - loss: 1.5067 - val_accuracy: 0.3149 - val_loss: 1.5128\n",
            "Epoch 18/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.3317 - loss: 1.4944 - val_accuracy: 0.3237 - val_loss: 1.5075\n",
            "Epoch 19/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.3321 - loss: 1.5025 - val_accuracy: 0.3168 - val_loss: 1.5096\n",
            "Epoch 20/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.3280 - loss: 1.5019 - val_accuracy: 0.3224 - val_loss: 1.5095\n",
            "Epoch 21/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.3314 - loss: 1.4938 - val_accuracy: 0.3243 - val_loss: 1.5068\n",
            "Epoch 22/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.3391 - loss: 1.4843 - val_accuracy: 0.3287 - val_loss: 1.5019\n",
            "Epoch 23/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.3355 - loss: 1.4875 - val_accuracy: 0.3338 - val_loss: 1.5019\n",
            "Epoch 24/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.3491 - loss: 1.4836 - val_accuracy: 0.3325 - val_loss: 1.5011\n",
            "Epoch 25/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.3479 - loss: 1.4845 - val_accuracy: 0.3413 - val_loss: 1.4994\n",
            "Epoch 26/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.3511 - loss: 1.4742 - val_accuracy: 0.3470 - val_loss: 1.4961\n",
            "Epoch 27/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.3557 - loss: 1.4662 - val_accuracy: 0.3482 - val_loss: 1.4962\n",
            "Epoch 28/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.3466 - loss: 1.4694 - val_accuracy: 0.3306 - val_loss: 1.4982\n",
            "Epoch 29/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.3549 - loss: 1.4627 - val_accuracy: 0.3432 - val_loss: 1.4934\n",
            "Epoch 30/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.3587 - loss: 1.4595 - val_accuracy: 0.3457 - val_loss: 1.4936\n",
            "Epoch 31/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.3589 - loss: 1.4576 - val_accuracy: 0.3463 - val_loss: 1.4918\n",
            "Epoch 32/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.3511 - loss: 1.4678 - val_accuracy: 0.3451 - val_loss: 1.4946\n",
            "Epoch 33/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.3702 - loss: 1.4602 - val_accuracy: 0.3488 - val_loss: 1.4910\n",
            "Epoch 34/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.3681 - loss: 1.4553 - val_accuracy: 0.3476 - val_loss: 1.4897\n",
            "Epoch 35/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.3714 - loss: 1.4458 - val_accuracy: 0.3526 - val_loss: 1.4849\n",
            "Epoch 36/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.3681 - loss: 1.4478 - val_accuracy: 0.3470 - val_loss: 1.4811\n",
            "Epoch 37/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.3584 - loss: 1.4515 - val_accuracy: 0.3501 - val_loss: 1.4835\n",
            "Epoch 38/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.3824 - loss: 1.4363 - val_accuracy: 0.3457 - val_loss: 1.4883\n",
            "Epoch 39/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.3661 - loss: 1.4509 - val_accuracy: 0.3576 - val_loss: 1.4873\n",
            "Epoch 40/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.3806 - loss: 1.4311 - val_accuracy: 0.3627 - val_loss: 1.4796\n",
            "Epoch 41/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.3927 - loss: 1.4193 - val_accuracy: 0.3495 - val_loss: 1.4809\n",
            "Epoch 42/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.3853 - loss: 1.4266 - val_accuracy: 0.3576 - val_loss: 1.4808\n",
            "Epoch 43/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.3825 - loss: 1.4296 - val_accuracy: 0.3570 - val_loss: 1.4789\n",
            "Epoch 44/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.3770 - loss: 1.4222 - val_accuracy: 0.3539 - val_loss: 1.4826\n",
            "Epoch 45/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.3880 - loss: 1.4171 - val_accuracy: 0.3620 - val_loss: 1.4772\n",
            "Epoch 46/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.3941 - loss: 1.4164 - val_accuracy: 0.3715 - val_loss: 1.4787\n",
            "Epoch 47/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.3782 - loss: 1.4228 - val_accuracy: 0.3564 - val_loss: 1.4748\n",
            "Epoch 48/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.3675 - loss: 1.4356 - val_accuracy: 0.3526 - val_loss: 1.4739\n",
            "Epoch 49/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.3909 - loss: 1.4226 - val_accuracy: 0.3482 - val_loss: 1.4760\n",
            "Epoch 50/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.3737 - loss: 1.4164 - val_accuracy: 0.3545 - val_loss: 1.4784\n",
            "Epoch 51/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.3814 - loss: 1.4235 - val_accuracy: 0.3545 - val_loss: 1.4758\n",
            "Epoch 52/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.3943 - loss: 1.4090 - val_accuracy: 0.3551 - val_loss: 1.4715\n",
            "Epoch 53/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.3798 - loss: 1.4167 - val_accuracy: 0.3482 - val_loss: 1.4730\n",
            "Epoch 54/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.3969 - loss: 1.4049 - val_accuracy: 0.3451 - val_loss: 1.4764\n",
            "Epoch 55/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.3930 - loss: 1.4115 - val_accuracy: 0.3463 - val_loss: 1.4785\n",
            "Epoch 56/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.3978 - loss: 1.3992 - val_accuracy: 0.3539 - val_loss: 1.4713\n",
            "Epoch 57/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.4039 - loss: 1.3950 - val_accuracy: 0.3564 - val_loss: 1.4767\n",
            "Epoch 58/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.4031 - loss: 1.3986 - val_accuracy: 0.3583 - val_loss: 1.4725\n",
            "Epoch 59/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.3922 - loss: 1.4033 - val_accuracy: 0.3558 - val_loss: 1.4788\n",
            "Epoch 60/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.3941 - loss: 1.4010 - val_accuracy: 0.3702 - val_loss: 1.4711\n",
            "Epoch 61/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.3997 - loss: 1.4062 - val_accuracy: 0.3520 - val_loss: 1.4769\n",
            "Epoch 62/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.3882 - loss: 1.4034 - val_accuracy: 0.3620 - val_loss: 1.4730\n",
            "Epoch 63/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.4038 - loss: 1.3884 - val_accuracy: 0.3595 - val_loss: 1.4764\n",
            "Epoch 64/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.4150 - loss: 1.3824 - val_accuracy: 0.3520 - val_loss: 1.4721\n",
            "Epoch 65/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.4083 - loss: 1.3889 - val_accuracy: 0.3526 - val_loss: 1.4787\n",
            "Epoch 66/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.4224 - loss: 1.3754 - val_accuracy: 0.3545 - val_loss: 1.4764\n",
            "Epoch 67/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.4120 - loss: 1.3708 - val_accuracy: 0.3620 - val_loss: 1.4742\n",
            "Epoch 68/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.4025 - loss: 1.3830 - val_accuracy: 0.3570 - val_loss: 1.4734\n",
            "Epoch 69/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.4108 - loss: 1.3754 - val_accuracy: 0.3526 - val_loss: 1.4727\n",
            "Epoch 70/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.3912 - loss: 1.3846 - val_accuracy: 0.3576 - val_loss: 1.4724\n",
            "Epoch 71/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.4203 - loss: 1.3677 - val_accuracy: 0.3589 - val_loss: 1.4749\n",
            "Epoch 72/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.4083 - loss: 1.3818 - val_accuracy: 0.3507 - val_loss: 1.4737\n",
            "Epoch 73/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.4216 - loss: 1.3628 - val_accuracy: 0.3551 - val_loss: 1.4718\n",
            "Epoch 74/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.4112 - loss: 1.3721 - val_accuracy: 0.3539 - val_loss: 1.4709\n",
            "Epoch 75/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.4075 - loss: 1.3803 - val_accuracy: 0.3677 - val_loss: 1.4661\n",
            "Epoch 76/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.4217 - loss: 1.3551 - val_accuracy: 0.3664 - val_loss: 1.4660\n",
            "Epoch 77/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.4074 - loss: 1.3730 - val_accuracy: 0.3627 - val_loss: 1.4712\n",
            "Epoch 78/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.4217 - loss: 1.3658 - val_accuracy: 0.3721 - val_loss: 1.4643\n",
            "Epoch 79/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.4096 - loss: 1.3679 - val_accuracy: 0.3752 - val_loss: 1.4663\n",
            "Epoch 80/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.4335 - loss: 1.3412 - val_accuracy: 0.3633 - val_loss: 1.4657\n",
            "Epoch 81/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.4316 - loss: 1.3525 - val_accuracy: 0.3658 - val_loss: 1.4701\n",
            "Epoch 82/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.4308 - loss: 1.3626 - val_accuracy: 0.3589 - val_loss: 1.4688\n",
            "Epoch 83/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.4281 - loss: 1.3494 - val_accuracy: 0.3740 - val_loss: 1.4656\n",
            "Epoch 84/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.4245 - loss: 1.3775 - val_accuracy: 0.3777 - val_loss: 1.4616\n",
            "Epoch 85/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.4237 - loss: 1.3540 - val_accuracy: 0.3671 - val_loss: 1.4619\n",
            "Epoch 86/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.4159 - loss: 1.3696 - val_accuracy: 0.3658 - val_loss: 1.4624\n",
            "Epoch 87/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.4275 - loss: 1.3560 - val_accuracy: 0.3658 - val_loss: 1.4686\n",
            "Epoch 88/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.4208 - loss: 1.3449 - val_accuracy: 0.3602 - val_loss: 1.4701\n",
            "Epoch 89/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.4313 - loss: 1.3529 - val_accuracy: 0.3488 - val_loss: 1.4668\n",
            "Epoch 90/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.4231 - loss: 1.3556 - val_accuracy: 0.3633 - val_loss: 1.4660\n",
            "Epoch 91/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.4271 - loss: 1.3534 - val_accuracy: 0.3558 - val_loss: 1.4637\n",
            "Epoch 92/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.4267 - loss: 1.3409 - val_accuracy: 0.3627 - val_loss: 1.4608\n",
            "Epoch 93/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.4177 - loss: 1.3593 - val_accuracy: 0.3639 - val_loss: 1.4595\n",
            "Epoch 94/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.4382 - loss: 1.3313 - val_accuracy: 0.3765 - val_loss: 1.4611\n",
            "Epoch 95/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.4138 - loss: 1.3487 - val_accuracy: 0.3652 - val_loss: 1.4652\n",
            "Epoch 96/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.4321 - loss: 1.3456 - val_accuracy: 0.3639 - val_loss: 1.4665\n",
            "Epoch 97/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.4292 - loss: 1.3447 - val_accuracy: 0.3671 - val_loss: 1.4642\n",
            "Epoch 98/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.4513 - loss: 1.3269 - val_accuracy: 0.3671 - val_loss: 1.4635\n",
            "Epoch 99/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.4402 - loss: 1.3427 - val_accuracy: 0.3677 - val_loss: 1.4656\n",
            "Epoch 100/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.4378 - loss: 1.3377 - val_accuracy: 0.3734 - val_loss: 1.4639\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000969 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 1091\n",
            "[LightGBM] [Info] Number of data points in the train set: 6364, number of used features: 20\n",
            "[LightGBM] [Info] Start training from score -1.604579\n",
            "[LightGBM] [Info] Start training from score -1.614795\n",
            "[LightGBM] [Info] Start training from score -1.608496\n",
            "[LightGBM] [Info] Start training from score -1.610853\n",
            "[LightGBM] [Info] Start training from score -1.608496\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000776 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 1091\n",
            "[LightGBM] [Info] Number of data points in the train set: 5091, number of used features: 20\n",
            "[LightGBM] [Info] Start training from score -1.604735\n",
            "[LightGBM] [Info] Start training from score -1.614558\n",
            "[LightGBM] [Info] Start training from score -1.608653\n",
            "[LightGBM] [Info] Start training from score -1.610617\n",
            "[LightGBM] [Info] Start training from score -1.608653\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000693 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 1091\n",
            "[LightGBM] [Info] Number of data points in the train set: 5091, number of used features: 20\n",
            "[LightGBM] [Info] Start training from score -1.604735\n",
            "[LightGBM] [Info] Start training from score -1.614558\n",
            "[LightGBM] [Info] Start training from score -1.608653\n",
            "[LightGBM] [Info] Start training from score -1.610617\n",
            "[LightGBM] [Info] Start training from score -1.608653\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000759 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 1091\n",
            "[LightGBM] [Info] Number of data points in the train set: 5091, number of used features: 20\n",
            "[LightGBM] [Info] Start training from score -1.604735\n",
            "[LightGBM] [Info] Start training from score -1.614558\n",
            "[LightGBM] [Info] Start training from score -1.608653\n",
            "[LightGBM] [Info] Start training from score -1.610617\n",
            "[LightGBM] [Info] Start training from score -1.608653\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000697 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 1091\n",
            "[LightGBM] [Info] Number of data points in the train set: 5091, number of used features: 20\n",
            "[LightGBM] [Info] Start training from score -1.604735\n",
            "[LightGBM] [Info] Start training from score -1.614558\n",
            "[LightGBM] [Info] Start training from score -1.608653\n",
            "[LightGBM] [Info] Start training from score -1.610617\n",
            "[LightGBM] [Info] Start training from score -1.608653\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001078 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 1091\n",
            "[LightGBM] [Info] Number of data points in the train set: 5092, number of used features: 20\n",
            "[LightGBM] [Info] Start training from score -1.603954\n",
            "[LightGBM] [Info] Start training from score -1.615742\n",
            "[LightGBM] [Info] Start training from score -1.607868\n",
            "[LightGBM] [Info] Start training from score -1.611797\n",
            "[LightGBM] [Info] Start training from score -1.607868\n",
            "ðŸ”— **Stacking + ANN Optimized Accuracy: 61.91%**\n",
            "âœ… **Optimized Stacking Model Saved Successfully!**\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from sklearn.ensemble import StackingClassifier\n",
        "from xgboost import XGBClassifier\n",
        "import joblib\n",
        "\n",
        "# âœ… Step 1ï¸âƒ£: Optimized Neural Network\n",
        "def build_better_ann(input_dim):\n",
        "    model = keras.Sequential([\n",
        "        layers.Input(shape=(input_dim,)),\n",
        "        layers.Dense(256, activation=\"relu\"),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.Dropout(0.3),\n",
        "        layers.Dense(128, activation=\"relu\"),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.Dropout(0.3),\n",
        "        layers.Dense(64, activation=\"relu\"),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.Dense(len(set(y_train)), activation=\"softmax\")  # Multi-class classification\n",
        "    ])\n",
        "    model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.0005),  # Lower learning rate\n",
        "                  loss=\"sparse_categorical_crossentropy\",\n",
        "                  metrics=[\"accuracy\"])\n",
        "    return model\n",
        "\n",
        "ann_model = build_better_ann(X_train_selected.shape[1])\n",
        "ann_model.fit(X_train_selected, y_train, epochs=100, batch_size=64, verbose=1, validation_data=(X_test_selected, y_test))\n",
        "\n",
        "# âœ… Step 2ï¸âƒ£: Use XGBoost as the Meta-Learner\n",
        "stacking_clf = StackingClassifier(\n",
        "    estimators=[\n",
        "        ('RandomForest', best_rf),\n",
        "        ('XGBoost', best_xgb),\n",
        "        ('LightGBM', best_lgb),\n",
        "        ('CatBoost', best_cat)\n",
        "    ],\n",
        "    final_estimator=XGBClassifier(n_estimators=100, learning_rate=0.05),  # Replace Logistic Regression\n",
        "    stack_method='predict_proba'\n",
        ")\n",
        "\n",
        "stacking_clf.fit(X_train_selected, y_train)\n",
        "y_pred_stack = stacking_clf.predict(X_test_selected)\n",
        "stacking_accuracy = accuracy_score(y_test, y_pred_stack)\n",
        "\n",
        "# âœ… Step 3ï¸âƒ£: Evaluate & Save the Best Model\n",
        "print(f\"ðŸ”— **Final Optimized Stacking Accuracy: {stacking_accuracy * 100:.2f}%**\")\n",
        "joblib.dump(stacking_clf, \"best_final_model.pkl\")\n",
        "print(\"âœ… **Final Optimized Stacking Model Saved!**\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-G56rlE9vA_N",
        "outputId": "b59e2f5d-8b36-45d6-948b-da24bbc1f1e7"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - accuracy: 0.2179 - loss: 2.0900 - val_accuracy: 0.2300 - val_loss: 1.5998\n",
            "Epoch 2/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.2592 - loss: 1.7615 - val_accuracy: 0.2407 - val_loss: 1.5836\n",
            "Epoch 3/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.2483 - loss: 1.6752 - val_accuracy: 0.2696 - val_loss: 1.5703\n",
            "Epoch 4/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.2754 - loss: 1.6156 - val_accuracy: 0.2898 - val_loss: 1.5535\n",
            "Epoch 5/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.2818 - loss: 1.5963 - val_accuracy: 0.2992 - val_loss: 1.5440\n",
            "Epoch 6/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.3114 - loss: 1.5632 - val_accuracy: 0.3105 - val_loss: 1.5336\n",
            "Epoch 7/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.3052 - loss: 1.5452 - val_accuracy: 0.3162 - val_loss: 1.5223\n",
            "Epoch 8/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.3175 - loss: 1.5243 - val_accuracy: 0.3174 - val_loss: 1.5204\n",
            "Epoch 9/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.3182 - loss: 1.5191 - val_accuracy: 0.3162 - val_loss: 1.5104\n",
            "Epoch 10/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.3257 - loss: 1.5143 - val_accuracy: 0.3206 - val_loss: 1.5078\n",
            "Epoch 11/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.3310 - loss: 1.5104 - val_accuracy: 0.3180 - val_loss: 1.5023\n",
            "Epoch 12/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.3307 - loss: 1.4978 - val_accuracy: 0.3338 - val_loss: 1.4978\n",
            "Epoch 13/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - accuracy: 0.3548 - loss: 1.4709 - val_accuracy: 0.3369 - val_loss: 1.4928\n",
            "Epoch 14/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.3499 - loss: 1.4712 - val_accuracy: 0.3444 - val_loss: 1.4921\n",
            "Epoch 15/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.3610 - loss: 1.4659 - val_accuracy: 0.3457 - val_loss: 1.4821\n",
            "Epoch 16/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.3493 - loss: 1.4710 - val_accuracy: 0.3369 - val_loss: 1.4855\n",
            "Epoch 17/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.3787 - loss: 1.4459 - val_accuracy: 0.3363 - val_loss: 1.4808\n",
            "Epoch 18/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.3740 - loss: 1.4486 - val_accuracy: 0.3457 - val_loss: 1.4819\n",
            "Epoch 19/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.3642 - loss: 1.4576 - val_accuracy: 0.3507 - val_loss: 1.4782\n",
            "Epoch 20/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.3778 - loss: 1.4373 - val_accuracy: 0.3419 - val_loss: 1.4798\n",
            "Epoch 21/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.3845 - loss: 1.4336 - val_accuracy: 0.3526 - val_loss: 1.4781\n",
            "Epoch 22/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.3767 - loss: 1.4304 - val_accuracy: 0.3457 - val_loss: 1.4785\n",
            "Epoch 23/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.3819 - loss: 1.4343 - val_accuracy: 0.3482 - val_loss: 1.4776\n",
            "Epoch 24/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.3982 - loss: 1.4043 - val_accuracy: 0.3532 - val_loss: 1.4724\n",
            "Epoch 25/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.4032 - loss: 1.4008 - val_accuracy: 0.3488 - val_loss: 1.4707\n",
            "Epoch 26/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.4026 - loss: 1.3999 - val_accuracy: 0.3495 - val_loss: 1.4728\n",
            "Epoch 27/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.4063 - loss: 1.3957 - val_accuracy: 0.3375 - val_loss: 1.4741\n",
            "Epoch 28/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.3920 - loss: 1.3988 - val_accuracy: 0.3652 - val_loss: 1.4685\n",
            "Epoch 29/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.4124 - loss: 1.3849 - val_accuracy: 0.3658 - val_loss: 1.4662\n",
            "Epoch 30/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.3954 - loss: 1.4089 - val_accuracy: 0.3633 - val_loss: 1.4609\n",
            "Epoch 31/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.4092 - loss: 1.3767 - val_accuracy: 0.3564 - val_loss: 1.4627\n",
            "Epoch 32/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.4021 - loss: 1.3968 - val_accuracy: 0.3627 - val_loss: 1.4576\n",
            "Epoch 33/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.4135 - loss: 1.3745 - val_accuracy: 0.3589 - val_loss: 1.4577\n",
            "Epoch 34/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.4254 - loss: 1.3628 - val_accuracy: 0.3551 - val_loss: 1.4579\n",
            "Epoch 35/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 0.4223 - loss: 1.3617 - val_accuracy: 0.3652 - val_loss: 1.4582\n",
            "Epoch 36/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.4195 - loss: 1.3613 - val_accuracy: 0.3646 - val_loss: 1.4524\n",
            "Epoch 37/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.4291 - loss: 1.3447 - val_accuracy: 0.3614 - val_loss: 1.4511\n",
            "Epoch 38/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.4445 - loss: 1.3349 - val_accuracy: 0.3664 - val_loss: 1.4488\n",
            "Epoch 39/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.4316 - loss: 1.3452 - val_accuracy: 0.3664 - val_loss: 1.4453\n",
            "Epoch 40/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.4371 - loss: 1.3387 - val_accuracy: 0.3721 - val_loss: 1.4441\n",
            "Epoch 41/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.4372 - loss: 1.3320 - val_accuracy: 0.3671 - val_loss: 1.4447\n",
            "Epoch 42/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.4447 - loss: 1.3423 - val_accuracy: 0.3752 - val_loss: 1.4420\n",
            "Epoch 43/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.4400 - loss: 1.3270 - val_accuracy: 0.3765 - val_loss: 1.4471\n",
            "Epoch 44/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.4275 - loss: 1.3380 - val_accuracy: 0.3771 - val_loss: 1.4479\n",
            "Epoch 45/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.4489 - loss: 1.3053 - val_accuracy: 0.3671 - val_loss: 1.4507\n",
            "Epoch 46/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.4504 - loss: 1.3175 - val_accuracy: 0.3627 - val_loss: 1.4462\n",
            "Epoch 47/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.4533 - loss: 1.3115 - val_accuracy: 0.3639 - val_loss: 1.4448\n",
            "Epoch 48/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.4496 - loss: 1.3053 - val_accuracy: 0.3727 - val_loss: 1.4436\n",
            "Epoch 49/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.4570 - loss: 1.3004 - val_accuracy: 0.3715 - val_loss: 1.4500\n",
            "Epoch 50/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.4518 - loss: 1.3076 - val_accuracy: 0.3740 - val_loss: 1.4477\n",
            "Epoch 51/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.4577 - loss: 1.2991 - val_accuracy: 0.3708 - val_loss: 1.4468\n",
            "Epoch 52/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.4598 - loss: 1.2959 - val_accuracy: 0.3740 - val_loss: 1.4427\n",
            "Epoch 53/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.4562 - loss: 1.2886 - val_accuracy: 0.3721 - val_loss: 1.4457\n",
            "Epoch 54/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.4569 - loss: 1.2967 - val_accuracy: 0.3740 - val_loss: 1.4494\n",
            "Epoch 55/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.4676 - loss: 1.2775 - val_accuracy: 0.3771 - val_loss: 1.4420\n",
            "Epoch 56/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.4604 - loss: 1.2840 - val_accuracy: 0.3790 - val_loss: 1.4405\n",
            "Epoch 57/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.4619 - loss: 1.2846 - val_accuracy: 0.3865 - val_loss: 1.4331\n",
            "Epoch 58/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.4702 - loss: 1.2827 - val_accuracy: 0.3859 - val_loss: 1.4371\n",
            "Epoch 59/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.4758 - loss: 1.2590 - val_accuracy: 0.3853 - val_loss: 1.4373\n",
            "Epoch 60/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.4693 - loss: 1.2627 - val_accuracy: 0.3916 - val_loss: 1.4266\n",
            "Epoch 61/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.4895 - loss: 1.2354 - val_accuracy: 0.3784 - val_loss: 1.4316\n",
            "Epoch 62/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - accuracy: 0.4890 - loss: 1.2594 - val_accuracy: 0.3884 - val_loss: 1.4336\n",
            "Epoch 63/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.4800 - loss: 1.2596 - val_accuracy: 0.3759 - val_loss: 1.4320\n",
            "Epoch 64/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.4857 - loss: 1.2480 - val_accuracy: 0.3847 - val_loss: 1.4392\n",
            "Epoch 65/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.4811 - loss: 1.2477 - val_accuracy: 0.3853 - val_loss: 1.4384\n",
            "Epoch 66/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.4885 - loss: 1.2420 - val_accuracy: 0.3859 - val_loss: 1.4299\n",
            "Epoch 67/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.4831 - loss: 1.2420 - val_accuracy: 0.3865 - val_loss: 1.4312\n",
            "Epoch 68/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.5033 - loss: 1.2274 - val_accuracy: 0.3891 - val_loss: 1.4338\n",
            "Epoch 69/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.4901 - loss: 1.2434 - val_accuracy: 0.3985 - val_loss: 1.4358\n",
            "Epoch 70/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.4888 - loss: 1.2320 - val_accuracy: 0.3916 - val_loss: 1.4301\n",
            "Epoch 71/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.5007 - loss: 1.2213 - val_accuracy: 0.3909 - val_loss: 1.4301\n",
            "Epoch 72/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.5040 - loss: 1.2282 - val_accuracy: 0.3966 - val_loss: 1.4314\n",
            "Epoch 73/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.4943 - loss: 1.2278 - val_accuracy: 0.3972 - val_loss: 1.4334\n",
            "Epoch 74/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.5029 - loss: 1.2052 - val_accuracy: 0.3941 - val_loss: 1.4315\n",
            "Epoch 75/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.4943 - loss: 1.2317 - val_accuracy: 0.3922 - val_loss: 1.4317\n",
            "Epoch 76/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.4975 - loss: 1.2188 - val_accuracy: 0.4067 - val_loss: 1.4291\n",
            "Epoch 77/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.4969 - loss: 1.2099 - val_accuracy: 0.4041 - val_loss: 1.4321\n",
            "Epoch 78/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.4964 - loss: 1.2207 - val_accuracy: 0.4016 - val_loss: 1.4339\n",
            "Epoch 79/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.5159 - loss: 1.2094 - val_accuracy: 0.3953 - val_loss: 1.4301\n",
            "Epoch 80/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.5084 - loss: 1.1962 - val_accuracy: 0.3997 - val_loss: 1.4291\n",
            "Epoch 81/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.5192 - loss: 1.1900 - val_accuracy: 0.3935 - val_loss: 1.4381\n",
            "Epoch 82/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.5121 - loss: 1.1999 - val_accuracy: 0.3979 - val_loss: 1.4402\n",
            "Epoch 83/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.5115 - loss: 1.2026 - val_accuracy: 0.3972 - val_loss: 1.4324\n",
            "Epoch 84/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.5070 - loss: 1.2075 - val_accuracy: 0.4054 - val_loss: 1.4326\n",
            "Epoch 85/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.5206 - loss: 1.1863 - val_accuracy: 0.3935 - val_loss: 1.4335\n",
            "Epoch 86/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.5197 - loss: 1.1745 - val_accuracy: 0.3972 - val_loss: 1.4305\n",
            "Epoch 87/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.5112 - loss: 1.1825 - val_accuracy: 0.3991 - val_loss: 1.4263\n",
            "Epoch 88/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.5003 - loss: 1.2046 - val_accuracy: 0.4035 - val_loss: 1.4328\n",
            "Epoch 89/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.5089 - loss: 1.1808 - val_accuracy: 0.4010 - val_loss: 1.4259\n",
            "Epoch 90/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.5241 - loss: 1.1683 - val_accuracy: 0.4067 - val_loss: 1.4180\n",
            "Epoch 91/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.5207 - loss: 1.1690 - val_accuracy: 0.4060 - val_loss: 1.4265\n",
            "Epoch 92/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.5171 - loss: 1.1832 - val_accuracy: 0.3928 - val_loss: 1.4329\n",
            "Epoch 93/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.5236 - loss: 1.1630 - val_accuracy: 0.3953 - val_loss: 1.4365\n",
            "Epoch 94/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.5370 - loss: 1.1517 - val_accuracy: 0.3947 - val_loss: 1.4363\n",
            "Epoch 95/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.5234 - loss: 1.1700 - val_accuracy: 0.4079 - val_loss: 1.4304\n",
            "Epoch 96/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.5225 - loss: 1.1673 - val_accuracy: 0.4060 - val_loss: 1.4289\n",
            "Epoch 97/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.5232 - loss: 1.1654 - val_accuracy: 0.4067 - val_loss: 1.4353\n",
            "Epoch 98/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.5413 - loss: 1.1356 - val_accuracy: 0.4085 - val_loss: 1.4310\n",
            "Epoch 99/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.5239 - loss: 1.1448 - val_accuracy: 0.4048 - val_loss: 1.4392\n",
            "Epoch 100/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.5380 - loss: 1.1511 - val_accuracy: 0.4023 - val_loss: 1.4417\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000951 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 1091\n",
            "[LightGBM] [Info] Number of data points in the train set: 6364, number of used features: 20\n",
            "[LightGBM] [Info] Start training from score -1.604579\n",
            "[LightGBM] [Info] Start training from score -1.614795\n",
            "[LightGBM] [Info] Start training from score -1.608496\n",
            "[LightGBM] [Info] Start training from score -1.610853\n",
            "[LightGBM] [Info] Start training from score -1.608496\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000722 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 1091\n",
            "[LightGBM] [Info] Number of data points in the train set: 5091, number of used features: 20\n",
            "[LightGBM] [Info] Start training from score -1.604735\n",
            "[LightGBM] [Info] Start training from score -1.614558\n",
            "[LightGBM] [Info] Start training from score -1.608653\n",
            "[LightGBM] [Info] Start training from score -1.610617\n",
            "[LightGBM] [Info] Start training from score -1.608653\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000717 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 1091\n",
            "[LightGBM] [Info] Number of data points in the train set: 5091, number of used features: 20\n",
            "[LightGBM] [Info] Start training from score -1.604735\n",
            "[LightGBM] [Info] Start training from score -1.614558\n",
            "[LightGBM] [Info] Start training from score -1.608653\n",
            "[LightGBM] [Info] Start training from score -1.610617\n",
            "[LightGBM] [Info] Start training from score -1.608653\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000754 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 1091\n",
            "[LightGBM] [Info] Number of data points in the train set: 5091, number of used features: 20\n",
            "[LightGBM] [Info] Start training from score -1.604735\n",
            "[LightGBM] [Info] Start training from score -1.614558\n",
            "[LightGBM] [Info] Start training from score -1.608653\n",
            "[LightGBM] [Info] Start training from score -1.610617\n",
            "[LightGBM] [Info] Start training from score -1.608653\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000724 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 1091\n",
            "[LightGBM] [Info] Number of data points in the train set: 5091, number of used features: 20\n",
            "[LightGBM] [Info] Start training from score -1.604735\n",
            "[LightGBM] [Info] Start training from score -1.614558\n",
            "[LightGBM] [Info] Start training from score -1.608653\n",
            "[LightGBM] [Info] Start training from score -1.610617\n",
            "[LightGBM] [Info] Start training from score -1.608653\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000738 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 1091\n",
            "[LightGBM] [Info] Number of data points in the train set: 5092, number of used features: 20\n",
            "[LightGBM] [Info] Start training from score -1.603954\n",
            "[LightGBM] [Info] Start training from score -1.615742\n",
            "[LightGBM] [Info] Start training from score -1.607868\n",
            "[LightGBM] [Info] Start training from score -1.611797\n",
            "[LightGBM] [Info] Start training from score -1.607868\n",
            "ðŸ”— **Final Optimized Stacking Accuracy: 60.97%**\n",
            "âœ… **Final Optimized Stacking Model Saved!**\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import joblib\n",
        "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from lightgbm import LGBMClassifier\n",
        "from catboost import CatBoostClassifier\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# âœ… 1ï¸âƒ£ Reintroduce Useful Features\n",
        "df[\"Height_Weight_Ratio\"] = df[\"Height (cm)\"] / df[\"Weight (kg)\"]\n",
        "\n",
        "# âœ… 2ï¸âƒ£ Use Class Weights Instead of SMOTE\n",
        "class_weights = compute_class_weight(\"balanced\", classes=np.unique(y_train), y=y_train)\n",
        "class_weight_dict = {i: class_weights[i] for i in range(len(class_weights))}\n",
        "\n",
        "# âœ… 3ï¸âƒ£ Hyperparameter Tuning for RandomForest\n",
        "rf_params = {\n",
        "    \"n_estimators\": [200, 300, 500],\n",
        "    \"max_depth\": [10, 20, None],\n",
        "    \"min_samples_split\": [2, 5, 10]\n",
        "}\n",
        "rf_model = RandomizedSearchCV(RandomForestClassifier(random_state=42), rf_params, cv=5, n_iter=10, n_jobs=-1)\n",
        "rf_model.fit(X_train_selected, y_train)\n",
        "\n",
        "# âœ… 4ï¸âƒ£ Hyperparameter Tuning for XGBoost\n",
        "xgb_params = {\n",
        "    \"learning_rate\": [0.01, 0.05, 0.1],\n",
        "    \"n_estimators\": [100, 200, 300],\n",
        "    \"max_depth\": [3, 5, 7]\n",
        "}\n",
        "xgb_model = RandomizedSearchCV(XGBClassifier(use_label_encoder=False, eval_metric=\"mlogloss\"), xgb_params, cv=5, n_iter=10, n_jobs=-1)\n",
        "xgb_model.fit(X_train_selected, y_train)\n",
        "\n",
        "# âœ… 5ï¸âƒ£ Train a Better Neural Network\n",
        "def build_ann():\n",
        "    model = keras.Sequential([\n",
        "        layers.Input(shape=(X_train_selected.shape[1],)),\n",
        "        layers.Dense(256, activation=\"relu\"),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.Dropout(0.3),\n",
        "        layers.Dense(128, activation=\"relu\"),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.Dropout(0.3),\n",
        "        layers.Dense(len(set(y_train)), activation=\"softmax\")  # Multi-class classification\n",
        "    ])\n",
        "    model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.0005),\n",
        "                  loss=\"sparse_categorical_crossentropy\",\n",
        "                  metrics=[\"accuracy\"])\n",
        "    return model\n",
        "\n",
        "ann_model = build_ann()\n",
        "ann_model.fit(X_train_selected, y_train, epochs=50, batch_size=64, verbose=1, validation_data=(X_test_selected, y_test), class_weight=class_weight_dict)\n",
        "\n",
        "# âœ… 6ï¸âƒ£ Use a Weighted Voting Ensemble\n",
        "voting_clf = VotingClassifier(\n",
        "    estimators=[\n",
        "        ('RandomForest', rf_model.best_estimator_),\n",
        "        ('XGBoost', xgb_model.best_estimator_),\n",
        "        ('LightGBM', LGBMClassifier(n_estimators=300, learning_rate=0.05)),\n",
        "        ('CatBoost', CatBoostClassifier(iterations=300, depth=6, learning_rate=0.05, verbose=0))\n",
        "    ],\n",
        "    voting='soft',  # Soft voting for better probability averaging\n",
        "    weights=[3, 3, 2, 2]  # Giving more weight to the best models\n",
        ")\n",
        "\n",
        "voting_clf.fit(X_train_selected, y_train)\n",
        "y_pred_voting = voting_clf.predict(X_test_selected)\n",
        "voting_accuracy = accuracy_score(y_test, y_pred_voting)\n",
        "\n",
        "# âœ… 7ï¸âƒ£ Save the Best Model\n",
        "print(f\"ðŸ”— **Final Optimized Voting Accuracy: {voting_accuracy * 100:.2f}%**\")\n",
        "joblib.dump(voting_clf, \"best_final_model_voting.pkl\")\n",
        "print(\"âœ… **Final Optimized Model Saved!**\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ITHAmUDyw7QO",
        "outputId": "a175e5b7-89a0-4131-dc24-1dab70647f85"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [16:08:55] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.2073 - loss: 2.2040 - val_accuracy: 0.2351 - val_loss: 1.6145\n",
            "Epoch 2/50\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.2511 - loss: 1.9137 - val_accuracy: 0.2564 - val_loss: 1.5871\n",
            "Epoch 3/50\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.2502 - loss: 1.7984 - val_accuracy: 0.2847 - val_loss: 1.5552\n",
            "Epoch 4/50\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.2610 - loss: 1.7131 - val_accuracy: 0.2979 - val_loss: 1.5326\n",
            "Epoch 5/50\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.2688 - loss: 1.6647 - val_accuracy: 0.3042 - val_loss: 1.5252\n",
            "Epoch 6/50\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.2856 - loss: 1.6390 - val_accuracy: 0.3262 - val_loss: 1.5122\n",
            "Epoch 7/50\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.2863 - loss: 1.6191 - val_accuracy: 0.3237 - val_loss: 1.5117\n",
            "Epoch 8/50\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.2986 - loss: 1.5896 - val_accuracy: 0.3306 - val_loss: 1.5050\n",
            "Epoch 9/50\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.3139 - loss: 1.5569 - val_accuracy: 0.3388 - val_loss: 1.5017\n",
            "Epoch 10/50\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.3249 - loss: 1.5378 - val_accuracy: 0.3306 - val_loss: 1.5030\n",
            "Epoch 11/50\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.3223 - loss: 1.5246 - val_accuracy: 0.3369 - val_loss: 1.4994\n",
            "Epoch 12/50\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.3363 - loss: 1.5090 - val_accuracy: 0.3413 - val_loss: 1.4948\n",
            "Epoch 13/50\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.3515 - loss: 1.5007 - val_accuracy: 0.3470 - val_loss: 1.4880\n",
            "Epoch 14/50\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.3311 - loss: 1.4907 - val_accuracy: 0.3457 - val_loss: 1.4901\n",
            "Epoch 15/50\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.3517 - loss: 1.4816 - val_accuracy: 0.3476 - val_loss: 1.4843\n",
            "Epoch 16/50\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.3565 - loss: 1.4714 - val_accuracy: 0.3488 - val_loss: 1.4812\n",
            "Epoch 17/50\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.3372 - loss: 1.4762 - val_accuracy: 0.3520 - val_loss: 1.4780\n",
            "Epoch 18/50\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.3545 - loss: 1.4673 - val_accuracy: 0.3470 - val_loss: 1.4787\n",
            "Epoch 19/50\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.3603 - loss: 1.4617 - val_accuracy: 0.3664 - val_loss: 1.4761\n",
            "Epoch 20/50\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.3666 - loss: 1.4503 - val_accuracy: 0.3583 - val_loss: 1.4706\n",
            "Epoch 21/50\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.3600 - loss: 1.4524 - val_accuracy: 0.3564 - val_loss: 1.4770\n",
            "Epoch 22/50\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.3673 - loss: 1.4621 - val_accuracy: 0.3589 - val_loss: 1.4727\n",
            "Epoch 23/50\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.3832 - loss: 1.4342 - val_accuracy: 0.3658 - val_loss: 1.4725\n",
            "Epoch 24/50\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.3824 - loss: 1.4255 - val_accuracy: 0.3501 - val_loss: 1.4724\n",
            "Epoch 25/50\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.3899 - loss: 1.4307 - val_accuracy: 0.3652 - val_loss: 1.4660\n",
            "Epoch 26/50\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.3661 - loss: 1.4471 - val_accuracy: 0.3734 - val_loss: 1.4696\n",
            "Epoch 27/50\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.3830 - loss: 1.4300 - val_accuracy: 0.3633 - val_loss: 1.4671\n",
            "Epoch 28/50\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.4031 - loss: 1.4057 - val_accuracy: 0.3690 - val_loss: 1.4633\n",
            "Epoch 29/50\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.4039 - loss: 1.4049 - val_accuracy: 0.3614 - val_loss: 1.4633\n",
            "Epoch 30/50\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.3964 - loss: 1.4068 - val_accuracy: 0.3589 - val_loss: 1.4576\n",
            "Epoch 31/50\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.4128 - loss: 1.3874 - val_accuracy: 0.3652 - val_loss: 1.4612\n",
            "Epoch 32/50\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.3987 - loss: 1.4151 - val_accuracy: 0.3708 - val_loss: 1.4614\n",
            "Epoch 33/50\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.4035 - loss: 1.4011 - val_accuracy: 0.3734 - val_loss: 1.4573\n",
            "Epoch 34/50\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.4039 - loss: 1.3964 - val_accuracy: 0.3658 - val_loss: 1.4533\n",
            "Epoch 35/50\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.4009 - loss: 1.3915 - val_accuracy: 0.3727 - val_loss: 1.4479\n",
            "Epoch 36/50\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.4076 - loss: 1.3964 - val_accuracy: 0.3828 - val_loss: 1.4497\n",
            "Epoch 37/50\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.4021 - loss: 1.3870 - val_accuracy: 0.3821 - val_loss: 1.4470\n",
            "Epoch 38/50\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.4231 - loss: 1.3689 - val_accuracy: 0.3834 - val_loss: 1.4441\n",
            "Epoch 39/50\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.4256 - loss: 1.3631 - val_accuracy: 0.3809 - val_loss: 1.4453\n",
            "Epoch 40/50\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.4198 - loss: 1.3776 - val_accuracy: 0.3790 - val_loss: 1.4453\n",
            "Epoch 41/50\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.4125 - loss: 1.3664 - val_accuracy: 0.3708 - val_loss: 1.4433\n",
            "Epoch 42/50\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.4193 - loss: 1.3687 - val_accuracy: 0.3752 - val_loss: 1.4439\n",
            "Epoch 43/50\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.4115 - loss: 1.3621 - val_accuracy: 0.3646 - val_loss: 1.4423\n",
            "Epoch 44/50\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.4206 - loss: 1.3528 - val_accuracy: 0.3752 - val_loss: 1.4414\n",
            "Epoch 45/50\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.4289 - loss: 1.3484 - val_accuracy: 0.3759 - val_loss: 1.4414\n",
            "Epoch 46/50\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.4190 - loss: 1.3672 - val_accuracy: 0.3759 - val_loss: 1.4399\n",
            "Epoch 47/50\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.4298 - loss: 1.3394 - val_accuracy: 0.3765 - val_loss: 1.4365\n",
            "Epoch 48/50\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.4305 - loss: 1.3390 - val_accuracy: 0.3765 - val_loss: 1.4407\n",
            "Epoch 49/50\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.4301 - loss: 1.3451 - val_accuracy: 0.3752 - val_loss: 1.4422\n",
            "Epoch 50/50\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.4356 - loss: 1.3548 - val_accuracy: 0.3865 - val_loss: 1.4433\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [16:09:49] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000920 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 1091\n",
            "[LightGBM] [Info] Number of data points in the train set: 6364, number of used features: 20\n",
            "[LightGBM] [Info] Start training from score -1.604579\n",
            "[LightGBM] [Info] Start training from score -1.614795\n",
            "[LightGBM] [Info] Start training from score -1.608496\n",
            "[LightGBM] [Info] Start training from score -1.610853\n",
            "[LightGBM] [Info] Start training from score -1.608496\n",
            "ðŸ”— **Final Optimized Voting Accuracy: 56.69%**\n",
            "âœ… **Final Optimized Model Saved!**\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import joblib\n",
        "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from lightgbm import LGBMClassifier\n",
        "from catboost import CatBoostClassifier\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.feature_selection import RFE\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "# âœ… **1ï¸âƒ£ Fix Data Issues - Check Label Distribution**\n",
        "unique_classes, class_counts = np.unique(y_train, return_counts=True)\n",
        "print(f\"ðŸ” Class Distribution: {dict(zip(unique_classes, class_counts))}\")\n",
        "\n",
        "# âœ… **2ï¸âƒ£ Recursive Feature Elimination (RFE) to Pick Best Features**\n",
        "selector = RFE(estimator=RandomForestClassifier(n_estimators=100, random_state=42), n_features_to_select=20)\n",
        "selector.fit(X_train, y_train)\n",
        "X_train_selected = selector.transform(X_train)\n",
        "X_test_selected = selector.transform(X_test)\n",
        "selected_features = np.array(feature_names)[selector.support_]\n",
        "\n",
        "# âœ… **3ï¸âƒ£ Hyperparameter Tuning for All Models**\n",
        "rf_params = {\"n_estimators\": [300, 500], \"max_depth\": [10, 20], \"min_samples_split\": [2, 5]}\n",
        "rf_model = RandomizedSearchCV(RandomForestClassifier(random_state=42), rf_params, cv=5, n_iter=10, n_jobs=-1)\n",
        "rf_model.fit(X_train_selected, y_train)\n",
        "\n",
        "xgb_params = {\"learning_rate\": [0.01, 0.05], \"n_estimators\": [200, 300], \"max_depth\": [3, 5]}\n",
        "xgb_model = RandomizedSearchCV(XGBClassifier(use_label_encoder=False, eval_metric=\"mlogloss\"), xgb_params, cv=5, n_iter=10, n_jobs=-1)\n",
        "xgb_model.fit(X_train_selected, y_train)\n",
        "\n",
        "# âœ… **4ï¸âƒ£ Train a Better Neural Network**\n",
        "def build_ann():\n",
        "    model = keras.Sequential([\n",
        "        layers.Dense(256, activation=\"relu\"),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.Dropout(0.3),\n",
        "        layers.Dense(128, activation=\"relu\"),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.Dropout(0.3),\n",
        "        layers.Dense(len(set(y_train)), activation=\"softmax\")\n",
        "    ])\n",
        "    model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.0003),\n",
        "                  loss=\"sparse_categorical_crossentropy\",\n",
        "                  metrics=[\"accuracy\"])\n",
        "    return model\n",
        "\n",
        "ann_model = build_ann()\n",
        "ann_model.fit(X_train_selected, y_train, epochs=100, batch_size=64, verbose=1, validation_data=(X_test_selected, y_test))\n",
        "\n",
        "# âœ… **5ï¸âƒ£ Stacking Classifier with Meta-Model**\n",
        "from sklearn.ensemble import StackingClassifier\n",
        "\n",
        "stacking_clf = StackingClassifier(\n",
        "    estimators=[\n",
        "        ('RandomForest', rf_model.best_estimator_),\n",
        "        ('XGBoost', xgb_model.best_estimator_),\n",
        "        ('LightGBM', LGBMClassifier(n_estimators=300, learning_rate=0.05)),\n",
        "        ('CatBoost', CatBoostClassifier(iterations=300, depth=6, learning_rate=0.05, verbose=0))\n",
        "    ],\n",
        "    final_estimator=LogisticRegression()  # Meta-model\n",
        ")\n",
        "\n",
        "stacking_clf.fit(X_train_selected, y_train)\n",
        "y_pred_stack = stacking_clf.predict(X_test_selected)\n",
        "stacking_accuracy = accuracy_score(y_test, y_pred_stack)\n",
        "\n",
        "# âœ… **6ï¸âƒ£ Save the Best Model**\n",
        "print(f\"ðŸ”— **Final Optimized Stacking Accuracy: {stacking_accuracy * 100:.2f}%**\")\n",
        "joblib.dump(stacking_clf, \"best_final_stacking_model.pkl\")\n",
        "print(\"âœ… **Final Optimized Stacking Model Saved!**\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ExvGNrHEy6JJ",
        "outputId": "28c1a3d5-1c15-4f59-e7b4-9c1bbcc273f9"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ” Class Distribution: {0: 1279, 1: 1266, 2: 1274, 3: 1271, 4: 1274}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_search.py:317: UserWarning: The total space of parameters 8 is smaller than n_iter=10. Running 8 iterations. For exhaustive searches, use GridSearchCV.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_search.py:317: UserWarning: The total space of parameters 8 is smaller than n_iter=10. Running 8 iterations. For exhaustive searches, use GridSearchCV.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [16:14:02] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - accuracy: 0.2059 - loss: 2.3240 - val_accuracy: 0.2106 - val_loss: 1.6334\n",
            "Epoch 2/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.2406 - loss: 1.9917 - val_accuracy: 0.2212 - val_loss: 1.6443\n",
            "Epoch 3/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.2513 - loss: 1.8995 - val_accuracy: 0.2539 - val_loss: 1.6058\n",
            "Epoch 4/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.2642 - loss: 1.8328 - val_accuracy: 0.2690 - val_loss: 1.5815\n",
            "Epoch 5/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.2623 - loss: 1.7786 - val_accuracy: 0.2847 - val_loss: 1.5668\n",
            "Epoch 6/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.2873 - loss: 1.7189 - val_accuracy: 0.2967 - val_loss: 1.5522\n",
            "Epoch 7/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.2845 - loss: 1.6831 - val_accuracy: 0.3004 - val_loss: 1.5471\n",
            "Epoch 8/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.2909 - loss: 1.6452 - val_accuracy: 0.3048 - val_loss: 1.5388\n",
            "Epoch 9/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.2842 - loss: 1.6614 - val_accuracy: 0.2979 - val_loss: 1.5294\n",
            "Epoch 10/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.2870 - loss: 1.6258 - val_accuracy: 0.3092 - val_loss: 1.5267\n",
            "Epoch 11/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.2898 - loss: 1.6126 - val_accuracy: 0.3130 - val_loss: 1.5254\n",
            "Epoch 12/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.3108 - loss: 1.5805 - val_accuracy: 0.3048 - val_loss: 1.5219\n",
            "Epoch 13/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.3082 - loss: 1.5689 - val_accuracy: 0.3162 - val_loss: 1.5161\n",
            "Epoch 14/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.3000 - loss: 1.5924 - val_accuracy: 0.3092 - val_loss: 1.5175\n",
            "Epoch 15/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.3165 - loss: 1.5508 - val_accuracy: 0.3124 - val_loss: 1.5150\n",
            "Epoch 16/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.3346 - loss: 1.5365 - val_accuracy: 0.3162 - val_loss: 1.5083\n",
            "Epoch 17/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.3277 - loss: 1.5301 - val_accuracy: 0.3099 - val_loss: 1.5108\n",
            "Epoch 18/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.3387 - loss: 1.5241 - val_accuracy: 0.3162 - val_loss: 1.5103\n",
            "Epoch 19/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.3113 - loss: 1.5300 - val_accuracy: 0.3118 - val_loss: 1.5076\n",
            "Epoch 20/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.3286 - loss: 1.4907 - val_accuracy: 0.3155 - val_loss: 1.5095\n",
            "Epoch 21/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.3389 - loss: 1.5002 - val_accuracy: 0.3193 - val_loss: 1.5067\n",
            "Epoch 22/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.3400 - loss: 1.5063 - val_accuracy: 0.3300 - val_loss: 1.5059\n",
            "Epoch 23/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.3356 - loss: 1.4909 - val_accuracy: 0.3187 - val_loss: 1.5008\n",
            "Epoch 24/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.3501 - loss: 1.4971 - val_accuracy: 0.3268 - val_loss: 1.5007\n",
            "Epoch 25/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.3605 - loss: 1.4671 - val_accuracy: 0.3237 - val_loss: 1.4990\n",
            "Epoch 26/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.3552 - loss: 1.4698 - val_accuracy: 0.3224 - val_loss: 1.4981\n",
            "Epoch 27/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.3382 - loss: 1.4892 - val_accuracy: 0.3231 - val_loss: 1.4968\n",
            "Epoch 28/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - accuracy: 0.3463 - loss: 1.4655 - val_accuracy: 0.3212 - val_loss: 1.4989\n",
            "Epoch 29/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.3493 - loss: 1.4605 - val_accuracy: 0.3237 - val_loss: 1.4972\n",
            "Epoch 30/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.3677 - loss: 1.4496 - val_accuracy: 0.3287 - val_loss: 1.4933\n",
            "Epoch 31/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.3562 - loss: 1.4594 - val_accuracy: 0.3356 - val_loss: 1.4935\n",
            "Epoch 32/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.3696 - loss: 1.4490 - val_accuracy: 0.3356 - val_loss: 1.4921\n",
            "Epoch 33/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.3756 - loss: 1.4384 - val_accuracy: 0.3275 - val_loss: 1.4934\n",
            "Epoch 34/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.3727 - loss: 1.4432 - val_accuracy: 0.3294 - val_loss: 1.4924\n",
            "Epoch 35/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.3873 - loss: 1.4306 - val_accuracy: 0.3319 - val_loss: 1.4889\n",
            "Epoch 36/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.3818 - loss: 1.4291 - val_accuracy: 0.3331 - val_loss: 1.4904\n",
            "Epoch 37/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.3686 - loss: 1.4488 - val_accuracy: 0.3319 - val_loss: 1.4875\n",
            "Epoch 38/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.3724 - loss: 1.4339 - val_accuracy: 0.3325 - val_loss: 1.4880\n",
            "Epoch 39/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.3840 - loss: 1.4278 - val_accuracy: 0.3281 - val_loss: 1.4878\n",
            "Epoch 40/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.3650 - loss: 1.4421 - val_accuracy: 0.3338 - val_loss: 1.4874\n",
            "Epoch 41/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.3757 - loss: 1.4369 - val_accuracy: 0.3388 - val_loss: 1.4858\n",
            "Epoch 42/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.3944 - loss: 1.4220 - val_accuracy: 0.3382 - val_loss: 1.4857\n",
            "Epoch 43/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.3948 - loss: 1.4050 - val_accuracy: 0.3344 - val_loss: 1.4848\n",
            "Epoch 44/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.4053 - loss: 1.4009 - val_accuracy: 0.3363 - val_loss: 1.4836\n",
            "Epoch 45/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.3897 - loss: 1.4046 - val_accuracy: 0.3438 - val_loss: 1.4814\n",
            "Epoch 46/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.3898 - loss: 1.4119 - val_accuracy: 0.3463 - val_loss: 1.4792\n",
            "Epoch 47/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.3887 - loss: 1.4103 - val_accuracy: 0.3476 - val_loss: 1.4803\n",
            "Epoch 48/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.3923 - loss: 1.4020 - val_accuracy: 0.3457 - val_loss: 1.4801\n",
            "Epoch 49/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.3973 - loss: 1.4100 - val_accuracy: 0.3400 - val_loss: 1.4814\n",
            "Epoch 50/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.4010 - loss: 1.3939 - val_accuracy: 0.3438 - val_loss: 1.4806\n",
            "Epoch 51/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.3968 - loss: 1.3995 - val_accuracy: 0.3419 - val_loss: 1.4800\n",
            "Epoch 52/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.3903 - loss: 1.4005 - val_accuracy: 0.3426 - val_loss: 1.4794\n",
            "Epoch 53/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.4030 - loss: 1.3784 - val_accuracy: 0.3394 - val_loss: 1.4731\n",
            "Epoch 54/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.3988 - loss: 1.4002 - val_accuracy: 0.3451 - val_loss: 1.4731\n",
            "Epoch 55/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.4062 - loss: 1.3842 - val_accuracy: 0.3470 - val_loss: 1.4717\n",
            "Epoch 56/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.4138 - loss: 1.3719 - val_accuracy: 0.3526 - val_loss: 1.4712\n",
            "Epoch 57/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.4136 - loss: 1.3940 - val_accuracy: 0.3457 - val_loss: 1.4718\n",
            "Epoch 58/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.4086 - loss: 1.3775 - val_accuracy: 0.3470 - val_loss: 1.4718\n",
            "Epoch 59/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.4148 - loss: 1.3775 - val_accuracy: 0.3419 - val_loss: 1.4730\n",
            "Epoch 60/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.4017 - loss: 1.3800 - val_accuracy: 0.3444 - val_loss: 1.4691\n",
            "Epoch 61/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.4004 - loss: 1.3805 - val_accuracy: 0.3444 - val_loss: 1.4677\n",
            "Epoch 62/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.4206 - loss: 1.3547 - val_accuracy: 0.3526 - val_loss: 1.4631\n",
            "Epoch 63/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.4115 - loss: 1.3764 - val_accuracy: 0.3463 - val_loss: 1.4641\n",
            "Epoch 64/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.4121 - loss: 1.3503 - val_accuracy: 0.3558 - val_loss: 1.4666\n",
            "Epoch 65/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.4139 - loss: 1.3589 - val_accuracy: 0.3507 - val_loss: 1.4682\n",
            "Epoch 66/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.4213 - loss: 1.3615 - val_accuracy: 0.3564 - val_loss: 1.4647\n",
            "Epoch 67/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.4324 - loss: 1.3452 - val_accuracy: 0.3514 - val_loss: 1.4657\n",
            "Epoch 68/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.4245 - loss: 1.3498 - val_accuracy: 0.3507 - val_loss: 1.4663\n",
            "Epoch 69/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.4181 - loss: 1.3571 - val_accuracy: 0.3539 - val_loss: 1.4636\n",
            "Epoch 70/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.4505 - loss: 1.3222 - val_accuracy: 0.3501 - val_loss: 1.4664\n",
            "Epoch 71/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.4554 - loss: 1.3286 - val_accuracy: 0.3476 - val_loss: 1.4627\n",
            "Epoch 72/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.4347 - loss: 1.3311 - val_accuracy: 0.3501 - val_loss: 1.4599\n",
            "Epoch 73/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.4426 - loss: 1.3323 - val_accuracy: 0.3539 - val_loss: 1.4597\n",
            "Epoch 74/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.4298 - loss: 1.3366 - val_accuracy: 0.3488 - val_loss: 1.4602\n",
            "Epoch 75/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.4458 - loss: 1.3291 - val_accuracy: 0.3633 - val_loss: 1.4577\n",
            "Epoch 76/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.4442 - loss: 1.3266 - val_accuracy: 0.3589 - val_loss: 1.4587\n",
            "Epoch 77/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.4453 - loss: 1.3188 - val_accuracy: 0.3608 - val_loss: 1.4576\n",
            "Epoch 78/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - accuracy: 0.4404 - loss: 1.3200 - val_accuracy: 0.3520 - val_loss: 1.4567\n",
            "Epoch 79/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.4349 - loss: 1.3290 - val_accuracy: 0.3470 - val_loss: 1.4611\n",
            "Epoch 80/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.4288 - loss: 1.3291 - val_accuracy: 0.3482 - val_loss: 1.4615\n",
            "Epoch 81/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.4463 - loss: 1.3167 - val_accuracy: 0.3539 - val_loss: 1.4599\n",
            "Epoch 82/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.4499 - loss: 1.3037 - val_accuracy: 0.3589 - val_loss: 1.4574\n",
            "Epoch 83/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.4448 - loss: 1.3146 - val_accuracy: 0.3576 - val_loss: 1.4586\n",
            "Epoch 84/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.4474 - loss: 1.3160 - val_accuracy: 0.3545 - val_loss: 1.4559\n",
            "Epoch 85/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.4459 - loss: 1.3189 - val_accuracy: 0.3608 - val_loss: 1.4586\n",
            "Epoch 86/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.4656 - loss: 1.3090 - val_accuracy: 0.3646 - val_loss: 1.4597\n",
            "Epoch 87/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.4635 - loss: 1.2981 - val_accuracy: 0.3589 - val_loss: 1.4576\n",
            "Epoch 88/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.4604 - loss: 1.2967 - val_accuracy: 0.3576 - val_loss: 1.4578\n",
            "Epoch 89/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.4615 - loss: 1.2886 - val_accuracy: 0.3570 - val_loss: 1.4577\n",
            "Epoch 90/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.4623 - loss: 1.2956 - val_accuracy: 0.3608 - val_loss: 1.4538\n",
            "Epoch 91/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.4455 - loss: 1.2998 - val_accuracy: 0.3658 - val_loss: 1.4553\n",
            "Epoch 92/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.4487 - loss: 1.3048 - val_accuracy: 0.3614 - val_loss: 1.4584\n",
            "Epoch 93/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.4501 - loss: 1.2930 - val_accuracy: 0.3564 - val_loss: 1.4594\n",
            "Epoch 94/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.4709 - loss: 1.2881 - val_accuracy: 0.3558 - val_loss: 1.4544\n",
            "Epoch 95/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.4596 - loss: 1.2844 - val_accuracy: 0.3602 - val_loss: 1.4549\n",
            "Epoch 96/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.4695 - loss: 1.2822 - val_accuracy: 0.3570 - val_loss: 1.4537\n",
            "Epoch 97/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.4790 - loss: 1.2717 - val_accuracy: 0.3620 - val_loss: 1.4536\n",
            "Epoch 98/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.4519 - loss: 1.2909 - val_accuracy: 0.3708 - val_loss: 1.4512\n",
            "Epoch 99/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.4575 - loss: 1.2864 - val_accuracy: 0.3595 - val_loss: 1.4524\n",
            "Epoch 100/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.4635 - loss: 1.2808 - val_accuracy: 0.3627 - val_loss: 1.4520\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [16:15:36] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001061 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 1091\n",
            "[LightGBM] [Info] Number of data points in the train set: 6364, number of used features: 20\n",
            "[LightGBM] [Info] Start training from score -1.604579\n",
            "[LightGBM] [Info] Start training from score -1.614795\n",
            "[LightGBM] [Info] Start training from score -1.608496\n",
            "[LightGBM] [Info] Start training from score -1.610853\n",
            "[LightGBM] [Info] Start training from score -1.608496\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [16:16:25] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [16:16:32] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [16:16:34] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [16:16:36] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [16:16:39] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000845 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 1091\n",
            "[LightGBM] [Info] Number of data points in the train set: 5091, number of used features: 20\n",
            "[LightGBM] [Info] Start training from score -1.604735\n",
            "[LightGBM] [Info] Start training from score -1.614558\n",
            "[LightGBM] [Info] Start training from score -1.608653\n",
            "[LightGBM] [Info] Start training from score -1.610617\n",
            "[LightGBM] [Info] Start training from score -1.608653\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000815 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 1091\n",
            "[LightGBM] [Info] Number of data points in the train set: 5091, number of used features: 20\n",
            "[LightGBM] [Info] Start training from score -1.604735\n",
            "[LightGBM] [Info] Start training from score -1.614558\n",
            "[LightGBM] [Info] Start training from score -1.608653\n",
            "[LightGBM] [Info] Start training from score -1.610617\n",
            "[LightGBM] [Info] Start training from score -1.608653\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001251 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 1091\n",
            "[LightGBM] [Info] Number of data points in the train set: 5091, number of used features: 20\n",
            "[LightGBM] [Info] Start training from score -1.604735\n",
            "[LightGBM] [Info] Start training from score -1.614558\n",
            "[LightGBM] [Info] Start training from score -1.608653\n",
            "[LightGBM] [Info] Start training from score -1.610617\n",
            "[LightGBM] [Info] Start training from score -1.608653\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007186 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 1091\n",
            "[LightGBM] [Info] Number of data points in the train set: 5091, number of used features: 20\n",
            "[LightGBM] [Info] Start training from score -1.604735\n",
            "[LightGBM] [Info] Start training from score -1.614558\n",
            "[LightGBM] [Info] Start training from score -1.608653\n",
            "[LightGBM] [Info] Start training from score -1.610617\n",
            "[LightGBM] [Info] Start training from score -1.608653\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001127 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 1091\n",
            "[LightGBM] [Info] Number of data points in the train set: 5092, number of used features: 20\n",
            "[LightGBM] [Info] Start training from score -1.603954\n",
            "[LightGBM] [Info] Start training from score -1.615742\n",
            "[LightGBM] [Info] Start training from score -1.607868\n",
            "[LightGBM] [Info] Start training from score -1.611797\n",
            "[LightGBM] [Info] Start training from score -1.607868\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ”— **Final Optimized Stacking Accuracy: 63.23%**\n",
            "âœ… **Final Optimized Stacking Model Saved!**\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from catboost import CatBoostClassifier\n",
        "\n",
        "# Assuming 'model' is your trained CatBoost model\n",
        "model.save_model(\"size_recommender.cbm\", format=\"cbm\")  # Saves as .cbm file\n",
        "\n"
      ],
      "metadata": {
        "id": "FCMGG4Lx1-QU"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from catboost import CatBoostClassifier\n",
        "\n",
        "# Save CatBoost model\n",
        "model.save_model(\"size_recommender.cbm\", format=\"cbm\")\n",
        "\n",
        "# Download the model file (for Colab)\n",
        "from google.colab import files\n",
        "files.download(\"size_recommender.cbm\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "yJzele4_ufaN",
        "outputId": "c624e9d8-ac5c-49c6-fdee-9f89c6f4139a"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_de477ce9-ffcb-4287-91f0-227bc4e0f79c\", \"size_recommender.cbm\", 3173104)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import catboost\n",
        "import joblib\n",
        "\n",
        "print(catboost.__version__)  # Get CatBoost version\n",
        "print(joblib.__version__)    # Get Joblib version\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WuV-9DYXyE3x",
        "outputId": "5f3ff3bf-4171-49a0-916e-54e654e10803"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.2.7\n",
            "1.4.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "87P5PFYw8MwO"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMORamtJtMNGAbkrCJBLkXR",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}