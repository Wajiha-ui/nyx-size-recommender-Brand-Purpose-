{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Wajiha-ui/nyx-size-recommender-Brand-Purpose-/blob/main/AI_Outfit_recommendation_model_beta.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OtNdIuuGnfJN",
        "outputId": "52cbf3d9-3d0c-43bd-9849-6827b907a57a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: catboost in /usr/local/lib/python3.11/dist-packages (1.2.7)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.11/dist-packages (from catboost) (0.20.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from catboost) (3.10.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.11/dist-packages (from catboost) (1.26.4)\n",
            "Requirement already satisfied: pandas>=0.24 in /usr/local/lib/python3.11/dist-packages (from catboost) (2.2.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from catboost) (1.15.2)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.11/dist-packages (from catboost) (5.24.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from catboost) (1.17.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.24->catboost) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.24->catboost) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.24->catboost) (2025.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (3.2.1)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from plotly->catboost) (9.0.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.4)\n",
            "Collecting numpy\n",
            "  Using cached numpy-2.2.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (1.15.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.3)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.11/dist-packages (3.0.0)\n",
            "Requirement already satisfied: lightgbm in /usr/local/lib/python3.11/dist-packages (4.6.0)\n",
            "Requirement already satisfied: catboost in /usr/local/lib/python3.11/dist-packages (1.2.7)\n",
            "Requirement already satisfied: imbalanced-learn in /usr/local/lib/python3.11/dist-packages (0.13.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: nvidia-nccl-cu12 in /usr/local/lib/python3.11/dist-packages (from xgboost) (2.21.5)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.11/dist-packages (from catboost) (0.20.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from catboost) (3.10.0)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.11/dist-packages (from catboost) (5.24.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from catboost) (1.17.0)\n",
            "Requirement already satisfied: sklearn-compat<1,>=0.1 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn) (0.1.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (3.2.1)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from plotly->catboost) (9.0.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install catboost\n",
        "!pip install --upgrade numpy scipy pandas scikit-learn xgboost lightgbm catboost imbalanced-learn\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4aOrobzPnLBD",
        "outputId": "92b80796-3dff-4a86-c3a8-6c9f6267fab5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ“Š RandomForest Accuracy: 64.97%\n",
            "ðŸ“Š XGBoost Accuracy: 60.83%\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002226 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 1115\n",
            "[LightGBM] [Info] Number of data points in the train set: 6760, number of used features: 28\n",
            "[LightGBM] [Info] Start training from score -1.610178\n",
            "[LightGBM] [Info] Start training from score -1.607960\n",
            "[LightGBM] [Info] Start training from score -1.604274\n",
            "[LightGBM] [Info] Start training from score -1.610918\n",
            "[LightGBM] [Info] Start training from score -1.613886\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ“Š LightGBM Accuracy: 57.10%\n",
            "ðŸ“Š CatBoost Accuracy: 61.95%\n",
            "ðŸ“Š NeuralNetwork Accuracy: 40.47%\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002226 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 1115\n",
            "[LightGBM] [Info] Number of data points in the train set: 6760, number of used features: 28\n",
            "[LightGBM] [Info] Start training from score -1.610178\n",
            "[LightGBM] [Info] Start training from score -1.607960\n",
            "[LightGBM] [Info] Start training from score -1.604274\n",
            "[LightGBM] [Info] Start training from score -1.610918\n",
            "[LightGBM] [Info] Start training from score -1.613886\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001831 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 1115\n",
            "[LightGBM] [Info] Number of data points in the train set: 5408, number of used features: 28\n",
            "[LightGBM] [Info] Start training from score -1.609993\n",
            "[LightGBM] [Info] Start training from score -1.608144\n",
            "[LightGBM] [Info] Start training from score -1.604458\n",
            "[LightGBM] [Info] Start training from score -1.610918\n",
            "[LightGBM] [Info] Start training from score -1.613700\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002463 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 1115\n",
            "[LightGBM] [Info] Number of data points in the train set: 5408, number of used features: 28\n",
            "[LightGBM] [Info] Start training from score -1.609993\n",
            "[LightGBM] [Info] Start training from score -1.608144\n",
            "[LightGBM] [Info] Start training from score -1.604458\n",
            "[LightGBM] [Info] Start training from score -1.610918\n",
            "[LightGBM] [Info] Start training from score -1.613700\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001706 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 1115\n",
            "[LightGBM] [Info] Number of data points in the train set: 5408, number of used features: 28\n",
            "[LightGBM] [Info] Start training from score -1.609993\n",
            "[LightGBM] [Info] Start training from score -1.608144\n",
            "[LightGBM] [Info] Start training from score -1.604458\n",
            "[LightGBM] [Info] Start training from score -1.610918\n",
            "[LightGBM] [Info] Start training from score -1.613700\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001689 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 1115\n",
            "[LightGBM] [Info] Number of data points in the train set: 5408, number of used features: 28\n",
            "[LightGBM] [Info] Start training from score -1.610918\n",
            "[LightGBM] [Info] Start training from score -1.608144\n",
            "[LightGBM] [Info] Start training from score -1.603538\n",
            "[LightGBM] [Info] Start training from score -1.610918\n",
            "[LightGBM] [Info] Start training from score -1.613700\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001704 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 1115\n",
            "[LightGBM] [Info] Number of data points in the train set: 5408, number of used features: 28\n",
            "[LightGBM] [Info] Start training from score -1.609993\n",
            "[LightGBM] [Info] Start training from score -1.607221\n",
            "[LightGBM] [Info] Start training from score -1.604458\n",
            "[LightGBM] [Info] Start training from score -1.610918\n",
            "[LightGBM] [Info] Start training from score -1.614629\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ”— Stacking Ensemble Accuracy: 66.75%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Best model saved successfully!\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "import joblib\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, StackingClassifier\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import xgboost as xgb\n",
        "import lightgbm as lgb\n",
        "import catboost as cb\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "# Generate synthetic dataset\n",
        "def generate_synthetic_data(num_entries=5000):\n",
        "    data = []\n",
        "    genders = [\"Male\", \"Female\"]\n",
        "    age_groups = [\"18-24\", \"25-34\", \"35-44\", \"45-54\", \"55+\"]\n",
        "    body_shapes = [\"Slim\", \"Athletic\", \"Regular\", \"Plus-size\"]\n",
        "    fit_preferences = [\"Slim\", \"Regular\", \"Loose\"]\n",
        "    clothing_types = [\"T-Shirt\", \"Hoodie\", \"Dress\", \"Jacket\", \"Pants\"]\n",
        "    materials = [\"Cotton\", \"Wool\", \"Silk\", \"Polyester\", \"Linen\"]\n",
        "    sizes = [\"XS\", \"S\", \"M\", \"L\", \"XL\", \"XXL\"]\n",
        "    regions = [\"Europe\", \"USA\", \"Asia\"]\n",
        "\n",
        "    for i in range(num_entries):\n",
        "        gender = random.choice(genders)\n",
        "        age_group = random.choice(age_groups)\n",
        "        height = random.randint(150, 200)\n",
        "        weight = random.randint(45, 120)\n",
        "        chest = random.randint(75, 130)\n",
        "        waist = random.randint(60, 110)\n",
        "        hips = random.randint(80, 130)\n",
        "        body_shape = random.choice(body_shapes)\n",
        "        fit = random.choice(fit_preferences)\n",
        "        clothing = random.choice(clothing_types)\n",
        "        size = random.choice(sizes)\n",
        "        material = random.choice(materials)\n",
        "        region = random.choice(regions)\n",
        "\n",
        "        data.append({\n",
        "            \"Gender\": gender,\n",
        "            \"Age Group\": age_group,\n",
        "            \"Height (cm)\": height,\n",
        "            \"Weight (kg)\": weight,\n",
        "            \"Chest (cm)\": chest,\n",
        "            \"Waist (cm)\": waist,\n",
        "            \"Hips (cm)\": hips,\n",
        "            \"Body Shape\": body_shape,\n",
        "            \"Preferred Fit\": fit,\n",
        "            \"Clothing Type\": clothing,\n",
        "            \"Brand Size\": size,\n",
        "            \"Material Preference\": material,\n",
        "            \"Country/Region\": region\n",
        "        })\n",
        "\n",
        "    return pd.DataFrame(data)\n",
        "\n",
        "# Generate and preprocess dataset\n",
        "df = generate_synthetic_data(5000)\n",
        "df[\"BMI\"] = df[\"Weight (kg)\"] / ((df[\"Height (cm)\"] / 100) ** 2)\n",
        "df[\"Chest_Waist_Ratio\"] = df[\"Chest (cm)\"] / df[\"Waist (cm)\"]\n",
        "df[\"Waist_Hips_Ratio\"] = df[\"Waist (cm)\"] / df[\"Hips (cm)\"]\n",
        "\n",
        "# Encode categorical variables\n",
        "df = pd.get_dummies(df, drop_first=True)\n",
        "\n",
        "# Identify the one-hot encoded 'Brand Size' columns\n",
        "brand_size_cols = [col for col in df.columns if col.startswith(\"Brand Size_\")]\n",
        "X = df.drop(columns=brand_size_cols)\n",
        "y = df[brand_size_cols].idxmax(axis=1).str.replace(\"Brand Size_\", \"\")\n",
        "\n",
        "# Encode target labels numerically\n",
        "label_encoder = LabelEncoder()\n",
        "y = label_encoder.fit_transform(y)\n",
        "\n",
        "# Balance the dataset with SMOTE\n",
        "smote = SMOTE(random_state=42)\n",
        "X, y = smote.fit_resample(X, y)\n",
        "\n",
        "# Standardize numerical features\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define models with improved hyperparameters\n",
        "rf_model = RandomForestClassifier(n_estimators=500, max_depth=30, random_state=42)\n",
        "xgb_model = xgb.XGBClassifier(objective=\"multi:softmax\", num_class=len(np.unique(y)), learning_rate=0.05, max_depth=10, n_estimators=300, random_state=42)\n",
        "lgb_model = lgb.LGBMClassifier(num_leaves=31, learning_rate=0.05, n_estimators=300, random_state=42)\n",
        "cat_model = cb.CatBoostClassifier(iterations=500, depth=10, learning_rate=0.05, verbose=0, random_state=42)\n",
        "nn_model = MLPClassifier(hidden_layer_sizes=(128, 64), activation='relu', solver='adam', max_iter=500, random_state=42)\n",
        "\n",
        "# Train models\n",
        "models = {\"RandomForest\": rf_model, \"XGBoost\": xgb_model, \"LightGBM\": lgb_model, \"CatBoost\": cat_model, \"NeuralNetwork\": nn_model}\n",
        "for name, model in models.items():\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    acc = accuracy_score(y_test, y_pred)\n",
        "    print(f\"ðŸ“Š {name} Accuracy: {acc * 100:.2f}%\")\n",
        "\n",
        "# Stacking Model\n",
        "stacking_clf = StackingClassifier(\n",
        "    estimators=[(\"RandomForest\", rf_model), (\"XGBoost\", xgb_model), (\"LightGBM\", lgb_model), (\"CatBoost\", cat_model)],\n",
        "    final_estimator=nn_model\n",
        ")\n",
        "\n",
        "# Train and evaluate stacking model\n",
        "stacking_clf.fit(X_train, y_train)\n",
        "y_pred_stack = stacking_clf.predict(X_test)\n",
        "stacking_accuracy = accuracy_score(y_test, y_pred_stack)\n",
        "print(f\"ðŸ”— Stacking Ensemble Accuracy: {stacking_accuracy * 100:.2f}%\")\n",
        "\n",
        "# Save best model\n",
        "best_model = max(models.items(), key=lambda x: accuracy_score(y_test, x[1].predict(X_test)))[1]\n",
        "joblib.dump(best_model, \"best_size_model.pkl\")\n",
        "print(\"âœ… Best model saved successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "3Ul8FrGQ1E-J",
        "outputId": "6eee2b36-c86b-481c-ffe0-16a682a07de6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸš€ Training RandomForest...\n",
            "ðŸš€ Training XGBoost...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ” Top Features from RandomForest:\n",
            "        Feature  Importance\n",
            "5    Feature_5    0.091450\n",
            "6    Feature_6    0.089880\n",
            "7    Feature_7    0.088777\n",
            "1    Feature_1    0.085436\n",
            "0    Feature_0    0.084273\n",
            "2    Feature_2    0.084088\n",
            "4    Feature_4    0.083390\n",
            "3    Feature_3    0.077976\n",
            "14  Feature_14    0.017565\n",
            "27  Feature_27    0.017091\n",
            "16  Feature_16    0.016879\n",
            "15  Feature_15    0.016583\n",
            "18  Feature_18    0.016521\n",
            "9    Feature_9    0.016482\n",
            "21  Feature_21    0.016102\n",
            "ðŸ” Top Features from XGBoost:\n",
            "        Feature  Importance\n",
            "26  Feature_26    0.041474\n",
            "12  Feature_12    0.041118\n",
            "14  Feature_14    0.041112\n",
            "11  Feature_11    0.039973\n",
            "10  Feature_10    0.039732\n",
            "23  Feature_23    0.039489\n",
            "9    Feature_9    0.038738\n",
            "24  Feature_24    0.037619\n",
            "17  Feature_17    0.037612\n",
            "27  Feature_27    0.036815\n",
            "15  Feature_15    0.036783\n",
            "16  Feature_16    0.036715\n",
            "13  Feature_13    0.036509\n",
            "21  Feature_21    0.035962\n",
            "22  Feature_22    0.035589\n"
          ]
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Ensure models are trained before calling feature importance\n",
        "models = {\n",
        "    \"RandomForest\": RandomForestClassifier(n_estimators=300, random_state=42),\n",
        "    \"XGBoost\": xgb.XGBClassifier(objective=\"multi:softmax\", num_class=len(np.unique(y_train)), random_state=42)\n",
        "}\n",
        "\n",
        "# Train models on the full dataset if not already trained\n",
        "for name, model in models.items():\n",
        "    print(f\"ðŸš€ Training {name}...\")\n",
        "    model.fit(X_train, y_train)\n",
        "    models[name] = model  # Save trained model\n",
        "\n",
        "# Assign trained models\n",
        "best_rf = models[\"RandomForest\"]\n",
        "best_xgb = models[\"XGBoost\"]\n",
        "\n",
        "# Function to get feature importance from different models\n",
        "def plot_feature_importance(model, X, model_name):\n",
        "    importance = model.feature_importances_\n",
        "    feature_importance_df = pd.DataFrame({\n",
        "        'Feature': X.columns,\n",
        "        'Importance': importance\n",
        "    }).sort_values(by=\"Importance\", ascending=False)\n",
        "\n",
        "    # Plot\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    sns.barplot(x=\"Importance\", y=\"Feature\", data=feature_importance_df[:15])\n",
        "    plt.title(f\"Top 15 Feature Importance - {model_name}\")\n",
        "    plt.show()\n",
        "\n",
        "    return feature_importance_df\n",
        "\n",
        "# Convert X_train to DataFrame (Ensure feature names are correctly assigned)\n",
        "feature_names = [f\"Feature_{i}\" for i in range(X_train.shape[1])]  # If feature names are missing\n",
        "X_train_df = pd.DataFrame(X_train, columns=feature_names)\n",
        "\n",
        "# Train models & plot feature importance\n",
        "rf_feature_importance = plot_feature_importance(best_rf, X_train_df, \"RandomForest\")\n",
        "xgb_feature_importance = plot_feature_importance(best_xgb, X_train_df, \"XGBoost\")\n",
        "\n",
        "# Display top features\n",
        "print(\"ðŸ” Top Features from RandomForest:\\n\", rf_feature_importance.head(15))\n",
        "print(\"ðŸ” Top Features from XGBoost:\\n\", xgb_feature_importance.head(15))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2O4FSMcMlvVO",
        "outputId": "5ab0c736-efdf-4261-fa16-ea13f7b1fe4d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸš€ Training RandomForest with selected features...\n",
            "ðŸ“Š RandomForest Accuracy (after feature selection): 63.47%\n",
            "ðŸš€ Training XGBoost with selected features...\n",
            "ðŸ“Š XGBoost Accuracy (after feature selection): 54.17%\n",
            "ðŸš€ Training LightGBM with selected features...\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001989 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 1100\n",
            "[LightGBM] [Info] Number of data points in the train set: 6712, number of used features: 23\n",
            "[LightGBM] [Info] Start training from score -1.595677\n",
            "[LightGBM] [Info] Start training from score -1.629300\n",
            "[LightGBM] [Info] Start training from score -1.603792\n",
            "[LightGBM] [Info] Start training from score -1.601573\n",
            "[LightGBM] [Info] Start training from score -1.617215\n",
            "ðŸ“Š LightGBM Accuracy (after feature selection): 52.15%\n",
            "ðŸš€ Training CatBoost with selected features...\n",
            "ðŸ“Š CatBoost Accuracy (after feature selection): 55.72%\n"
          ]
        }
      ],
      "source": [
        "# Select important features from both RandomForest & XGBoost\n",
        "selected_features = [\n",
        "    \"Feature_0\", \"Feature_1\", \"Feature_2\", \"Feature_3\", \"Feature_4\", \"Feature_5\", \"Feature_6\", \"Feature_7\",\n",
        "    \"Feature_22\", \"Feature_24\", \"Feature_23\", \"Feature_15\", \"Feature_19\", \"Feature_18\", \"Feature_21\", \"Feature_11\",\n",
        "    \"Feature_20\", \"Feature_25\", \"Feature_9\", \"Feature_27\", \"Feature_14\", \"Feature_12\", \"Feature_10\"\n",
        "]\n",
        "\n",
        "# Filter dataset to keep only selected features\n",
        "X_train_selected = pd.DataFrame(X_train, columns=feature_names)[selected_features]\n",
        "X_test_selected = pd.DataFrame(X_test, columns=feature_names)[selected_features]\n",
        "\n",
        "# Retrain models on reduced feature set\n",
        "models = {\n",
        "    \"RandomForest\": RandomForestClassifier(n_estimators=300, random_state=42),\n",
        "    \"XGBoost\": xgb.XGBClassifier(objective=\"multi:softmax\", num_class=len(np.unique(y_train)), random_state=42),\n",
        "    \"LightGBM\": lgb.LGBMClassifier(random_state=42),\n",
        "    \"CatBoost\": cb.CatBoostClassifier(verbose=0, random_state=42)\n",
        "}\n",
        "\n",
        "# Train & evaluate models\n",
        "for name, model in models.items():\n",
        "    print(f\"ðŸš€ Training {name} with selected features...\")\n",
        "    model.fit(X_train_selected, y_train)\n",
        "    y_pred = model.predict(X_test_selected)\n",
        "    acc = accuracy_score(y_test, y_pred)\n",
        "    print(f\"ðŸ“Š {name} Accuracy (after feature selection): {acc * 100:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mq2FotXot0xQ",
        "outputId": "4d6b0be2-90ae-49bd-d9c6-c899f978e8ae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002108 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 1115\n",
            "[LightGBM] [Info] Number of data points in the train set: 6712, number of used features: 28\n",
            "[LightGBM] [Info] Start training from score -1.595677\n",
            "[LightGBM] [Info] Start training from score -1.629300\n",
            "[LightGBM] [Info] Start training from score -1.603792\n",
            "[LightGBM] [Info] Start training from score -1.601573\n",
            "[LightGBM] [Info] Start training from score -1.617215\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002144 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 1115\n",
            "[LightGBM] [Info] Number of data points in the train set: 6712, number of used features: 28\n",
            "[LightGBM] [Info] Start training from score -1.595677\n",
            "[LightGBM] [Info] Start training from score -1.629300\n",
            "[LightGBM] [Info] Start training from score -1.603792\n",
            "[LightGBM] [Info] Start training from score -1.601573\n",
            "[LightGBM] [Info] Start training from score -1.617215\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001837 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 1115\n",
            "[LightGBM] [Info] Number of data points in the train set: 5369, number of used features: 28\n",
            "[LightGBM] [Info] Start training from score -1.595382\n",
            "[LightGBM] [Info] Start training from score -1.628998\n",
            "[LightGBM] [Info] Start training from score -1.603681\n",
            "[LightGBM] [Info] Start training from score -1.601830\n",
            "[LightGBM] [Info] Start training from score -1.617667\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001671 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 1115\n",
            "[LightGBM] [Info] Number of data points in the train set: 5369, number of used features: 28\n",
            "[LightGBM] [Info] Start training from score -1.595382\n",
            "[LightGBM] [Info] Start training from score -1.628998\n",
            "[LightGBM] [Info] Start training from score -1.603681\n",
            "[LightGBM] [Info] Start training from score -1.601830\n",
            "[LightGBM] [Info] Start training from score -1.617667\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001882 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 1115\n",
            "[LightGBM] [Info] Number of data points in the train set: 5370, number of used features: 28\n",
            "[LightGBM] [Info] Start training from score -1.595568\n",
            "[LightGBM] [Info] Start training from score -1.630135\n",
            "[LightGBM] [Info] Start training from score -1.603867\n",
            "[LightGBM] [Info] Start training from score -1.601093\n",
            "[LightGBM] [Info] Start training from score -1.616915\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001734 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 1115\n",
            "[LightGBM] [Info] Number of data points in the train set: 5370, number of used features: 28\n",
            "[LightGBM] [Info] Start training from score -1.596487\n",
            "[LightGBM] [Info] Start training from score -1.629185\n",
            "[LightGBM] [Info] Start training from score -1.603867\n",
            "[LightGBM] [Info] Start training from score -1.601093\n",
            "[LightGBM] [Info] Start training from score -1.616915\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001644 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 1115\n",
            "[LightGBM] [Info] Number of data points in the train set: 5370, number of used features: 28\n",
            "[LightGBM] [Info] Start training from score -1.595568\n",
            "[LightGBM] [Info] Start training from score -1.629185\n",
            "[LightGBM] [Info] Start training from score -1.603867\n",
            "[LightGBM] [Info] Start training from score -1.602017\n",
            "[LightGBM] [Info] Start training from score -1.616915\n",
            "ðŸ”— Stacking Ensemble Accuracy: 63.89%\n",
            "âœ… Best model saved successfully!\n"
          ]
        }
      ],
      "source": [
        "from sklearn.ensemble import StackingClassifier\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "# Reintroduce mid-importance features\n",
        "selected_features_extended = [\n",
        "    \"Feature_0\", \"Feature_1\", \"Feature_2\", \"Feature_3\", \"Feature_4\", \"Feature_5\", \"Feature_6\", \"Feature_7\",\n",
        "    \"Feature_22\", \"Feature_24\", \"Feature_23\", \"Feature_15\", \"Feature_19\", \"Feature_18\", \"Feature_21\", \"Feature_11\",\n",
        "    \"Feature_20\", \"Feature_25\", \"Feature_9\", \"Feature_27\", \"Feature_14\", \"Feature_12\", \"Feature_10\",\n",
        "    \"Feature_8\", \"Feature_13\", \"Feature_16\", \"Feature_17\", \"Feature_26\"  # Bringing back some mid-range features\n",
        "]\n",
        "\n",
        "# Filter dataset to keep only selected features\n",
        "X_train_selected = pd.DataFrame(X_train, columns=feature_names)[selected_features_extended]\n",
        "X_test_selected = pd.DataFrame(X_test, columns=feature_names)[selected_features_extended]\n",
        "\n",
        "# Hyperparameter tuning for RandomForest\n",
        "rf_params = {\n",
        "    \"n_estimators\": [300, 500, 700],\n",
        "    \"max_depth\": [20, 30, None],\n",
        "    \"min_samples_split\": [2, 5, 10],\n",
        "    \"min_samples_leaf\": [1, 2, 4]\n",
        "}\n",
        "best_rf = RandomizedSearchCV(RandomForestClassifier(random_state=42), rf_params, n_iter=5, cv=3, n_jobs=-1)\n",
        "best_rf.fit(X_train_selected, y_train)\n",
        "\n",
        "# Hyperparameter tuning for XGBoost\n",
        "best_xgb = xgb.XGBClassifier(objective=\"multi:softmax\", num_class=len(np.unique(y_train)), random_state=42)\n",
        "best_xgb.fit(X_train_selected, y_train)\n",
        "\n",
        "# Hyperparameter tuning for LightGBM\n",
        "best_lgb = lgb.LGBMClassifier(random_state=42)\n",
        "best_lgb.fit(X_train_selected, y_train)\n",
        "\n",
        "# Hyperparameter tuning for CatBoost\n",
        "best_cat = cb.CatBoostClassifier(verbose=0, random_state=42)\n",
        "best_cat.fit(X_train_selected, y_train)\n",
        "\n",
        "# Stacking Classifier (Combining models)\n",
        "stacking_clf = StackingClassifier(\n",
        "    estimators=[\n",
        "        (\"RandomForest\", best_rf.best_estimator_),\n",
        "        (\"XGBoost\", best_xgb),\n",
        "        (\"LightGBM\", best_lgb),\n",
        "        (\"CatBoost\", best_cat)\n",
        "    ],\n",
        "    final_estimator=RandomForestClassifier(n_estimators=500, random_state=42)  # Meta-model\n",
        ")\n",
        "\n",
        "# Train Stacking Classifier\n",
        "stacking_clf.fit(X_train_selected, y_train)\n",
        "y_pred_stack = stacking_clf.predict(X_test_selected)\n",
        "stacking_accuracy = accuracy_score(y_test, y_pred_stack)\n",
        "\n",
        "# Print Results\n",
        "print(f\"ðŸ”— Stacking Ensemble Accuracy: {stacking_accuracy * 100:.2f}%\")\n",
        "\n",
        "# Save Best Model\n",
        "best_model = stacking_clf if stacking_accuracy > 70 else best_rf.best_estimator_  # Choose best model\n",
        "joblib.dump(best_model, \"best_size_model.pkl\")\n",
        "print(\"âœ… Best model saved successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZKwBSpAAxD5b",
        "outputId": "6d4357d3-d58a-4d86-d69b-bce9230c2dbf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001293 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 1091\n",
            "[LightGBM] [Info] Number of data points in the train set: 6712, number of used features: 20\n",
            "[LightGBM] [Info] Start training from score -1.595677\n",
            "[LightGBM] [Info] Start training from score -1.629300\n",
            "[LightGBM] [Info] Start training from score -1.603792\n",
            "[LightGBM] [Info] Start training from score -1.601573\n",
            "[LightGBM] [Info] Start training from score -1.617215\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001052 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 1091\n",
            "[LightGBM] [Info] Number of data points in the train set: 5369, number of used features: 20\n",
            "[LightGBM] [Info] Start training from score -1.595382\n",
            "[LightGBM] [Info] Start training from score -1.628998\n",
            "[LightGBM] [Info] Start training from score -1.603681\n",
            "[LightGBM] [Info] Start training from score -1.601830\n",
            "[LightGBM] [Info] Start training from score -1.617667\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000989 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 1091\n",
            "[LightGBM] [Info] Number of data points in the train set: 5369, number of used features: 20\n",
            "[LightGBM] [Info] Start training from score -1.595382\n",
            "[LightGBM] [Info] Start training from score -1.628998\n",
            "[LightGBM] [Info] Start training from score -1.603681\n",
            "[LightGBM] [Info] Start training from score -1.601830\n",
            "[LightGBM] [Info] Start training from score -1.617667\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000980 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 1091\n",
            "[LightGBM] [Info] Number of data points in the train set: 5370, number of used features: 20\n",
            "[LightGBM] [Info] Start training from score -1.595568\n",
            "[LightGBM] [Info] Start training from score -1.630135\n",
            "[LightGBM] [Info] Start training from score -1.603867\n",
            "[LightGBM] [Info] Start training from score -1.601093\n",
            "[LightGBM] [Info] Start training from score -1.616915\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001026 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 1091\n",
            "[LightGBM] [Info] Number of data points in the train set: 5370, number of used features: 20\n",
            "[LightGBM] [Info] Start training from score -1.596487\n",
            "[LightGBM] [Info] Start training from score -1.629185\n",
            "[LightGBM] [Info] Start training from score -1.603867\n",
            "[LightGBM] [Info] Start training from score -1.601093\n",
            "[LightGBM] [Info] Start training from score -1.616915\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001384 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 1091\n",
            "[LightGBM] [Info] Number of data points in the train set: 5370, number of used features: 20\n",
            "[LightGBM] [Info] Start training from score -1.595568\n",
            "[LightGBM] [Info] Start training from score -1.629185\n",
            "[LightGBM] [Info] Start training from score -1.603867\n",
            "[LightGBM] [Info] Start training from score -1.602017\n",
            "[LightGBM] [Info] Start training from score -1.616915\n",
            "ðŸ”— **Stacking Model Accuracy: 61.80%**\n",
            "âœ… **Stacking Model Saved Successfully!**\n"
          ]
        }
      ],
      "source": [
        "from sklearn.ensemble import StackingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import xgboost as xgb\n",
        "import lightgbm as lgb\n",
        "import joblib\n",
        "\n",
        "# Step 1ï¸âƒ£: Train RandomForest to Get Feature Importance\n",
        "rf_temp = RandomForestClassifier(n_estimators=200, random_state=42)\n",
        "rf_temp.fit(X_train, y_train)\n",
        "\n",
        "# Step 2ï¸âƒ£: Select Top 20 Features\n",
        "feature_importance = pd.DataFrame({\n",
        "    \"Feature\": feature_names,\n",
        "    \"Importance\": rf_temp.feature_importances_\n",
        "}).sort_values(by=\"Importance\", ascending=False)\n",
        "\n",
        "selected_features = feature_importance.nlargest(20, \"Importance\")[\"Feature\"].tolist()\n",
        "X_train_selected = pd.DataFrame(X_train, columns=feature_names)[selected_features]\n",
        "X_test_selected = pd.DataFrame(X_test, columns=feature_names)[selected_features]\n",
        "\n",
        "# Step 3ï¸âƒ£: Define Stronger Models\n",
        "best_rf = RandomForestClassifier(n_estimators=500, max_depth=30, min_samples_split=5, random_state=42)\n",
        "best_xgb = xgb.XGBClassifier(n_estimators=300, max_depth=12, learning_rate=0.05, random_state=42)\n",
        "best_lgb = lgb.LGBMClassifier(n_estimators=300, learning_rate=0.03, max_depth=20, random_state=42)\n",
        "\n",
        "# Step 4ï¸âƒ£: Create Stacking Model\n",
        "stacking_clf = StackingClassifier(\n",
        "    estimators=[\n",
        "        ('RandomForest', best_rf),\n",
        "        ('XGBoost', best_xgb),\n",
        "        ('LightGBM', best_lgb)\n",
        "    ],\n",
        "    final_estimator=LogisticRegression(max_iter=1000)\n",
        ")\n",
        "\n",
        "# Step 5ï¸âƒ£: Train & Evaluate\n",
        "stacking_clf.fit(X_train_selected, y_train)\n",
        "y_pred_stack = stacking_clf.predict(X_test_selected)\n",
        "stacking_accuracy = accuracy_score(y_test, y_pred_stack)\n",
        "\n",
        "print(f\"ðŸ”— **Stacking Model Accuracy: {stacking_accuracy * 100:.2f}%**\")\n",
        "joblib.dump(stacking_clf, \"best_size_model.pkl\")\n",
        "print(\"âœ… **Stacking Model Saved Successfully!**\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from sklearn.ensemble import StackingClassifier\n",
        "import joblib\n",
        "\n",
        "# âœ… Step 1ï¸âƒ£: Train a Neural Network\n",
        "def build_ann(input_dim):\n",
        "    model = keras.Sequential([\n",
        "        layers.Dense(64, activation=\"relu\", input_shape=(input_dim,)),\n",
        "        layers.Dense(32, activation=\"relu\"),\n",
        "        layers.Dense(16, activation=\"relu\"),\n",
        "        layers.Dense(len(set(y_train)), activation=\"softmax\")  # Multi-class output\n",
        "    ])\n",
        "    model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "    return model\n",
        "\n",
        "ann_model = build_ann(X_train_selected.shape[1])\n",
        "ann_model.fit(X_train_selected, y_train, epochs=50, batch_size=32, verbose=1, validation_data=(X_test_selected, y_test))\n",
        "\n",
        "# âœ… Step 2ï¸âƒ£: Train Stacking Model with Neural Network\n",
        "stacking_clf = StackingClassifier(\n",
        "    estimators=[\n",
        "        ('RandomForest', best_rf),\n",
        "        ('XGBoost', best_xgb),\n",
        "        ('LightGBM', best_lgb)\n",
        "    ],\n",
        "    final_estimator=xgb.XGBClassifier(n_estimators=200, learning_rate=0.05, random_state=42)  # Use XGBoost as meta-learner\n",
        ")\n",
        "\n",
        "stacking_clf.fit(X_train_selected, y_train)\n",
        "y_pred_stack = stacking_clf.predict(X_test_selected)\n",
        "stacking_accuracy = accuracy_score(y_test, y_pred_stack)\n",
        "\n",
        "# âœ… Step 3ï¸âƒ£: Evaluate & Save the Best Model\n",
        "print(f\"ðŸ”— **Stacking + ANN Model Accuracy: {stacking_accuracy * 100:.2f}%**\")\n",
        "joblib.dump(stacking_clf, \"best_stacking_model.pkl\")\n",
        "print(\"âœ… **Stacking + ANN Model Saved Successfully!**\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wTFZbEeosSd6",
        "outputId": "ebc25927-754a-4c9f-eb7d-9d47636cc041"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "\u001b[1m210/210\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 17ms/step - accuracy: 0.1934 - loss: 1.6204 - val_accuracy: 0.2241 - val_loss: 1.6002\n",
            "Epoch 2/50\n",
            "\u001b[1m210/210\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.2718 - loss: 1.5865 - val_accuracy: 0.2539 - val_loss: 1.5831\n",
            "Epoch 3/50\n",
            "\u001b[1m210/210\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.3075 - loss: 1.5577 - val_accuracy: 0.2723 - val_loss: 1.5608\n",
            "Epoch 4/50\n",
            "\u001b[1m210/210\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 12ms/step - accuracy: 0.3173 - loss: 1.5266 - val_accuracy: 0.2884 - val_loss: 1.5471\n",
            "Epoch 5/50\n",
            "\u001b[1m210/210\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.3326 - loss: 1.5041 - val_accuracy: 0.2908 - val_loss: 1.5383\n",
            "Epoch 6/50\n",
            "\u001b[1m210/210\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.3545 - loss: 1.4723 - val_accuracy: 0.3010 - val_loss: 1.5309\n",
            "Epoch 7/50\n",
            "\u001b[1m210/210\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.3683 - loss: 1.4563 - val_accuracy: 0.3021 - val_loss: 1.5280\n",
            "Epoch 8/50\n",
            "\u001b[1m210/210\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.3787 - loss: 1.4409 - val_accuracy: 0.2956 - val_loss: 1.5221\n",
            "Epoch 9/50\n",
            "\u001b[1m210/210\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.3765 - loss: 1.4184 - val_accuracy: 0.3105 - val_loss: 1.5226\n",
            "Epoch 10/50\n",
            "\u001b[1m210/210\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.3998 - loss: 1.3959 - val_accuracy: 0.3206 - val_loss: 1.5129\n",
            "Epoch 11/50\n",
            "\u001b[1m210/210\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.4071 - loss: 1.3821 - val_accuracy: 0.3194 - val_loss: 1.5186\n",
            "Epoch 12/50\n",
            "\u001b[1m210/210\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.4035 - loss: 1.3748 - val_accuracy: 0.3164 - val_loss: 1.5224\n",
            "Epoch 13/50\n",
            "\u001b[1m210/210\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.4345 - loss: 1.3421 - val_accuracy: 0.3093 - val_loss: 1.5230\n",
            "Epoch 14/50\n",
            "\u001b[1m210/210\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.4282 - loss: 1.3510 - val_accuracy: 0.3164 - val_loss: 1.5193\n",
            "Epoch 15/50\n",
            "\u001b[1m210/210\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.4348 - loss: 1.3353 - val_accuracy: 0.3230 - val_loss: 1.5292\n",
            "Epoch 16/50\n",
            "\u001b[1m210/210\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.4410 - loss: 1.3236 - val_accuracy: 0.3188 - val_loss: 1.5389\n",
            "Epoch 17/50\n",
            "\u001b[1m210/210\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.4443 - loss: 1.3094 - val_accuracy: 0.3182 - val_loss: 1.5329\n",
            "Epoch 18/50\n",
            "\u001b[1m210/210\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.4745 - loss: 1.2820 - val_accuracy: 0.3343 - val_loss: 1.5378\n",
            "Epoch 19/50\n",
            "\u001b[1m210/210\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.4699 - loss: 1.2734 - val_accuracy: 0.3278 - val_loss: 1.5355\n",
            "Epoch 20/50\n",
            "\u001b[1m210/210\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.4752 - loss: 1.2778 - val_accuracy: 0.3260 - val_loss: 1.5421\n",
            "Epoch 21/50\n",
            "\u001b[1m210/210\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.4737 - loss: 1.2684 - val_accuracy: 0.3164 - val_loss: 1.5466\n",
            "Epoch 22/50\n",
            "\u001b[1m210/210\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.4850 - loss: 1.2606 - val_accuracy: 0.3319 - val_loss: 1.5568\n",
            "Epoch 23/50\n",
            "\u001b[1m210/210\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.4794 - loss: 1.2461 - val_accuracy: 0.3373 - val_loss: 1.5609\n",
            "Epoch 24/50\n",
            "\u001b[1m210/210\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.4807 - loss: 1.2407 - val_accuracy: 0.3361 - val_loss: 1.5765\n",
            "Epoch 25/50\n",
            "\u001b[1m210/210\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.4883 - loss: 1.2437 - val_accuracy: 0.3284 - val_loss: 1.5650\n",
            "Epoch 26/50\n",
            "\u001b[1m210/210\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.4908 - loss: 1.2286 - val_accuracy: 0.3141 - val_loss: 1.5847\n",
            "Epoch 27/50\n",
            "\u001b[1m210/210\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.4998 - loss: 1.2162 - val_accuracy: 0.3260 - val_loss: 1.5717\n",
            "Epoch 28/50\n",
            "\u001b[1m210/210\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5041 - loss: 1.2131 - val_accuracy: 0.3337 - val_loss: 1.5784\n",
            "Epoch 29/50\n",
            "\u001b[1m210/210\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.4918 - loss: 1.2068 - val_accuracy: 0.3254 - val_loss: 1.5862\n",
            "Epoch 30/50\n",
            "\u001b[1m210/210\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5071 - loss: 1.2017 - val_accuracy: 0.3290 - val_loss: 1.6011\n",
            "Epoch 31/50\n",
            "\u001b[1m210/210\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5108 - loss: 1.1880 - val_accuracy: 0.3367 - val_loss: 1.6090\n",
            "Epoch 32/50\n",
            "\u001b[1m210/210\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5146 - loss: 1.1837 - val_accuracy: 0.3284 - val_loss: 1.6212\n",
            "Epoch 33/50\n",
            "\u001b[1m210/210\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.5255 - loss: 1.1699 - val_accuracy: 0.3254 - val_loss: 1.6043\n",
            "Epoch 34/50\n",
            "\u001b[1m210/210\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.5215 - loss: 1.1792 - val_accuracy: 0.3182 - val_loss: 1.6158\n",
            "Epoch 35/50\n",
            "\u001b[1m210/210\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5177 - loss: 1.1592 - val_accuracy: 0.3224 - val_loss: 1.6239\n",
            "Epoch 36/50\n",
            "\u001b[1m210/210\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.5314 - loss: 1.1367 - val_accuracy: 0.3278 - val_loss: 1.6368\n",
            "Epoch 37/50\n",
            "\u001b[1m210/210\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5370 - loss: 1.1474 - val_accuracy: 0.3206 - val_loss: 1.6271\n",
            "Epoch 38/50\n",
            "\u001b[1m210/210\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5324 - loss: 1.1434 - val_accuracy: 0.3218 - val_loss: 1.6424\n",
            "Epoch 39/50\n",
            "\u001b[1m210/210\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5425 - loss: 1.1306 - val_accuracy: 0.3236 - val_loss: 1.6395\n",
            "Epoch 40/50\n",
            "\u001b[1m210/210\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5417 - loss: 1.1334 - val_accuracy: 0.3302 - val_loss: 1.6625\n",
            "Epoch 41/50\n",
            "\u001b[1m210/210\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5430 - loss: 1.1232 - val_accuracy: 0.3242 - val_loss: 1.6574\n",
            "Epoch 42/50\n",
            "\u001b[1m210/210\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5531 - loss: 1.1084 - val_accuracy: 0.3194 - val_loss: 1.6637\n",
            "Epoch 43/50\n",
            "\u001b[1m210/210\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5391 - loss: 1.1228 - val_accuracy: 0.3218 - val_loss: 1.6638\n",
            "Epoch 44/50\n",
            "\u001b[1m210/210\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.5567 - loss: 1.0972 - val_accuracy: 0.3218 - val_loss: 1.6736\n",
            "Epoch 45/50\n",
            "\u001b[1m210/210\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.5362 - loss: 1.1294 - val_accuracy: 0.3302 - val_loss: 1.6819\n",
            "Epoch 46/50\n",
            "\u001b[1m210/210\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5664 - loss: 1.0949 - val_accuracy: 0.3242 - val_loss: 1.6875\n",
            "Epoch 47/50\n",
            "\u001b[1m210/210\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5587 - loss: 1.0880 - val_accuracy: 0.3272 - val_loss: 1.6779\n",
            "Epoch 48/50\n",
            "\u001b[1m210/210\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5711 - loss: 1.0928 - val_accuracy: 0.3164 - val_loss: 1.7037\n",
            "Epoch 49/50\n",
            "\u001b[1m210/210\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5520 - loss: 1.1007 - val_accuracy: 0.3272 - val_loss: 1.7002\n",
            "Epoch 50/50\n",
            "\u001b[1m210/210\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5481 - loss: 1.0940 - val_accuracy: 0.3278 - val_loss: 1.6980\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001304 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 1091\n",
            "[LightGBM] [Info] Number of data points in the train set: 6712, number of used features: 20\n",
            "[LightGBM] [Info] Start training from score -1.595677\n",
            "[LightGBM] [Info] Start training from score -1.629300\n",
            "[LightGBM] [Info] Start training from score -1.603792\n",
            "[LightGBM] [Info] Start training from score -1.601573\n",
            "[LightGBM] [Info] Start training from score -1.617215\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001020 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 1091\n",
            "[LightGBM] [Info] Number of data points in the train set: 5369, number of used features: 20\n",
            "[LightGBM] [Info] Start training from score -1.595382\n",
            "[LightGBM] [Info] Start training from score -1.628998\n",
            "[LightGBM] [Info] Start training from score -1.603681\n",
            "[LightGBM] [Info] Start training from score -1.601830\n",
            "[LightGBM] [Info] Start training from score -1.617667\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000959 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 1091\n",
            "[LightGBM] [Info] Number of data points in the train set: 5369, number of used features: 20\n",
            "[LightGBM] [Info] Start training from score -1.595382\n",
            "[LightGBM] [Info] Start training from score -1.628998\n",
            "[LightGBM] [Info] Start training from score -1.603681\n",
            "[LightGBM] [Info] Start training from score -1.601830\n",
            "[LightGBM] [Info] Start training from score -1.617667\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000974 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 1091\n",
            "[LightGBM] [Info] Number of data points in the train set: 5370, number of used features: 20\n",
            "[LightGBM] [Info] Start training from score -1.595568\n",
            "[LightGBM] [Info] Start training from score -1.630135\n",
            "[LightGBM] [Info] Start training from score -1.603867\n",
            "[LightGBM] [Info] Start training from score -1.601093\n",
            "[LightGBM] [Info] Start training from score -1.616915\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000964 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 1091\n",
            "[LightGBM] [Info] Number of data points in the train set: 5370, number of used features: 20\n",
            "[LightGBM] [Info] Start training from score -1.596487\n",
            "[LightGBM] [Info] Start training from score -1.629185\n",
            "[LightGBM] [Info] Start training from score -1.603867\n",
            "[LightGBM] [Info] Start training from score -1.601093\n",
            "[LightGBM] [Info] Start training from score -1.616915\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000984 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 1091\n",
            "[LightGBM] [Info] Number of data points in the train set: 5370, number of used features: 20\n",
            "[LightGBM] [Info] Start training from score -1.595568\n",
            "[LightGBM] [Info] Start training from score -1.629185\n",
            "[LightGBM] [Info] Start training from score -1.603867\n",
            "[LightGBM] [Info] Start training from score -1.602017\n",
            "[LightGBM] [Info] Start training from score -1.616915\n",
            "ðŸ”— **Stacking + ANN Model Accuracy: 62.22%**\n",
            "âœ… **Stacking + ANN Model Saved Successfully!**\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from sklearn.ensemble import StackingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import joblib\n",
        "\n",
        "# âœ… Step 1ï¸âƒ£: Improved Neural Network Model\n",
        "def build_optimized_ann(input_dim):\n",
        "    model = keras.Sequential([\n",
        "        layers.Input(shape=(input_dim,)),  # Proper input layer\n",
        "        layers.Dense(128, activation=\"relu\"),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.Dropout(0.3),  # Prevent overfitting\n",
        "        layers.Dense(64, activation=\"relu\"),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.Dropout(0.3),\n",
        "        layers.Dense(32, activation=\"relu\"),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.Dense(len(set(y_train)), activation=\"softmax\")  # Multi-class classification\n",
        "    ])\n",
        "    model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.001),  # Lower learning rate\n",
        "                  loss=\"sparse_categorical_crossentropy\",\n",
        "                  metrics=[\"accuracy\"])\n",
        "    return model\n",
        "\n",
        "ann_model = build_optimized_ann(X_train_selected.shape[1])\n",
        "ann_model.fit(X_train_selected, y_train, epochs=100, batch_size=64, verbose=1, validation_data=(X_test_selected, y_test))\n",
        "\n",
        "# âœ… Step 2ï¸âƒ£: Train an Optimized Stacking Model\n",
        "stacking_clf = StackingClassifier(\n",
        "    estimators=[\n",
        "        ('RandomForest', best_rf),\n",
        "        ('XGBoost', best_xgb),\n",
        "        ('LightGBM', best_lgb),\n",
        "        ('CatBoost', best_cat)\n",
        "    ],\n",
        "    final_estimator=LogisticRegression(),  # Using Logistic Regression as meta-learner\n",
        "    stack_method='predict_proba'  # Use probabilities instead of hard labels\n",
        ")\n",
        "\n",
        "stacking_clf.fit(X_train_selected, y_train)\n",
        "y_pred_stack = stacking_clf.predict(X_test_selected)\n",
        "stacking_accuracy = accuracy_score(y_test, y_pred_stack)\n",
        "\n",
        "# âœ… Step 3ï¸âƒ£: Evaluate & Save the Best Model\n",
        "print(f\"ðŸ”— **Stacking + ANN Optimized Accuracy: {stacking_accuracy * 100:.2f}%**\")\n",
        "joblib.dump(stacking_clf, \"best_optimized_stacking_model.pkl\")\n",
        "print(\"âœ… **Optimized Stacking Model Saved Successfully!**\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "exsNoRG_touy",
        "outputId": "f97645f3-db23-4e0b-b6a1-e0f89f0f6701"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 8ms/step - accuracy: 0.2102 - loss: 1.9479 - val_accuracy: 0.2193 - val_loss: 1.6041\n",
            "Epoch 2/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.2333 - loss: 1.6667 - val_accuracy: 0.2330 - val_loss: 1.5924\n",
            "Epoch 3/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.2631 - loss: 1.5971 - val_accuracy: 0.2473 - val_loss: 1.5780\n",
            "Epoch 4/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.2649 - loss: 1.5867 - val_accuracy: 0.2688 - val_loss: 1.5697\n",
            "Epoch 5/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.2871 - loss: 1.5641 - val_accuracy: 0.2765 - val_loss: 1.5598\n",
            "Epoch 6/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.2956 - loss: 1.5410 - val_accuracy: 0.2735 - val_loss: 1.5564\n",
            "Epoch 7/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.3084 - loss: 1.5371 - val_accuracy: 0.2813 - val_loss: 1.5496\n",
            "Epoch 8/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 0.3081 - loss: 1.5311 - val_accuracy: 0.2884 - val_loss: 1.5417\n",
            "Epoch 9/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.3089 - loss: 1.5194 - val_accuracy: 0.2872 - val_loss: 1.5388\n",
            "Epoch 10/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - accuracy: 0.3211 - loss: 1.5127 - val_accuracy: 0.2908 - val_loss: 1.5345\n",
            "Epoch 11/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.3217 - loss: 1.5134 - val_accuracy: 0.3010 - val_loss: 1.5284\n",
            "Epoch 12/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.3267 - loss: 1.5097 - val_accuracy: 0.3039 - val_loss: 1.5273\n",
            "Epoch 13/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.3288 - loss: 1.4985 - val_accuracy: 0.3069 - val_loss: 1.5250\n",
            "Epoch 14/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.3447 - loss: 1.4956 - val_accuracy: 0.2980 - val_loss: 1.5256\n",
            "Epoch 15/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.3255 - loss: 1.4944 - val_accuracy: 0.2974 - val_loss: 1.5277\n",
            "Epoch 16/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.3318 - loss: 1.4985 - val_accuracy: 0.3051 - val_loss: 1.5221\n",
            "Epoch 17/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.3275 - loss: 1.4920 - val_accuracy: 0.3123 - val_loss: 1.5164\n",
            "Epoch 18/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.3224 - loss: 1.4860 - val_accuracy: 0.3057 - val_loss: 1.5163\n",
            "Epoch 19/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.3374 - loss: 1.4838 - val_accuracy: 0.3063 - val_loss: 1.5158\n",
            "Epoch 20/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.3513 - loss: 1.4792 - val_accuracy: 0.3147 - val_loss: 1.5120\n",
            "Epoch 21/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.3386 - loss: 1.4782 - val_accuracy: 0.3099 - val_loss: 1.5071\n",
            "Epoch 22/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.3547 - loss: 1.4587 - val_accuracy: 0.3111 - val_loss: 1.5108\n",
            "Epoch 23/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.3531 - loss: 1.4592 - val_accuracy: 0.3105 - val_loss: 1.5091\n",
            "Epoch 24/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.3688 - loss: 1.4510 - val_accuracy: 0.3081 - val_loss: 1.5131\n",
            "Epoch 25/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.3439 - loss: 1.4644 - val_accuracy: 0.3164 - val_loss: 1.5124\n",
            "Epoch 26/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.3658 - loss: 1.4514 - val_accuracy: 0.3266 - val_loss: 1.5003\n",
            "Epoch 27/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.3515 - loss: 1.4556 - val_accuracy: 0.3117 - val_loss: 1.4988\n",
            "Epoch 28/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.3626 - loss: 1.4469 - val_accuracy: 0.3194 - val_loss: 1.5042\n",
            "Epoch 29/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.3551 - loss: 1.4554 - val_accuracy: 0.3248 - val_loss: 1.5037\n",
            "Epoch 30/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.3615 - loss: 1.4492 - val_accuracy: 0.3260 - val_loss: 1.5040\n",
            "Epoch 31/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.3761 - loss: 1.4358 - val_accuracy: 0.3260 - val_loss: 1.5031\n",
            "Epoch 32/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.3584 - loss: 1.4441 - val_accuracy: 0.3254 - val_loss: 1.5027\n",
            "Epoch 33/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.3678 - loss: 1.4378 - val_accuracy: 0.3379 - val_loss: 1.5005\n",
            "Epoch 34/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.3754 - loss: 1.4237 - val_accuracy: 0.3385 - val_loss: 1.5000\n",
            "Epoch 35/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.3804 - loss: 1.4267 - val_accuracy: 0.3302 - val_loss: 1.4945\n",
            "Epoch 36/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.3837 - loss: 1.4204 - val_accuracy: 0.3266 - val_loss: 1.4996\n",
            "Epoch 37/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.3713 - loss: 1.4309 - val_accuracy: 0.3224 - val_loss: 1.4979\n",
            "Epoch 38/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.3745 - loss: 1.4288 - val_accuracy: 0.3230 - val_loss: 1.4969\n",
            "Epoch 39/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.3953 - loss: 1.4041 - val_accuracy: 0.3266 - val_loss: 1.4947\n",
            "Epoch 40/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.3898 - loss: 1.4119 - val_accuracy: 0.3385 - val_loss: 1.4876\n",
            "Epoch 41/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.3832 - loss: 1.4228 - val_accuracy: 0.3355 - val_loss: 1.4905\n",
            "Epoch 42/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.3799 - loss: 1.4142 - val_accuracy: 0.3302 - val_loss: 1.4877\n",
            "Epoch 43/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.3784 - loss: 1.4222 - val_accuracy: 0.3266 - val_loss: 1.4937\n",
            "Epoch 44/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.4008 - loss: 1.3906 - val_accuracy: 0.3409 - val_loss: 1.4887\n",
            "Epoch 45/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.3772 - loss: 1.4105 - val_accuracy: 0.3403 - val_loss: 1.4838\n",
            "Epoch 46/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.3820 - loss: 1.4162 - val_accuracy: 0.3385 - val_loss: 1.4871\n",
            "Epoch 47/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.3916 - loss: 1.4066 - val_accuracy: 0.3433 - val_loss: 1.4792\n",
            "Epoch 48/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.3814 - loss: 1.4061 - val_accuracy: 0.3367 - val_loss: 1.4876\n",
            "Epoch 49/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.4096 - loss: 1.3777 - val_accuracy: 0.3349 - val_loss: 1.4839\n",
            "Epoch 50/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.3974 - loss: 1.3985 - val_accuracy: 0.3397 - val_loss: 1.4845\n",
            "Epoch 51/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.4040 - loss: 1.3788 - val_accuracy: 0.3397 - val_loss: 1.4868\n",
            "Epoch 52/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.4117 - loss: 1.3789 - val_accuracy: 0.3415 - val_loss: 1.4847\n",
            "Epoch 53/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.4038 - loss: 1.3918 - val_accuracy: 0.3415 - val_loss: 1.4831\n",
            "Epoch 54/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.4069 - loss: 1.3863 - val_accuracy: 0.3480 - val_loss: 1.4812\n",
            "Epoch 55/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.3946 - loss: 1.3935 - val_accuracy: 0.3576 - val_loss: 1.4772\n",
            "Epoch 56/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.4098 - loss: 1.3835 - val_accuracy: 0.3445 - val_loss: 1.4788\n",
            "Epoch 57/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.4098 - loss: 1.3784 - val_accuracy: 0.3397 - val_loss: 1.4761\n",
            "Epoch 58/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.4089 - loss: 1.3831 - val_accuracy: 0.3373 - val_loss: 1.4853\n",
            "Epoch 59/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.4080 - loss: 1.3724 - val_accuracy: 0.3504 - val_loss: 1.4784\n",
            "Epoch 60/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.4084 - loss: 1.3666 - val_accuracy: 0.3451 - val_loss: 1.4813\n",
            "Epoch 61/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.4020 - loss: 1.3819 - val_accuracy: 0.3474 - val_loss: 1.4838\n",
            "Epoch 62/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.4143 - loss: 1.3708 - val_accuracy: 0.3540 - val_loss: 1.4804\n",
            "Epoch 63/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.4045 - loss: 1.3699 - val_accuracy: 0.3588 - val_loss: 1.4729\n",
            "Epoch 64/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.4199 - loss: 1.3551 - val_accuracy: 0.3451 - val_loss: 1.4712\n",
            "Epoch 65/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.4118 - loss: 1.3693 - val_accuracy: 0.3558 - val_loss: 1.4727\n",
            "Epoch 66/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.4251 - loss: 1.3698 - val_accuracy: 0.3600 - val_loss: 1.4690\n",
            "Epoch 67/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.4100 - loss: 1.3635 - val_accuracy: 0.3635 - val_loss: 1.4675\n",
            "Epoch 68/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.4232 - loss: 1.3550 - val_accuracy: 0.3588 - val_loss: 1.4666\n",
            "Epoch 69/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.4001 - loss: 1.3661 - val_accuracy: 0.3617 - val_loss: 1.4658\n",
            "Epoch 70/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.4107 - loss: 1.3687 - val_accuracy: 0.3516 - val_loss: 1.4666\n",
            "Epoch 71/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.4206 - loss: 1.3537 - val_accuracy: 0.3456 - val_loss: 1.4708\n",
            "Epoch 72/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.4266 - loss: 1.3565 - val_accuracy: 0.3558 - val_loss: 1.4643\n",
            "Epoch 73/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.4160 - loss: 1.3548 - val_accuracy: 0.3695 - val_loss: 1.4574\n",
            "Epoch 74/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.4322 - loss: 1.3411 - val_accuracy: 0.3498 - val_loss: 1.4648\n",
            "Epoch 75/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.4187 - loss: 1.3621 - val_accuracy: 0.3653 - val_loss: 1.4650\n",
            "Epoch 76/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.4265 - loss: 1.3512 - val_accuracy: 0.3629 - val_loss: 1.4701\n",
            "Epoch 77/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.4154 - loss: 1.3567 - val_accuracy: 0.3623 - val_loss: 1.4611\n",
            "Epoch 78/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.4218 - loss: 1.3389 - val_accuracy: 0.3558 - val_loss: 1.4642\n",
            "Epoch 79/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.4296 - loss: 1.3460 - val_accuracy: 0.3588 - val_loss: 1.4670\n",
            "Epoch 80/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.4315 - loss: 1.3459 - val_accuracy: 0.3605 - val_loss: 1.4622\n",
            "Epoch 81/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.4248 - loss: 1.3403 - val_accuracy: 0.3576 - val_loss: 1.4675\n",
            "Epoch 82/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.4271 - loss: 1.3434 - val_accuracy: 0.3564 - val_loss: 1.4642\n",
            "Epoch 83/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.4155 - loss: 1.3590 - val_accuracy: 0.3564 - val_loss: 1.4686\n",
            "Epoch 84/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.4266 - loss: 1.3292 - val_accuracy: 0.3588 - val_loss: 1.4612\n",
            "Epoch 85/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.4435 - loss: 1.3192 - val_accuracy: 0.3534 - val_loss: 1.4618\n",
            "Epoch 86/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.4357 - loss: 1.3171 - val_accuracy: 0.3635 - val_loss: 1.4633\n",
            "Epoch 87/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.4237 - loss: 1.3428 - val_accuracy: 0.3623 - val_loss: 1.4627\n",
            "Epoch 88/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.4213 - loss: 1.3479 - val_accuracy: 0.3617 - val_loss: 1.4663\n",
            "Epoch 89/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.4321 - loss: 1.3329 - val_accuracy: 0.3516 - val_loss: 1.4638\n",
            "Epoch 90/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.4254 - loss: 1.3464 - val_accuracy: 0.3600 - val_loss: 1.4589\n",
            "Epoch 91/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.4375 - loss: 1.3220 - val_accuracy: 0.3588 - val_loss: 1.4653\n",
            "Epoch 92/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.4396 - loss: 1.3240 - val_accuracy: 0.3576 - val_loss: 1.4640\n",
            "Epoch 93/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.4434 - loss: 1.3156 - val_accuracy: 0.3611 - val_loss: 1.4654\n",
            "Epoch 94/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.4353 - loss: 1.3317 - val_accuracy: 0.3600 - val_loss: 1.4625\n",
            "Epoch 95/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.4163 - loss: 1.3383 - val_accuracy: 0.3600 - val_loss: 1.4618\n",
            "Epoch 96/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.4263 - loss: 1.3278 - val_accuracy: 0.3629 - val_loss: 1.4549\n",
            "Epoch 97/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.4230 - loss: 1.3447 - val_accuracy: 0.3635 - val_loss: 1.4517\n",
            "Epoch 98/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.4370 - loss: 1.3211 - val_accuracy: 0.3605 - val_loss: 1.4576\n",
            "Epoch 99/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.4481 - loss: 1.3081 - val_accuracy: 0.3665 - val_loss: 1.4552\n",
            "Epoch 100/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.4316 - loss: 1.3252 - val_accuracy: 0.3677 - val_loss: 1.4616\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001211 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 1091\n",
            "[LightGBM] [Info] Number of data points in the train set: 6712, number of used features: 20\n",
            "[LightGBM] [Info] Start training from score -1.595677\n",
            "[LightGBM] [Info] Start training from score -1.629300\n",
            "[LightGBM] [Info] Start training from score -1.603792\n",
            "[LightGBM] [Info] Start training from score -1.601573\n",
            "[LightGBM] [Info] Start training from score -1.617215\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001007 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 1091\n",
            "[LightGBM] [Info] Number of data points in the train set: 5369, number of used features: 20\n",
            "[LightGBM] [Info] Start training from score -1.595382\n",
            "[LightGBM] [Info] Start training from score -1.628998\n",
            "[LightGBM] [Info] Start training from score -1.603681\n",
            "[LightGBM] [Info] Start training from score -1.601830\n",
            "[LightGBM] [Info] Start training from score -1.617667\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000999 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 1091\n",
            "[LightGBM] [Info] Number of data points in the train set: 5369, number of used features: 20\n",
            "[LightGBM] [Info] Start training from score -1.595382\n",
            "[LightGBM] [Info] Start training from score -1.628998\n",
            "[LightGBM] [Info] Start training from score -1.603681\n",
            "[LightGBM] [Info] Start training from score -1.601830\n",
            "[LightGBM] [Info] Start training from score -1.617667\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001165 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 1091\n",
            "[LightGBM] [Info] Number of data points in the train set: 5370, number of used features: 20\n",
            "[LightGBM] [Info] Start training from score -1.595568\n",
            "[LightGBM] [Info] Start training from score -1.630135\n",
            "[LightGBM] [Info] Start training from score -1.603867\n",
            "[LightGBM] [Info] Start training from score -1.601093\n",
            "[LightGBM] [Info] Start training from score -1.616915\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000978 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 1091\n",
            "[LightGBM] [Info] Number of data points in the train set: 5370, number of used features: 20\n",
            "[LightGBM] [Info] Start training from score -1.596487\n",
            "[LightGBM] [Info] Start training from score -1.629185\n",
            "[LightGBM] [Info] Start training from score -1.603867\n",
            "[LightGBM] [Info] Start training from score -1.601093\n",
            "[LightGBM] [Info] Start training from score -1.616915\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000997 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 1091\n",
            "[LightGBM] [Info] Number of data points in the train set: 5370, number of used features: 20\n",
            "[LightGBM] [Info] Start training from score -1.595568\n",
            "[LightGBM] [Info] Start training from score -1.629185\n",
            "[LightGBM] [Info] Start training from score -1.603867\n",
            "[LightGBM] [Info] Start training from score -1.602017\n",
            "[LightGBM] [Info] Start training from score -1.616915\n",
            "ðŸ”— **Stacking + ANN Optimized Accuracy: 62.16%**\n",
            "âœ… **Optimized Stacking Model Saved Successfully!**\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from sklearn.ensemble import StackingClassifier\n",
        "from xgboost import XGBClassifier\n",
        "import joblib\n",
        "\n",
        "# âœ… Step 1ï¸âƒ£: Optimized Neural Network\n",
        "def build_better_ann(input_dim):\n",
        "    model = keras.Sequential([\n",
        "        layers.Input(shape=(input_dim,)),\n",
        "        layers.Dense(256, activation=\"relu\"),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.Dropout(0.3),\n",
        "        layers.Dense(128, activation=\"relu\"),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.Dropout(0.3),\n",
        "        layers.Dense(64, activation=\"relu\"),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.Dense(len(set(y_train)), activation=\"softmax\")  # Multi-class classification\n",
        "    ])\n",
        "    model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.0005),  # Lower learning rate\n",
        "                  loss=\"sparse_categorical_crossentropy\",\n",
        "                  metrics=[\"accuracy\"])\n",
        "    return model\n",
        "\n",
        "ann_model = build_better_ann(X_train_selected.shape[1])\n",
        "ann_model.fit(X_train_selected, y_train, epochs=100, batch_size=64, verbose=1, validation_data=(X_test_selected, y_test))\n",
        "\n",
        "# âœ… Step 2ï¸âƒ£: Use XGBoost as the Meta-Learner\n",
        "stacking_clf = StackingClassifier(\n",
        "    estimators=[\n",
        "        ('RandomForest', best_rf),\n",
        "        ('XGBoost', best_xgb),\n",
        "        ('LightGBM', best_lgb),\n",
        "        ('CatBoost', best_cat)\n",
        "    ],\n",
        "    final_estimator=XGBClassifier(n_estimators=100, learning_rate=0.05),  # Replace Logistic Regression\n",
        "    stack_method='predict_proba'\n",
        ")\n",
        "\n",
        "stacking_clf.fit(X_train_selected, y_train)\n",
        "y_pred_stack = stacking_clf.predict(X_test_selected)\n",
        "stacking_accuracy = accuracy_score(y_test, y_pred_stack)\n",
        "\n",
        "# âœ… Step 3ï¸âƒ£: Evaluate & Save the Best Model\n",
        "print(f\"ðŸ”— **Final Optimized Stacking Accuracy: {stacking_accuracy * 100:.2f}%**\")\n",
        "joblib.dump(stacking_clf, \"best_final_model.pkl\")\n",
        "print(\"âœ… **Final Optimized Stacking Model Saved!**\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-G56rlE9vA_N",
        "outputId": "8b323c87-ecc2-466e-9729-2f0c73ebb042"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 11ms/step - accuracy: 0.2127 - loss: 2.0932 - val_accuracy: 0.2229 - val_loss: 1.6155\n",
            "Epoch 2/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.2346 - loss: 1.7871 - val_accuracy: 0.2473 - val_loss: 1.5999\n",
            "Epoch 3/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 23ms/step - accuracy: 0.2643 - loss: 1.6651 - val_accuracy: 0.2771 - val_loss: 1.5744\n",
            "Epoch 4/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.2578 - loss: 1.6361 - val_accuracy: 0.2813 - val_loss: 1.5625\n",
            "Epoch 5/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 0.2934 - loss: 1.5917 - val_accuracy: 0.2968 - val_loss: 1.5493\n",
            "Epoch 6/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.2898 - loss: 1.5583 - val_accuracy: 0.3069 - val_loss: 1.5428\n",
            "Epoch 7/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.2864 - loss: 1.5614 - val_accuracy: 0.3111 - val_loss: 1.5359\n",
            "Epoch 8/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.3298 - loss: 1.5153 - val_accuracy: 0.3099 - val_loss: 1.5300\n",
            "Epoch 9/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.3185 - loss: 1.5100 - val_accuracy: 0.3153 - val_loss: 1.5189\n",
            "Epoch 10/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.3272 - loss: 1.5015 - val_accuracy: 0.3188 - val_loss: 1.5144\n",
            "Epoch 11/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.3367 - loss: 1.4911 - val_accuracy: 0.3206 - val_loss: 1.5198\n",
            "Epoch 12/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.3383 - loss: 1.4886 - val_accuracy: 0.3319 - val_loss: 1.5114\n",
            "Epoch 13/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.3454 - loss: 1.4662 - val_accuracy: 0.3236 - val_loss: 1.5153\n",
            "Epoch 14/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.3433 - loss: 1.4703 - val_accuracy: 0.3230 - val_loss: 1.5119\n",
            "Epoch 15/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.3601 - loss: 1.4599 - val_accuracy: 0.3313 - val_loss: 1.5033\n",
            "Epoch 16/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.3557 - loss: 1.4676 - val_accuracy: 0.3266 - val_loss: 1.5025\n",
            "Epoch 17/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.3792 - loss: 1.4395 - val_accuracy: 0.3248 - val_loss: 1.5032\n",
            "Epoch 18/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.3740 - loss: 1.4425 - val_accuracy: 0.3284 - val_loss: 1.4959\n",
            "Epoch 19/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.3634 - loss: 1.4367 - val_accuracy: 0.3325 - val_loss: 1.4922\n",
            "Epoch 20/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.3695 - loss: 1.4366 - val_accuracy: 0.3367 - val_loss: 1.4889\n",
            "Epoch 21/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.3744 - loss: 1.4234 - val_accuracy: 0.3379 - val_loss: 1.4873\n",
            "Epoch 22/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.3769 - loss: 1.4237 - val_accuracy: 0.3361 - val_loss: 1.4818\n",
            "Epoch 23/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.3763 - loss: 1.4220 - val_accuracy: 0.3468 - val_loss: 1.4858\n",
            "Epoch 24/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.3859 - loss: 1.4112 - val_accuracy: 0.3451 - val_loss: 1.4844\n",
            "Epoch 25/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.3980 - loss: 1.4095 - val_accuracy: 0.3337 - val_loss: 1.4822\n",
            "Epoch 26/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.4003 - loss: 1.3930 - val_accuracy: 0.3308 - val_loss: 1.4824\n",
            "Epoch 27/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.4101 - loss: 1.3879 - val_accuracy: 0.3456 - val_loss: 1.4737\n",
            "Epoch 28/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.4071 - loss: 1.3851 - val_accuracy: 0.3439 - val_loss: 1.4773\n",
            "Epoch 29/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.4017 - loss: 1.3831 - val_accuracy: 0.3486 - val_loss: 1.4730\n",
            "Epoch 30/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.4188 - loss: 1.3668 - val_accuracy: 0.3486 - val_loss: 1.4725\n",
            "Epoch 31/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.4170 - loss: 1.3682 - val_accuracy: 0.3564 - val_loss: 1.4740\n",
            "Epoch 32/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.4170 - loss: 1.3657 - val_accuracy: 0.3468 - val_loss: 1.4770\n",
            "Epoch 33/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.4292 - loss: 1.3511 - val_accuracy: 0.3564 - val_loss: 1.4628\n",
            "Epoch 34/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.4225 - loss: 1.3587 - val_accuracy: 0.3564 - val_loss: 1.4646\n",
            "Epoch 35/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.4223 - loss: 1.3507 - val_accuracy: 0.3510 - val_loss: 1.4682\n",
            "Epoch 36/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.4256 - loss: 1.3557 - val_accuracy: 0.3588 - val_loss: 1.4674\n",
            "Epoch 37/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.4451 - loss: 1.3271 - val_accuracy: 0.3647 - val_loss: 1.4584\n",
            "Epoch 38/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.4268 - loss: 1.3368 - val_accuracy: 0.3653 - val_loss: 1.4566\n",
            "Epoch 39/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.4336 - loss: 1.3407 - val_accuracy: 0.3701 - val_loss: 1.4607\n",
            "Epoch 40/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.4319 - loss: 1.3253 - val_accuracy: 0.3605 - val_loss: 1.4527\n",
            "Epoch 41/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.4297 - loss: 1.3328 - val_accuracy: 0.3522 - val_loss: 1.4573\n",
            "Epoch 42/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.4346 - loss: 1.3318 - val_accuracy: 0.3695 - val_loss: 1.4532\n",
            "Epoch 43/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.4576 - loss: 1.3106 - val_accuracy: 0.3695 - val_loss: 1.4547\n",
            "Epoch 44/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.4381 - loss: 1.3135 - val_accuracy: 0.3749 - val_loss: 1.4518\n",
            "Epoch 45/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.4439 - loss: 1.3151 - val_accuracy: 0.3737 - val_loss: 1.4498\n",
            "Epoch 46/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.4400 - loss: 1.3027 - val_accuracy: 0.3689 - val_loss: 1.4487\n",
            "Epoch 47/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.4511 - loss: 1.3047 - val_accuracy: 0.3671 - val_loss: 1.4467\n",
            "Epoch 48/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.4646 - loss: 1.2777 - val_accuracy: 0.3743 - val_loss: 1.4498\n",
            "Epoch 49/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.4602 - loss: 1.2794 - val_accuracy: 0.3743 - val_loss: 1.4431\n",
            "Epoch 50/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.4687 - loss: 1.2785 - val_accuracy: 0.3683 - val_loss: 1.4368\n",
            "Epoch 51/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.4627 - loss: 1.2696 - val_accuracy: 0.3784 - val_loss: 1.4408\n",
            "Epoch 52/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.4653 - loss: 1.2652 - val_accuracy: 0.3737 - val_loss: 1.4499\n",
            "Epoch 53/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.4589 - loss: 1.2804 - val_accuracy: 0.3784 - val_loss: 1.4388\n",
            "Epoch 54/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.4836 - loss: 1.2599 - val_accuracy: 0.3790 - val_loss: 1.4380\n",
            "Epoch 55/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.4789 - loss: 1.2514 - val_accuracy: 0.3778 - val_loss: 1.4347\n",
            "Epoch 56/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.4750 - loss: 1.2677 - val_accuracy: 0.3820 - val_loss: 1.4325\n",
            "Epoch 57/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.4689 - loss: 1.2631 - val_accuracy: 0.3772 - val_loss: 1.4354\n",
            "Epoch 58/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.4828 - loss: 1.2445 - val_accuracy: 0.3868 - val_loss: 1.4300\n",
            "Epoch 59/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - accuracy: 0.4866 - loss: 1.2376 - val_accuracy: 0.3820 - val_loss: 1.4319\n",
            "Epoch 60/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.4908 - loss: 1.2402 - val_accuracy: 0.3832 - val_loss: 1.4322\n",
            "Epoch 61/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.4987 - loss: 1.2306 - val_accuracy: 0.3838 - val_loss: 1.4330\n",
            "Epoch 62/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.4912 - loss: 1.2365 - val_accuracy: 0.3868 - val_loss: 1.4345\n",
            "Epoch 63/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.4992 - loss: 1.2196 - val_accuracy: 0.3892 - val_loss: 1.4341\n",
            "Epoch 64/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.5020 - loss: 1.2205 - val_accuracy: 0.3993 - val_loss: 1.4290\n",
            "Epoch 65/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.5111 - loss: 1.2168 - val_accuracy: 0.3975 - val_loss: 1.4260\n",
            "Epoch 66/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.4926 - loss: 1.2274 - val_accuracy: 0.3933 - val_loss: 1.4334\n",
            "Epoch 67/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.4893 - loss: 1.2172 - val_accuracy: 0.3862 - val_loss: 1.4369\n",
            "Epoch 68/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.5070 - loss: 1.2104 - val_accuracy: 0.3897 - val_loss: 1.4291\n",
            "Epoch 69/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.4825 - loss: 1.2473 - val_accuracy: 0.3969 - val_loss: 1.4296\n",
            "Epoch 70/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.4976 - loss: 1.2162 - val_accuracy: 0.3963 - val_loss: 1.4291\n",
            "Epoch 71/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.4995 - loss: 1.2080 - val_accuracy: 0.3874 - val_loss: 1.4261\n",
            "Epoch 72/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.5059 - loss: 1.1945 - val_accuracy: 0.3874 - val_loss: 1.4317\n",
            "Epoch 73/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.5016 - loss: 1.2107 - val_accuracy: 0.3832 - val_loss: 1.4348\n",
            "Epoch 74/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.5092 - loss: 1.1966 - val_accuracy: 0.3820 - val_loss: 1.4321\n",
            "Epoch 75/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.5042 - loss: 1.1947 - val_accuracy: 0.4011 - val_loss: 1.4287\n",
            "Epoch 76/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.5154 - loss: 1.1903 - val_accuracy: 0.3945 - val_loss: 1.4354\n",
            "Epoch 77/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.5332 - loss: 1.1735 - val_accuracy: 0.3856 - val_loss: 1.4359\n",
            "Epoch 78/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.5219 - loss: 1.1805 - val_accuracy: 0.3945 - val_loss: 1.4367\n",
            "Epoch 79/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.5195 - loss: 1.1745 - val_accuracy: 0.3939 - val_loss: 1.4304\n",
            "Epoch 80/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.5160 - loss: 1.1920 - val_accuracy: 0.3933 - val_loss: 1.4305\n",
            "Epoch 81/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.5344 - loss: 1.1671 - val_accuracy: 0.3802 - val_loss: 1.4403\n",
            "Epoch 82/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.5245 - loss: 1.1796 - val_accuracy: 0.3981 - val_loss: 1.4310\n",
            "Epoch 83/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 0.5190 - loss: 1.1717 - val_accuracy: 0.3927 - val_loss: 1.4294\n",
            "Epoch 84/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.5355 - loss: 1.1616 - val_accuracy: 0.3892 - val_loss: 1.4248\n",
            "Epoch 85/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.5286 - loss: 1.1655 - val_accuracy: 0.3993 - val_loss: 1.4362\n",
            "Epoch 86/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.5299 - loss: 1.1551 - val_accuracy: 0.3945 - val_loss: 1.4292\n",
            "Epoch 87/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.5170 - loss: 1.1749 - val_accuracy: 0.3880 - val_loss: 1.4387\n",
            "Epoch 88/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.5187 - loss: 1.1722 - val_accuracy: 0.3921 - val_loss: 1.4312\n",
            "Epoch 89/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.5350 - loss: 1.1543 - val_accuracy: 0.3957 - val_loss: 1.4333\n",
            "Epoch 90/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.5156 - loss: 1.1638 - val_accuracy: 0.3868 - val_loss: 1.4323\n",
            "Epoch 91/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.5409 - loss: 1.1426 - val_accuracy: 0.3969 - val_loss: 1.4276\n",
            "Epoch 92/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.5159 - loss: 1.1558 - val_accuracy: 0.3945 - val_loss: 1.4330\n",
            "Epoch 93/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 0.5300 - loss: 1.1488 - val_accuracy: 0.3909 - val_loss: 1.4295\n",
            "Epoch 94/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.5468 - loss: 1.1259 - val_accuracy: 0.3933 - val_loss: 1.4333\n",
            "Epoch 95/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.5230 - loss: 1.1544 - val_accuracy: 0.4005 - val_loss: 1.4242\n",
            "Epoch 96/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.5340 - loss: 1.1325 - val_accuracy: 0.4046 - val_loss: 1.4275\n",
            "Epoch 97/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.5283 - loss: 1.1659 - val_accuracy: 0.3933 - val_loss: 1.4335\n",
            "Epoch 98/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.5376 - loss: 1.1372 - val_accuracy: 0.3957 - val_loss: 1.4320\n",
            "Epoch 99/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.5412 - loss: 1.1266 - val_accuracy: 0.3981 - val_loss: 1.4371\n",
            "Epoch 100/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.5524 - loss: 1.1238 - val_accuracy: 0.3921 - val_loss: 1.4376\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001238 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 1091\n",
            "[LightGBM] [Info] Number of data points in the train set: 6712, number of used features: 20\n",
            "[LightGBM] [Info] Start training from score -1.595677\n",
            "[LightGBM] [Info] Start training from score -1.629300\n",
            "[LightGBM] [Info] Start training from score -1.603792\n",
            "[LightGBM] [Info] Start training from score -1.601573\n",
            "[LightGBM] [Info] Start training from score -1.617215\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001014 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 1091\n",
            "[LightGBM] [Info] Number of data points in the train set: 5369, number of used features: 20\n",
            "[LightGBM] [Info] Start training from score -1.595382\n",
            "[LightGBM] [Info] Start training from score -1.628998\n",
            "[LightGBM] [Info] Start training from score -1.603681\n",
            "[LightGBM] [Info] Start training from score -1.601830\n",
            "[LightGBM] [Info] Start training from score -1.617667\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001011 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 1091\n",
            "[LightGBM] [Info] Number of data points in the train set: 5369, number of used features: 20\n",
            "[LightGBM] [Info] Start training from score -1.595382\n",
            "[LightGBM] [Info] Start training from score -1.628998\n",
            "[LightGBM] [Info] Start training from score -1.603681\n",
            "[LightGBM] [Info] Start training from score -1.601830\n",
            "[LightGBM] [Info] Start training from score -1.617667\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001518 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 1091\n",
            "[LightGBM] [Info] Number of data points in the train set: 5370, number of used features: 20\n",
            "[LightGBM] [Info] Start training from score -1.595568\n",
            "[LightGBM] [Info] Start training from score -1.630135\n",
            "[LightGBM] [Info] Start training from score -1.603867\n",
            "[LightGBM] [Info] Start training from score -1.601093\n",
            "[LightGBM] [Info] Start training from score -1.616915\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000978 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 1091\n",
            "[LightGBM] [Info] Number of data points in the train set: 5370, number of used features: 20\n",
            "[LightGBM] [Info] Start training from score -1.596487\n",
            "[LightGBM] [Info] Start training from score -1.629185\n",
            "[LightGBM] [Info] Start training from score -1.603867\n",
            "[LightGBM] [Info] Start training from score -1.601093\n",
            "[LightGBM] [Info] Start training from score -1.616915\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001047 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 1091\n",
            "[LightGBM] [Info] Number of data points in the train set: 5370, number of used features: 20\n",
            "[LightGBM] [Info] Start training from score -1.595568\n",
            "[LightGBM] [Info] Start training from score -1.629185\n",
            "[LightGBM] [Info] Start training from score -1.603867\n",
            "[LightGBM] [Info] Start training from score -1.602017\n",
            "[LightGBM] [Info] Start training from score -1.616915\n",
            "ðŸ”— **Final Optimized Stacking Accuracy: 62.04%**\n",
            "âœ… **Final Optimized Stacking Model Saved!**\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import joblib\n",
        "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from lightgbm import LGBMClassifier\n",
        "from catboost import CatBoostClassifier\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# âœ… 1ï¸âƒ£ Reintroduce Useful Features\n",
        "df[\"Height_Weight_Ratio\"] = df[\"Height (cm)\"] / df[\"Weight (kg)\"]\n",
        "\n",
        "# âœ… 2ï¸âƒ£ Use Class Weights Instead of SMOTE\n",
        "class_weights = compute_class_weight(\"balanced\", classes=np.unique(y_train), y=y_train)\n",
        "class_weight_dict = {i: class_weights[i] for i in range(len(class_weights))}\n",
        "\n",
        "# âœ… 3ï¸âƒ£ Hyperparameter Tuning for RandomForest\n",
        "rf_params = {\n",
        "    \"n_estimators\": [200, 300, 500],\n",
        "    \"max_depth\": [10, 20, None],\n",
        "    \"min_samples_split\": [2, 5, 10]\n",
        "}\n",
        "rf_model = RandomizedSearchCV(RandomForestClassifier(random_state=42), rf_params, cv=5, n_iter=10, n_jobs=-1)\n",
        "rf_model.fit(X_train_selected, y_train)\n",
        "\n",
        "# âœ… 4ï¸âƒ£ Hyperparameter Tuning for XGBoost\n",
        "xgb_params = {\n",
        "    \"learning_rate\": [0.01, 0.05, 0.1],\n",
        "    \"n_estimators\": [100, 200, 300],\n",
        "    \"max_depth\": [3, 5, 7]\n",
        "}\n",
        "xgb_model = RandomizedSearchCV(XGBClassifier(use_label_encoder=False, eval_metric=\"mlogloss\"), xgb_params, cv=5, n_iter=10, n_jobs=-1)\n",
        "xgb_model.fit(X_train_selected, y_train)\n",
        "\n",
        "# âœ… 5ï¸âƒ£ Train a Better Neural Network\n",
        "def build_ann():\n",
        "    model = keras.Sequential([\n",
        "        layers.Input(shape=(X_train_selected.shape[1],)),\n",
        "        layers.Dense(256, activation=\"relu\"),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.Dropout(0.3),\n",
        "        layers.Dense(128, activation=\"relu\"),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.Dropout(0.3),\n",
        "        layers.Dense(len(set(y_train)), activation=\"softmax\")  # Multi-class classification\n",
        "    ])\n",
        "    model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.0005),\n",
        "                  loss=\"sparse_categorical_crossentropy\",\n",
        "                  metrics=[\"accuracy\"])\n",
        "    return model\n",
        "\n",
        "ann_model = build_ann()\n",
        "ann_model.fit(X_train_selected, y_train, epochs=50, batch_size=64, verbose=1, validation_data=(X_test_selected, y_test), class_weight=class_weight_dict)\n",
        "\n",
        "# âœ… 6ï¸âƒ£ Use a Weighted Voting Ensemble\n",
        "voting_clf = VotingClassifier(\n",
        "    estimators=[\n",
        "        ('RandomForest', rf_model.best_estimator_),\n",
        "        ('XGBoost', xgb_model.best_estimator_),\n",
        "        ('LightGBM', LGBMClassifier(n_estimators=300, learning_rate=0.05)),\n",
        "        ('CatBoost', CatBoostClassifier(iterations=300, depth=6, learning_rate=0.05, verbose=0))\n",
        "    ],\n",
        "    voting='soft',  # Soft voting for better probability averaging\n",
        "    weights=[3, 3, 2, 2]  # Giving more weight to the best models\n",
        ")\n",
        "\n",
        "voting_clf.fit(X_train_selected, y_train)\n",
        "y_pred_voting = voting_clf.predict(X_test_selected)\n",
        "voting_accuracy = accuracy_score(y_test, y_pred_voting)\n",
        "\n",
        "# âœ… 7ï¸âƒ£ Save the Best Model\n",
        "print(f\"ðŸ”— **Final Optimized Voting Accuracy: {voting_accuracy * 100:.2f}%**\")\n",
        "joblib.dump(voting_clf, \"best_final_model_voting.pkl\")\n",
        "print(\"âœ… **Final Optimized Model Saved!**\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ITHAmUDyw7QO",
        "outputId": "b5c0c16d-e6af-433b-97d5-ecf82ecdcab7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [16:27:12] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.2065 - loss: 2.2655 - val_accuracy: 0.2515 - val_loss: 1.6026\n",
            "Epoch 2/50\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.2506 - loss: 1.8857 - val_accuracy: 0.2586 - val_loss: 1.5854\n",
            "Epoch 3/50\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - accuracy: 0.2447 - loss: 1.7975 - val_accuracy: 0.2855 - val_loss: 1.5595\n",
            "Epoch 4/50\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.2817 - loss: 1.6782 - val_accuracy: 0.2652 - val_loss: 1.5594\n",
            "Epoch 5/50\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.2924 - loss: 1.6385 - val_accuracy: 0.2872 - val_loss: 1.5441\n",
            "Epoch 6/50\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.3004 - loss: 1.6114 - val_accuracy: 0.2998 - val_loss: 1.5336\n",
            "Epoch 7/50\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.3062 - loss: 1.5839 - val_accuracy: 0.3051 - val_loss: 1.5278\n",
            "Epoch 8/50\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.2921 - loss: 1.5830 - val_accuracy: 0.3123 - val_loss: 1.5234\n",
            "Epoch 9/50\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.3271 - loss: 1.5365 - val_accuracy: 0.3033 - val_loss: 1.5197\n",
            "Epoch 10/50\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.3210 - loss: 1.5191 - val_accuracy: 0.3021 - val_loss: 1.5145\n",
            "Epoch 11/50\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.3298 - loss: 1.5115 - val_accuracy: 0.3063 - val_loss: 1.5157\n",
            "Epoch 12/50\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.3306 - loss: 1.5118 - val_accuracy: 0.3033 - val_loss: 1.5090\n",
            "Epoch 13/50\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.3320 - loss: 1.4944 - val_accuracy: 0.3153 - val_loss: 1.5082\n",
            "Epoch 14/50\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.3455 - loss: 1.4789 - val_accuracy: 0.3260 - val_loss: 1.5059\n",
            "Epoch 15/50\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.3465 - loss: 1.4722 - val_accuracy: 0.3224 - val_loss: 1.5044\n",
            "Epoch 16/50\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.3526 - loss: 1.4696 - val_accuracy: 0.3212 - val_loss: 1.4989\n",
            "Epoch 17/50\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.3491 - loss: 1.4713 - val_accuracy: 0.3182 - val_loss: 1.4994\n",
            "Epoch 18/50\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.3488 - loss: 1.4615 - val_accuracy: 0.3182 - val_loss: 1.4954\n",
            "Epoch 19/50\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.3613 - loss: 1.4590 - val_accuracy: 0.3200 - val_loss: 1.4967\n",
            "Epoch 20/50\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.3667 - loss: 1.4341 - val_accuracy: 0.3164 - val_loss: 1.4993\n",
            "Epoch 21/50\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.3675 - loss: 1.4359 - val_accuracy: 0.3170 - val_loss: 1.4946\n",
            "Epoch 22/50\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.3754 - loss: 1.4442 - val_accuracy: 0.3135 - val_loss: 1.4991\n",
            "Epoch 23/50\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.3786 - loss: 1.4283 - val_accuracy: 0.3266 - val_loss: 1.4950\n",
            "Epoch 24/50\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.3754 - loss: 1.4264 - val_accuracy: 0.3200 - val_loss: 1.4943\n",
            "Epoch 25/50\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.3795 - loss: 1.4235 - val_accuracy: 0.3319 - val_loss: 1.4907\n",
            "Epoch 26/50\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.3935 - loss: 1.4022 - val_accuracy: 0.3373 - val_loss: 1.4897\n",
            "Epoch 27/50\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.3863 - loss: 1.4119 - val_accuracy: 0.3302 - val_loss: 1.4850\n",
            "Epoch 28/50\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.3857 - loss: 1.4128 - val_accuracy: 0.3391 - val_loss: 1.4793\n",
            "Epoch 29/50\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.4002 - loss: 1.3924 - val_accuracy: 0.3367 - val_loss: 1.4820\n",
            "Epoch 30/50\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.4104 - loss: 1.3930 - val_accuracy: 0.3302 - val_loss: 1.4786\n",
            "Epoch 31/50\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.3918 - loss: 1.3955 - val_accuracy: 0.3385 - val_loss: 1.4768\n",
            "Epoch 32/50\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.4229 - loss: 1.3756 - val_accuracy: 0.3361 - val_loss: 1.4782\n",
            "Epoch 33/50\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.3882 - loss: 1.3989 - val_accuracy: 0.3498 - val_loss: 1.4744\n",
            "Epoch 34/50\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.3892 - loss: 1.3934 - val_accuracy: 0.3433 - val_loss: 1.4763\n",
            "Epoch 35/50\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.4151 - loss: 1.3682 - val_accuracy: 0.3433 - val_loss: 1.4728\n",
            "Epoch 36/50\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.4053 - loss: 1.3792 - val_accuracy: 0.3516 - val_loss: 1.4700\n",
            "Epoch 37/50\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.4149 - loss: 1.3600 - val_accuracy: 0.3451 - val_loss: 1.4731\n",
            "Epoch 38/50\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.4239 - loss: 1.3553 - val_accuracy: 0.3474 - val_loss: 1.4709\n",
            "Epoch 39/50\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.4147 - loss: 1.3673 - val_accuracy: 0.3403 - val_loss: 1.4710\n",
            "Epoch 40/50\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.4135 - loss: 1.3534 - val_accuracy: 0.3355 - val_loss: 1.4734\n",
            "Epoch 41/50\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.4247 - loss: 1.3484 - val_accuracy: 0.3498 - val_loss: 1.4671\n",
            "Epoch 42/50\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.4369 - loss: 1.3447 - val_accuracy: 0.3445 - val_loss: 1.4689\n",
            "Epoch 43/50\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.4353 - loss: 1.3297 - val_accuracy: 0.3480 - val_loss: 1.4646\n",
            "Epoch 44/50\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.4335 - loss: 1.3477 - val_accuracy: 0.3504 - val_loss: 1.4651\n",
            "Epoch 45/50\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.4292 - loss: 1.3461 - val_accuracy: 0.3451 - val_loss: 1.4607\n",
            "Epoch 46/50\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.4447 - loss: 1.3168 - val_accuracy: 0.3498 - val_loss: 1.4637\n",
            "Epoch 47/50\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.4449 - loss: 1.3226 - val_accuracy: 0.3528 - val_loss: 1.4582\n",
            "Epoch 48/50\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.4486 - loss: 1.3215 - val_accuracy: 0.3534 - val_loss: 1.4583\n",
            "Epoch 49/50\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.4578 - loss: 1.3034 - val_accuracy: 0.3617 - val_loss: 1.4560\n",
            "Epoch 50/50\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.4509 - loss: 1.3140 - val_accuracy: 0.3653 - val_loss: 1.4487\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [16:28:16] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001218 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 1091\n",
            "[LightGBM] [Info] Number of data points in the train set: 6712, number of used features: 20\n",
            "[LightGBM] [Info] Start training from score -1.595677\n",
            "[LightGBM] [Info] Start training from score -1.629300\n",
            "[LightGBM] [Info] Start training from score -1.603792\n",
            "[LightGBM] [Info] Start training from score -1.601573\n",
            "[LightGBM] [Info] Start training from score -1.617215\n",
            "ðŸ”— **Final Optimized Voting Accuracy: 57.51%**\n",
            "âœ… **Final Optimized Model Saved!**\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import joblib\n",
        "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from lightgbm import LGBMClassifier\n",
        "from catboost import CatBoostClassifier\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.feature_selection import RFE\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "# âœ… **1ï¸âƒ£ Fix Data Issues - Check Label Distribution**\n",
        "unique_classes, class_counts = np.unique(y_train, return_counts=True)\n",
        "print(f\"ðŸ” Class Distribution: {dict(zip(unique_classes, class_counts))}\")\n",
        "\n",
        "# âœ… **2ï¸âƒ£ Recursive Feature Elimination (RFE) to Pick Best Features**\n",
        "selector = RFE(estimator=RandomForestClassifier(n_estimators=100, random_state=42), n_features_to_select=20)\n",
        "selector.fit(X_train, y_train)\n",
        "X_train_selected = selector.transform(X_train)\n",
        "X_test_selected = selector.transform(X_test)\n",
        "selected_features = np.array(feature_names)[selector.support_]\n",
        "\n",
        "# âœ… **3ï¸âƒ£ Hyperparameter Tuning for All Models**\n",
        "rf_params = {\"n_estimators\": [300, 500], \"max_depth\": [10, 20], \"min_samples_split\": [2, 5]}\n",
        "rf_model = RandomizedSearchCV(RandomForestClassifier(random_state=42), rf_params, cv=5, n_iter=10, n_jobs=-1)\n",
        "rf_model.fit(X_train_selected, y_train)\n",
        "\n",
        "xgb_params = {\"learning_rate\": [0.01, 0.05], \"n_estimators\": [200, 300], \"max_depth\": [3, 5]}\n",
        "xgb_model = RandomizedSearchCV(XGBClassifier(use_label_encoder=False, eval_metric=\"mlogloss\"), xgb_params, cv=5, n_iter=10, n_jobs=-1)\n",
        "xgb_model.fit(X_train_selected, y_train)\n",
        "\n",
        "# âœ… **4ï¸âƒ£ Train a Better Neural Network**\n",
        "def build_ann():\n",
        "    model = keras.Sequential([\n",
        "        layers.Dense(256, activation=\"relu\"),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.Dropout(0.3),\n",
        "        layers.Dense(128, activation=\"relu\"),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.Dropout(0.3),\n",
        "        layers.Dense(len(set(y_train)), activation=\"softmax\")\n",
        "    ])\n",
        "    model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.0003),\n",
        "                  loss=\"sparse_categorical_crossentropy\",\n",
        "                  metrics=[\"accuracy\"])\n",
        "    return model\n",
        "\n",
        "ann_model = build_ann()\n",
        "ann_model.fit(X_train_selected, y_train, epochs=100, batch_size=64, verbose=1, validation_data=(X_test_selected, y_test))\n",
        "\n",
        "# âœ… **5ï¸âƒ£ Stacking Classifier with Meta-Model**\n",
        "from sklearn.ensemble import StackingClassifier\n",
        "\n",
        "stacking_clf = StackingClassifier(\n",
        "    estimators=[\n",
        "        ('RandomForest', rf_model.best_estimator_),\n",
        "        ('XGBoost', xgb_model.best_estimator_),\n",
        "        ('LightGBM', LGBMClassifier(n_estimators=300, learning_rate=0.05)),\n",
        "        ('CatBoost', CatBoostClassifier(iterations=300, depth=6, learning_rate=0.05, verbose=0))\n",
        "    ],\n",
        "    final_estimator=LogisticRegression()  # Meta-model\n",
        ")\n",
        "\n",
        "stacking_clf.fit(X_train_selected, y_train)\n",
        "y_pred_stack = stacking_clf.predict(X_test_selected)\n",
        "stacking_accuracy = accuracy_score(y_test, y_pred_stack)\n",
        "\n",
        "# âœ… **6ï¸âƒ£ Save the Best Model**\n",
        "print(f\"ðŸ”— **Final Optimized Stacking Accuracy: {stacking_accuracy * 100:.2f}%**\")\n",
        "joblib.dump(stacking_clf, \"best_final_stacking_model.pkl\")\n",
        "print(\"âœ… **Final Optimized Stacking Model Saved!**\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ExvGNrHEy6JJ",
        "outputId": "96dc3791-8382-40f1-9926-48b33e588fc0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ” Class Distribution: {0: 1361, 1: 1316, 2: 1350, 3: 1353, 4: 1332}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_search.py:317: UserWarning: The total space of parameters 8 is smaller than n_iter=10. Running 8 iterations. For exhaustive searches, use GridSearchCV.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_search.py:317: UserWarning: The total space of parameters 8 is smaller than n_iter=10. Running 8 iterations. For exhaustive searches, use GridSearchCV.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [16:35:19] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - accuracy: 0.2148 - loss: 2.1944 - val_accuracy: 0.2294 - val_loss: 1.5977\n",
            "Epoch 2/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.2380 - loss: 1.9496 - val_accuracy: 0.2479 - val_loss: 1.5818\n",
            "Epoch 3/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.2540 - loss: 1.8559 - val_accuracy: 0.2628 - val_loss: 1.5756\n",
            "Epoch 4/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.2564 - loss: 1.7708 - val_accuracy: 0.2706 - val_loss: 1.5708\n",
            "Epoch 5/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.2811 - loss: 1.7062 - val_accuracy: 0.2861 - val_loss: 1.5609\n",
            "Epoch 6/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.2791 - loss: 1.6809 - val_accuracy: 0.2968 - val_loss: 1.5505\n",
            "Epoch 7/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.2912 - loss: 1.6534 - val_accuracy: 0.2980 - val_loss: 1.5502\n",
            "Epoch 8/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.2877 - loss: 1.6183 - val_accuracy: 0.3010 - val_loss: 1.5415\n",
            "Epoch 9/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.2978 - loss: 1.6034 - val_accuracy: 0.3117 - val_loss: 1.5372\n",
            "Epoch 10/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.2998 - loss: 1.5874 - val_accuracy: 0.3075 - val_loss: 1.5331\n",
            "Epoch 11/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.3024 - loss: 1.5708 - val_accuracy: 0.3111 - val_loss: 1.5316\n",
            "Epoch 12/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 25ms/step - accuracy: 0.3115 - loss: 1.5634 - val_accuracy: 0.3081 - val_loss: 1.5258\n",
            "Epoch 13/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.3193 - loss: 1.5493 - val_accuracy: 0.3099 - val_loss: 1.5237\n",
            "Epoch 14/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.3074 - loss: 1.5512 - val_accuracy: 0.3194 - val_loss: 1.5214\n",
            "Epoch 15/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - accuracy: 0.3217 - loss: 1.5254 - val_accuracy: 0.3224 - val_loss: 1.5166\n",
            "Epoch 16/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.3456 - loss: 1.4993 - val_accuracy: 0.3153 - val_loss: 1.5162\n",
            "Epoch 17/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.3372 - loss: 1.5054 - val_accuracy: 0.3206 - val_loss: 1.5142\n",
            "Epoch 18/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.3481 - loss: 1.4854 - val_accuracy: 0.3254 - val_loss: 1.5122\n",
            "Epoch 19/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.3328 - loss: 1.4913 - val_accuracy: 0.3254 - val_loss: 1.5106\n",
            "Epoch 20/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.3341 - loss: 1.4925 - val_accuracy: 0.3290 - val_loss: 1.5099\n",
            "Epoch 21/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.3466 - loss: 1.4869 - val_accuracy: 0.3266 - val_loss: 1.5075\n",
            "Epoch 22/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.3599 - loss: 1.4742 - val_accuracy: 0.3206 - val_loss: 1.5082\n",
            "Epoch 23/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.3431 - loss: 1.4728 - val_accuracy: 0.3242 - val_loss: 1.5109\n",
            "Epoch 24/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.3622 - loss: 1.4617 - val_accuracy: 0.3278 - val_loss: 1.5090\n",
            "Epoch 25/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.3569 - loss: 1.4720 - val_accuracy: 0.3284 - val_loss: 1.5055\n",
            "Epoch 26/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.3600 - loss: 1.4648 - val_accuracy: 0.3194 - val_loss: 1.5047\n",
            "Epoch 27/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.3766 - loss: 1.4589 - val_accuracy: 0.3248 - val_loss: 1.5036\n",
            "Epoch 28/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.3601 - loss: 1.4569 - val_accuracy: 0.3284 - val_loss: 1.4998\n",
            "Epoch 29/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.3682 - loss: 1.4463 - val_accuracy: 0.3242 - val_loss: 1.5010\n",
            "Epoch 30/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.3755 - loss: 1.4450 - val_accuracy: 0.3254 - val_loss: 1.5009\n",
            "Epoch 31/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.3707 - loss: 1.4371 - val_accuracy: 0.3260 - val_loss: 1.4968\n",
            "Epoch 32/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.3741 - loss: 1.4382 - val_accuracy: 0.3266 - val_loss: 1.4970\n",
            "Epoch 33/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.3775 - loss: 1.4278 - val_accuracy: 0.3313 - val_loss: 1.4932\n",
            "Epoch 34/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.3738 - loss: 1.4330 - val_accuracy: 0.3361 - val_loss: 1.4922\n",
            "Epoch 35/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.3815 - loss: 1.4214 - val_accuracy: 0.3290 - val_loss: 1.4920\n",
            "Epoch 36/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.3878 - loss: 1.4104 - val_accuracy: 0.3355 - val_loss: 1.4937\n",
            "Epoch 37/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.3993 - loss: 1.4145 - val_accuracy: 0.3313 - val_loss: 1.4904\n",
            "Epoch 38/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.3835 - loss: 1.4150 - val_accuracy: 0.3308 - val_loss: 1.4899\n",
            "Epoch 39/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.3958 - loss: 1.4145 - val_accuracy: 0.3278 - val_loss: 1.4886\n",
            "Epoch 40/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.4059 - loss: 1.3999 - val_accuracy: 0.3284 - val_loss: 1.4867\n",
            "Epoch 41/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.3835 - loss: 1.4115 - val_accuracy: 0.3331 - val_loss: 1.4870\n",
            "Epoch 42/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.3974 - loss: 1.3918 - val_accuracy: 0.3343 - val_loss: 1.4859\n",
            "Epoch 43/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.3887 - loss: 1.4002 - val_accuracy: 0.3337 - val_loss: 1.4837\n",
            "Epoch 44/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.3974 - loss: 1.3962 - val_accuracy: 0.3355 - val_loss: 1.4831\n",
            "Epoch 45/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.4002 - loss: 1.3963 - val_accuracy: 0.3456 - val_loss: 1.4807\n",
            "Epoch 46/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.4033 - loss: 1.3919 - val_accuracy: 0.3391 - val_loss: 1.4804\n",
            "Epoch 47/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.4038 - loss: 1.3849 - val_accuracy: 0.3385 - val_loss: 1.4788\n",
            "Epoch 48/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.4067 - loss: 1.3807 - val_accuracy: 0.3391 - val_loss: 1.4790\n",
            "Epoch 49/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.4139 - loss: 1.3792 - val_accuracy: 0.3355 - val_loss: 1.4798\n",
            "Epoch 50/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.4187 - loss: 1.3773 - val_accuracy: 0.3415 - val_loss: 1.4760\n",
            "Epoch 51/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.4129 - loss: 1.3818 - val_accuracy: 0.3486 - val_loss: 1.4775\n",
            "Epoch 52/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.4182 - loss: 1.3697 - val_accuracy: 0.3492 - val_loss: 1.4726\n",
            "Epoch 53/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.4037 - loss: 1.3947 - val_accuracy: 0.3456 - val_loss: 1.4728\n",
            "Epoch 54/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.4228 - loss: 1.3763 - val_accuracy: 0.3445 - val_loss: 1.4714\n",
            "Epoch 55/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.4278 - loss: 1.3496 - val_accuracy: 0.3492 - val_loss: 1.4702\n",
            "Epoch 56/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.4228 - loss: 1.3612 - val_accuracy: 0.3492 - val_loss: 1.4697\n",
            "Epoch 57/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.4359 - loss: 1.3421 - val_accuracy: 0.3456 - val_loss: 1.4675\n",
            "Epoch 58/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.4304 - loss: 1.3444 - val_accuracy: 0.3510 - val_loss: 1.4666\n",
            "Epoch 59/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.4223 - loss: 1.3518 - val_accuracy: 0.3540 - val_loss: 1.4683\n",
            "Epoch 60/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.4285 - loss: 1.3445 - val_accuracy: 0.3623 - val_loss: 1.4639\n",
            "Epoch 61/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.4393 - loss: 1.3322 - val_accuracy: 0.3558 - val_loss: 1.4656\n",
            "Epoch 62/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.4225 - loss: 1.3466 - val_accuracy: 0.3474 - val_loss: 1.4643\n",
            "Epoch 63/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.4412 - loss: 1.3382 - val_accuracy: 0.3605 - val_loss: 1.4603\n",
            "Epoch 64/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.4341 - loss: 1.3359 - val_accuracy: 0.3588 - val_loss: 1.4634\n",
            "Epoch 65/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.4270 - loss: 1.3401 - val_accuracy: 0.3552 - val_loss: 1.4616\n",
            "Epoch 66/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.4300 - loss: 1.3410 - val_accuracy: 0.3611 - val_loss: 1.4577\n",
            "Epoch 67/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.4370 - loss: 1.3265 - val_accuracy: 0.3701 - val_loss: 1.4541\n",
            "Epoch 68/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.4371 - loss: 1.3239 - val_accuracy: 0.3641 - val_loss: 1.4541\n",
            "Epoch 69/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.4298 - loss: 1.3314 - val_accuracy: 0.3647 - val_loss: 1.4573\n",
            "Epoch 70/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.4335 - loss: 1.3275 - val_accuracy: 0.3629 - val_loss: 1.4561\n",
            "Epoch 71/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.4412 - loss: 1.3129 - val_accuracy: 0.3558 - val_loss: 1.4538\n",
            "Epoch 72/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.4560 - loss: 1.3099 - val_accuracy: 0.3677 - val_loss: 1.4500\n",
            "Epoch 73/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.4614 - loss: 1.3113 - val_accuracy: 0.3689 - val_loss: 1.4529\n",
            "Epoch 74/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.4551 - loss: 1.3030 - val_accuracy: 0.3605 - val_loss: 1.4526\n",
            "Epoch 75/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.4501 - loss: 1.3088 - val_accuracy: 0.3576 - val_loss: 1.4479\n",
            "Epoch 76/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - accuracy: 0.4588 - loss: 1.3015 - val_accuracy: 0.3772 - val_loss: 1.4449\n",
            "Epoch 77/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.4429 - loss: 1.3119 - val_accuracy: 0.3707 - val_loss: 1.4482\n",
            "Epoch 78/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.4588 - loss: 1.3045 - val_accuracy: 0.3665 - val_loss: 1.4439\n",
            "Epoch 79/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.4658 - loss: 1.2978 - val_accuracy: 0.3683 - val_loss: 1.4392\n",
            "Epoch 80/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.4594 - loss: 1.2858 - val_accuracy: 0.3731 - val_loss: 1.4421\n",
            "Epoch 81/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.4584 - loss: 1.2920 - val_accuracy: 0.3635 - val_loss: 1.4412\n",
            "Epoch 82/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.4561 - loss: 1.3002 - val_accuracy: 0.3754 - val_loss: 1.4470\n",
            "Epoch 83/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.4655 - loss: 1.2828 - val_accuracy: 0.3749 - val_loss: 1.4443\n",
            "Epoch 84/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.4631 - loss: 1.2794 - val_accuracy: 0.3725 - val_loss: 1.4415\n",
            "Epoch 85/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.4660 - loss: 1.2795 - val_accuracy: 0.3695 - val_loss: 1.4429\n",
            "Epoch 86/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.4639 - loss: 1.2754 - val_accuracy: 0.3737 - val_loss: 1.4468\n",
            "Epoch 87/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.4682 - loss: 1.2861 - val_accuracy: 0.3743 - val_loss: 1.4458\n",
            "Epoch 88/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.4726 - loss: 1.2679 - val_accuracy: 0.3778 - val_loss: 1.4437\n",
            "Epoch 89/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.4603 - loss: 1.2937 - val_accuracy: 0.3814 - val_loss: 1.4441\n",
            "Epoch 90/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.4578 - loss: 1.2789 - val_accuracy: 0.3731 - val_loss: 1.4444\n",
            "Epoch 91/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.4774 - loss: 1.2607 - val_accuracy: 0.3772 - val_loss: 1.4445\n",
            "Epoch 92/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.4713 - loss: 1.2675 - val_accuracy: 0.3754 - val_loss: 1.4398\n",
            "Epoch 93/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.4662 - loss: 1.2796 - val_accuracy: 0.3749 - val_loss: 1.4406\n",
            "Epoch 94/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.4766 - loss: 1.2677 - val_accuracy: 0.3832 - val_loss: 1.4395\n",
            "Epoch 95/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.4789 - loss: 1.2568 - val_accuracy: 0.3772 - val_loss: 1.4380\n",
            "Epoch 96/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.4814 - loss: 1.2555 - val_accuracy: 0.3826 - val_loss: 1.4349\n",
            "Epoch 97/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.4832 - loss: 1.2609 - val_accuracy: 0.3838 - val_loss: 1.4371\n",
            "Epoch 98/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.4771 - loss: 1.2604 - val_accuracy: 0.3707 - val_loss: 1.4408\n",
            "Epoch 99/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.4935 - loss: 1.2478 - val_accuracy: 0.3802 - val_loss: 1.4406\n",
            "Epoch 100/100\n",
            "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.4778 - loss: 1.2442 - val_accuracy: 0.3808 - val_loss: 1.4378\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [16:37:10] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001422 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 1091\n",
            "[LightGBM] [Info] Number of data points in the train set: 6712, number of used features: 20\n",
            "[LightGBM] [Info] Start training from score -1.595677\n",
            "[LightGBM] [Info] Start training from score -1.629300\n",
            "[LightGBM] [Info] Start training from score -1.603792\n",
            "[LightGBM] [Info] Start training from score -1.601573\n",
            "[LightGBM] [Info] Start training from score -1.617215\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [16:37:59] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [16:38:01] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [16:38:03] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [16:38:08] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [16:38:10] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001177 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 1091\n",
            "[LightGBM] [Info] Number of data points in the train set: 5369, number of used features: 20\n",
            "[LightGBM] [Info] Start training from score -1.595382\n",
            "[LightGBM] [Info] Start training from score -1.628998\n",
            "[LightGBM] [Info] Start training from score -1.603681\n",
            "[LightGBM] [Info] Start training from score -1.601830\n",
            "[LightGBM] [Info] Start training from score -1.617667\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001128 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 1091\n",
            "[LightGBM] [Info] Number of data points in the train set: 5369, number of used features: 20\n",
            "[LightGBM] [Info] Start training from score -1.595382\n",
            "[LightGBM] [Info] Start training from score -1.628998\n",
            "[LightGBM] [Info] Start training from score -1.603681\n",
            "[LightGBM] [Info] Start training from score -1.601830\n",
            "[LightGBM] [Info] Start training from score -1.617667\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001098 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 1091\n",
            "[LightGBM] [Info] Number of data points in the train set: 5370, number of used features: 20\n",
            "[LightGBM] [Info] Start training from score -1.595568\n",
            "[LightGBM] [Info] Start training from score -1.630135\n",
            "[LightGBM] [Info] Start training from score -1.603867\n",
            "[LightGBM] [Info] Start training from score -1.601093\n",
            "[LightGBM] [Info] Start training from score -1.616915\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001664 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 1091\n",
            "[LightGBM] [Info] Number of data points in the train set: 5370, number of used features: 20\n",
            "[LightGBM] [Info] Start training from score -1.596487\n",
            "[LightGBM] [Info] Start training from score -1.629185\n",
            "[LightGBM] [Info] Start training from score -1.603867\n",
            "[LightGBM] [Info] Start training from score -1.601093\n",
            "[LightGBM] [Info] Start training from score -1.616915\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001151 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 1091\n",
            "[LightGBM] [Info] Number of data points in the train set: 5370, number of used features: 20\n",
            "[LightGBM] [Info] Start training from score -1.595568\n",
            "[LightGBM] [Info] Start training from score -1.629185\n",
            "[LightGBM] [Info] Start training from score -1.603867\n",
            "[LightGBM] [Info] Start training from score -1.602017\n",
            "[LightGBM] [Info] Start training from score -1.616915\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ”— **Final Optimized Stacking Accuracy: 64.90%**\n",
            "âœ… **Final Optimized Stacking Model Saved!**\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import StackingClassifier\n",
        "\n",
        "stacking_clf = StackingClassifier(\n",
        "    estimators=[\n",
        "        ('RandomForest', rf_grid.best_estimator_),\n",
        "        ('XGBoost', xgb_grid.best_estimator_),\n",
        "        ('LightGBM', LGBMClassifier(n_estimators=500, learning_rate=0.05)),\n",
        "        ('CatBoost', CatBoostClassifier(iterations=500, depth=8, learning_rate=0.05, verbose=0))\n",
        "    ],\n",
        "    final_estimator=LogisticRegression()\n",
        ")\n",
        "\n",
        "stacking_clf.fit(X_train_selected, y_train)\n",
        "y_pred_stack = stacking_clf.predict(X_test_selected)\n",
        "stacking_accuracy = accuracy_score(y_test, y_pred_stack)\n",
        "\n",
        "print(f\"ðŸ”— **Final Optimized Stacking Accuracy: {stacking_accuracy * 100:.2f}%**\")\n",
        "joblib.dump(stacking_clf, \"best_final_stacking_model.pkl\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "FCMGG4Lx1-QU",
        "outputId": "f586248d-326d-495a-9835-720ad05b905c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'rf_grid' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-c446ca49683b>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m stacking_clf = StackingClassifier(\n\u001b[1;32m      4\u001b[0m     estimators=[\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0;34m(\u001b[0m\u001b[0;34m'RandomForest'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrf_grid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0;34m(\u001b[0m\u001b[0;34m'XGBoost'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxgb_grid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;34m(\u001b[0m\u001b[0;34m'LightGBM'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLGBMClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_estimators\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.05\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'rf_grid' is not defined"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMiD+gBJ2Q7aDcVGC0qtdsj",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}